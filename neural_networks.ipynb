{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T03:30:07.299320Z",
     "start_time": "2018-05-07T03:30:04.086982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import dicom2nifti\n",
    "import dicom2nifti.compressed_dicom as compressed_dicom\n",
    "import importlib\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.transforms as tr\n",
    "import niftiutils.registration as regs\n",
    "import niftiutils.visualization as vis\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import shutil\n",
    "import config\n",
    "import cnn_builder as cbuild\n",
    "from keras.models import Model\n",
    "import dcgan as dcg\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T04:19:38.793148Z",
     "start_time": "2018-05-07T04:19:38.414359Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "reshape_55 (Reshape)         (None, 64, 2048)          0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64, 1024)          2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 64, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "reshape_56 (Reshape)         (None, 64, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_30 (Conv3D)           (None, 64, 32, 32, 64)    1792      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 32, 32, 32, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_31 (Conv3D)           (None, 32, 32, 32, 64)    110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 32, 32, 32, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_15 (Conv3DT (None, 32, 32, 32, 1)     8001      \n",
      "_________________________________________________________________\n",
      "reshape_57 (Reshape)         (None, 32, 32, 32)        0         \n",
      "=================================================================\n",
      "Total params: 3,276,673\n",
      "Trainable params: 3,272,449\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(cbuild)\n",
    "C = config.Config()\n",
    "\n",
    "model = cbuild.build_cnn(lr=.002)\n",
    "model.summary()\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T04:22:56.898190Z",
     "start_time": "2018-05-07T04:19:38.884366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 4s 242ms/step - loss: 0.1595\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0916\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0846\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0972\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0830\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0826\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0804\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0733\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0573\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0554\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0620\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0433\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0430\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0362\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0331\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0304\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0272\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0275\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0240\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0219\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0222\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0196\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0180\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0191\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0166\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0178\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0176\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0171\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0162\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0165\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0137\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0145\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 2s 151ms/step - loss: 0.0154\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.0156\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0149\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0136\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0138\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0152\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0137\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0142\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0149\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.0133\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0126\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0128\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0137\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0137\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0132\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.0149\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.0143\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.0141\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.0128\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.0135\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0125\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.0143\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.0139\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 2s 156ms/step - loss: 0.0111\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 2s 156ms/step - loss: 0.0120\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0123\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0131\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0115\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0114\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 2s 159ms/step - loss: 0.0123\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0147\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0129\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0144\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 2s 160ms/step - loss: 0.0122\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 2s 156ms/step - loss: 0.0120\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 2s 156ms/step - loss: 0.0120\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0125\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0139\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 2s 156ms/step - loss: 0.0112\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0111\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.0113\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.0122\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 2s 159ms/step - loss: 0.0124\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0109\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0115\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 2s 163ms/step - loss: 0.0104\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0110\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 2s 162ms/step - loss: 0.0140\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 2s 164ms/step - loss: 0.0121\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.0119\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0115\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.0126\n",
      "Epoch 85/100\n",
      " 4/15 [=======>......................] - ETA: 1s - loss: 0.0129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-68bc579f04f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbuild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[early_stopping])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2147\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2148\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2149\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2472\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2473\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2474\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2475\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(cbuild.train_generator(), steps_per_epoch=15, epochs=100)#, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T04:19:29.379238Z",
     "start_time": "2018-05-07T04:19:29.328985Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y=next(cbuild.train_generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T04:19:30.784992Z",
     "start_time": "2018-05-07T04:19:30.652120Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEELYNHkZCiGEEMKm+f+EeuANt14ZtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f3d5ee630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.draw_slices(X[0], normalize=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:18:20.269727Z",
     "start_time": "2018-05-07T13:18:20.103173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIUmS1LX/A3ThBkepI5xIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23faf1752b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.draw_slices(Y[0], normalize=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:18:11.280924Z",
     "start_time": "2018-05-07T13:18:11.275884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:00:36.176996Z",
     "start_time": "2018-05-07T13:00:36.013471Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGlqLoYkSdLUXAxJkqSpuRiSJElT+x/vUiVpdvj9pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f7e37d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_pred = model.predict(X[:1])[0]\n",
    "y_pred = dcgan.generator.predict(X[:1])[0]\n",
    "vis.draw_slices(y_pred, normalize=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T05:47:02.415503Z",
     "start_time": "2018-05-07T04:41:43.530965Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_34 (InputLayer)        (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "reshape_82 (Reshape)         (None, 32, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_62 (Conv3D)           (None, 30, 30, 30, 64)    1792      \n",
      "_________________________________________________________________\n",
      "conv3d_63 (Conv3D)           (None, 15, 15, 15, 64)    110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 15, 15, 15, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_64 (Conv3D)           (None, 8, 8, 8, 64)       110656    \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 32768)             131072    \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 387,201\n",
      "Trainable params: 321,537\n",
      "Non-trainable params: 65,664\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_35 (InputLayer)        (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "reshape_83 (Reshape)         (None, 64, 2048)          0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64, 1024)          2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "reshape_84 (Reshape)         (None, 64, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 32, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_65 (Conv3D)           (None, 32, 32, 32, 64)    1792      \n",
      "_________________________________________________________________\n",
      "conv3d_66 (Conv3D)           (None, 32, 32, 32, 64)    110656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 32, 32, 32, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_22 (Conv3DT (None, 32, 32, 32, 1)     1729      \n",
      "_________________________________________________________________\n",
      "reshape_85 (Reshape)         (None, 32, 32, 32)        0         \n",
      "=================================================================\n",
      "Total params: 3,270,401\n",
      "Trainable params: 3,266,177\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\engine\\training.py:945: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.805990, acc.: 37.50%] [G loss: 0.125079] [C loss: 0.642659]\n",
      "1 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.117041] [C loss: 1.111043]\n",
      "2 [D loss: 1.013528, acc.: 37.50%] [G loss: 0.100765] [C loss: 0.802222]\n",
      "3 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.105306] [C loss: 1.396841]\n",
      "4 [D loss: 1.241776, acc.: 50.00%] [G loss: 0.082751] [C loss: 1.138247]\n",
      "5 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.093038] [C loss: 1.353363]\n",
      "6 [D loss: 1.365288, acc.: 50.00%] [G loss: 0.094416] [C loss: 0.971725]\n",
      "7 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.077331] [C loss: 0.941004]\n",
      "8 [D loss: 1.358604, acc.: 56.25%] [G loss: 0.066406] [C loss: 0.860510]\n",
      "9 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.068876] [C loss: 1.027691]\n",
      "10 [D loss: 1.495440, acc.: 50.00%] [G loss: 0.061585] [C loss: 1.477662]\n",
      "11 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.065514] [C loss: 0.893060]\n",
      "12 [D loss: 1.928228, acc.: 37.50%] [G loss: 0.064022] [C loss: 1.468141]\n",
      "13 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.079000] [C loss: 1.616902]\n",
      "14 [D loss: 1.718159, acc.: 56.25%] [G loss: 0.049550] [C loss: 1.640129]\n",
      "15 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.054809] [C loss: 1.812046]\n",
      "16 [D loss: 1.616960, acc.: 43.75%] [G loss: 0.048584] [C loss: 1.876944]\n",
      "17 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.047455] [C loss: 2.035467]\n",
      "18 [D loss: 1.292222, acc.: 56.25%] [G loss: 0.041487] [C loss: 1.493311]\n",
      "19 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.036185] [C loss: 2.240366]\n",
      "20 [D loss: 1.374220, acc.: 62.50%] [G loss: 0.036714] [C loss: 0.906960]\n",
      "21 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.037496] [C loss: 0.974295]\n",
      "22 [D loss: 1.831699, acc.: 43.75%] [G loss: 0.037142] [C loss: 1.720527]\n",
      "23 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.032588] [C loss: 1.720413]\n",
      "24 [D loss: 1.514461, acc.: 50.00%] [G loss: 0.033248] [C loss: 1.620840]\n",
      "25 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.028863] [C loss: 2.595787]\n",
      "26 [D loss: 1.330830, acc.: 50.00%] [G loss: 0.035470] [C loss: 2.111893]\n",
      "27 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.042700] [C loss: 1.547334]\n",
      "28 [D loss: 1.253727, acc.: 50.00%] [G loss: 0.027174] [C loss: 1.608361]\n",
      "29 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.027524] [C loss: 1.113099]\n",
      "30 [D loss: 1.745647, acc.: 43.75%] [G loss: 0.029769] [C loss: 1.804430]\n",
      "31 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.025913] [C loss: 1.810254]\n",
      "32 [D loss: 2.688973, acc.: 50.00%] [G loss: 0.033107] [C loss: 1.665191]\n",
      "33 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.030280] [C loss: 1.671116]\n",
      "34 [D loss: 1.498700, acc.: 62.50%] [G loss: 0.022631] [C loss: 1.452789]\n",
      "35 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020993] [C loss: 0.865372]\n",
      "36 [D loss: 2.495688, acc.: 50.00%] [G loss: 0.030415] [C loss: 1.441010]\n",
      "37 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.030704] [C loss: 1.151187]\n",
      "38 [D loss: 2.371415, acc.: 50.00%] [G loss: 0.025619] [C loss: 0.967339]\n",
      "39 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.024431] [C loss: 1.125951]\n",
      "40 [D loss: 1.852428, acc.: 43.75%] [G loss: 0.033778] [C loss: 2.916990]\n",
      "41 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.026438] [C loss: 3.249875]\n",
      "42 [D loss: 3.581148, acc.: 43.75%] [G loss: 0.023756] [C loss: 2.944835]\n",
      "43 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.030548] [C loss: 2.218325]\n",
      "44 [D loss: 1.739402, acc.: 56.25%] [G loss: 0.025202] [C loss: 1.380427]\n",
      "45 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023728] [C loss: 0.789569]\n",
      "46 [D loss: 1.451586, acc.: 43.75%] [G loss: 0.030938] [C loss: 1.326215]\n",
      "47 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.024598] [C loss: 0.823715]\n",
      "48 [D loss: 1.779658, acc.: 56.25%] [G loss: 0.023805] [C loss: 2.092325]\n",
      "49 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022587] [C loss: 2.389536]\n",
      "50 [D loss: 2.151123, acc.: 43.75%] [G loss: 0.021473] [C loss: 2.264250]\n",
      "51 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021127] [C loss: 3.087776]\n",
      "52 [D loss: 2.116767, acc.: 62.50%] [G loss: 0.019089] [C loss: 2.527544]\n",
      "53 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018950] [C loss: 2.495863]\n",
      "54 [D loss: 2.651184, acc.: 56.25%] [G loss: 0.020373] [C loss: 1.494179]\n",
      "55 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.025261] [C loss: 1.060362]\n",
      "56 [D loss: 1.659297, acc.: 50.00%] [G loss: 0.029389] [C loss: 1.438285]\n",
      "57 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022227] [C loss: 2.590736]\n",
      "58 [D loss: 1.186393, acc.: 50.00%] [G loss: 0.025524] [C loss: 2.746185]\n",
      "59 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.029254] [C loss: 2.371751]\n",
      "60 [D loss: 1.207149, acc.: 62.50%] [G loss: 0.021542] [C loss: 1.489051]\n",
      "61 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018846] [C loss: 2.016951]\n",
      "62 [D loss: 1.782105, acc.: 50.00%] [G loss: 0.020510] [C loss: 2.649186]\n",
      "63 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021991] [C loss: 1.647652]\n",
      "64 [D loss: 2.099716, acc.: 50.00%] [G loss: 0.022647] [C loss: 2.182644]\n",
      "65 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018936] [C loss: 2.146821]\n",
      "66 [D loss: 2.331195, acc.: 62.50%] [G loss: 0.026888] [C loss: 1.534633]\n",
      "67 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019557] [C loss: 1.302083]\n",
      "68 [D loss: 1.747692, acc.: 56.25%] [G loss: 0.016445] [C loss: 1.786664]\n",
      "69 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019510] [C loss: 1.636227]\n",
      "70 [D loss: 2.822744, acc.: 43.75%] [G loss: 0.016390] [C loss: 2.235738]\n",
      "71 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019996] [C loss: 2.272228]\n",
      "72 [D loss: 2.276408, acc.: 43.75%] [G loss: 0.019044] [C loss: 1.680605]\n",
      "73 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016228] [C loss: 1.074945]\n",
      "74 [D loss: 1.341321, acc.: 56.25%] [G loss: 0.024736] [C loss: 2.086295]\n",
      "75 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016843] [C loss: 1.270697]\n",
      "76 [D loss: 1.938476, acc.: 43.75%] [G loss: 0.018715] [C loss: 1.712646]\n",
      "77 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019189] [C loss: 1.740237]\n",
      "78 [D loss: 2.429784, acc.: 50.00%] [G loss: 0.030724] [C loss: 1.479313]\n",
      "79 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018210] [C loss: 1.583373]\n",
      "80 [D loss: 1.890891, acc.: 68.75%] [G loss: 0.018608] [C loss: 2.473557]\n",
      "81 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016137] [C loss: 2.518845]\n",
      "82 [D loss: 1.685064, acc.: 56.25%] [G loss: 0.021198] [C loss: 2.741930]\n",
      "83 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019549] [C loss: 3.126529]\n",
      "84 [D loss: 1.541564, acc.: 50.00%] [G loss: 0.017700] [C loss: 2.012554]\n",
      "85 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016397] [C loss: 2.293740]\n",
      "86 [D loss: 1.778748, acc.: 56.25%] [G loss: 0.018628] [C loss: 1.056470]\n",
      "87 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020373] [C loss: 1.668515]\n",
      "88 [D loss: 1.456094, acc.: 31.25%] [G loss: 0.017275] [C loss: 1.142659]\n",
      "89 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021080] [C loss: 1.399842]\n",
      "90 [D loss: 1.649574, acc.: 31.25%] [G loss: 0.016035] [C loss: 1.195315]\n",
      "91 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015382] [C loss: 1.195192]\n",
      "92 [D loss: 1.360702, acc.: 43.75%] [G loss: 0.012869] [C loss: 1.073016]\n",
      "93 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016833] [C loss: 1.197593]\n",
      "94 [D loss: 1.345568, acc.: 56.25%] [G loss: 0.016183] [C loss: 0.952556]\n",
      "95 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016784] [C loss: 0.957895]\n",
      "96 [D loss: 1.077685, acc.: 31.25%] [G loss: 0.015709] [C loss: 1.167955]\n",
      "97 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012090] [C loss: 1.078073]\n",
      "98 [D loss: 0.976969, acc.: 56.25%] [G loss: 0.017083] [C loss: 1.100930]\n",
      "99 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014424] [C loss: 0.976738]\n",
      "100 [D loss: 0.924895, acc.: 50.00%] [G loss: 0.020701] [C loss: 0.767615]\n",
      "101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.026143] [C loss: 1.143500]\n",
      "102 [D loss: 1.218695, acc.: 43.75%] [G loss: 0.018399] [C loss: 1.273200]\n",
      "103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018825] [C loss: 0.815395]\n",
      "104 [D loss: 1.037623, acc.: 50.00%] [G loss: 0.017798] [C loss: 0.963071]\n",
      "105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019216] [C loss: 0.980533]\n",
      "106 [D loss: 1.015880, acc.: 56.25%] [G loss: 0.013772] [C loss: 0.726905]\n",
      "107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014054] [C loss: 0.741712]\n",
      "108 [D loss: 1.057515, acc.: 62.50%] [G loss: 0.014938] [C loss: 0.765675]\n",
      "109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.028182] [C loss: 0.808885]\n",
      "110 [D loss: 0.856220, acc.: 56.25%] [G loss: 0.013588] [C loss: 0.720369]\n",
      "111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022897] [C loss: 0.782577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 [D loss: 0.868654, acc.: 43.75%] [G loss: 0.021099] [C loss: 0.840649]\n",
      "113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015630] [C loss: 0.917310]\n",
      "114 [D loss: 0.886561, acc.: 37.50%] [G loss: 0.016514] [C loss: 1.018362]\n",
      "115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012294] [C loss: 0.894759]\n",
      "116 [D loss: 0.893258, acc.: 56.25%] [G loss: 0.013013] [C loss: 0.848575]\n",
      "117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014028] [C loss: 0.856098]\n",
      "118 [D loss: 0.924023, acc.: 56.25%] [G loss: 0.018840] [C loss: 0.952379]\n",
      "119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010923] [C loss: 0.805289]\n",
      "120 [D loss: 0.889680, acc.: 62.50%] [G loss: 0.013022] [C loss: 0.782987]\n",
      "121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015125] [C loss: 1.084008]\n",
      "122 [D loss: 0.916069, acc.: 50.00%] [G loss: 0.011061] [C loss: 0.796384]\n",
      "123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017021] [C loss: 1.095506]\n",
      "124 [D loss: 0.852505, acc.: 43.75%] [G loss: 0.015563] [C loss: 0.944797]\n",
      "125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012572] [C loss: 1.020239]\n",
      "126 [D loss: 0.959354, acc.: 56.25%] [G loss: 0.016196] [C loss: 0.924896]\n",
      "127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014318] [C loss: 0.891149]\n",
      "128 [D loss: 0.953795, acc.: 43.75%] [G loss: 0.016069] [C loss: 0.765465]\n",
      "129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017250] [C loss: 0.852394]\n",
      "130 [D loss: 0.824676, acc.: 62.50%] [G loss: 0.012657] [C loss: 0.754874]\n",
      "131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012408] [C loss: 0.818499]\n",
      "132 [D loss: 0.857104, acc.: 56.25%] [G loss: 0.016999] [C loss: 0.835305]\n",
      "133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016873] [C loss: 0.804227]\n",
      "134 [D loss: 0.825622, acc.: 50.00%] [G loss: 0.016696] [C loss: 0.790312]\n",
      "135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012510] [C loss: 0.747389]\n",
      "136 [D loss: 0.778204, acc.: 43.75%] [G loss: 0.011472] [C loss: 0.739710]\n",
      "137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014140] [C loss: 0.754457]\n",
      "138 [D loss: 0.864954, acc.: 56.25%] [G loss: 0.014914] [C loss: 0.793821]\n",
      "139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019917] [C loss: 0.919233]\n",
      "140 [D loss: 0.848124, acc.: 50.00%] [G loss: 0.014222] [C loss: 0.848871]\n",
      "141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.024345] [C loss: 1.076638]\n",
      "142 [D loss: 0.900562, acc.: 43.75%] [G loss: 0.014383] [C loss: 0.935288]\n",
      "143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022899] [C loss: 0.937442]\n",
      "144 [D loss: 0.844000, acc.: 43.75%] [G loss: 0.012954] [C loss: 0.853897]\n",
      "145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014906] [C loss: 0.830404]\n",
      "146 [D loss: 0.886209, acc.: 56.25%] [G loss: 0.014711] [C loss: 0.816756]\n",
      "147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013377] [C loss: 0.789204]\n",
      "148 [D loss: 0.818717, acc.: 50.00%] [G loss: 0.014746] [C loss: 0.749572]\n",
      "149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020716] [C loss: 0.803168]\n",
      "150 [D loss: 0.792785, acc.: 56.25%] [G loss: 0.012806] [C loss: 0.755398]\n",
      "151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011871] [C loss: 0.933137]\n",
      "152 [D loss: 0.871939, acc.: 50.00%] [G loss: 0.010907] [C loss: 0.763345]\n",
      "153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016150] [C loss: 0.896556]\n",
      "154 [D loss: 0.865118, acc.: 43.75%] [G loss: 0.011295] [C loss: 0.842652]\n",
      "155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013541] [C loss: 0.855627]\n",
      "156 [D loss: 0.788500, acc.: 50.00%] [G loss: 0.013460] [C loss: 0.775930]\n",
      "157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016360] [C loss: 0.956666]\n",
      "158 [D loss: 0.929082, acc.: 37.50%] [G loss: 0.010232] [C loss: 0.874806]\n",
      "159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011674] [C loss: 1.029658]\n",
      "160 [D loss: 0.870896, acc.: 43.75%] [G loss: 0.014775] [C loss: 0.826151]\n",
      "161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008912] [C loss: 0.778399]\n",
      "162 [D loss: 0.834589, acc.: 31.25%] [G loss: 0.013431] [C loss: 0.810118]\n",
      "163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010508] [C loss: 0.856324]\n",
      "164 [D loss: 0.791727, acc.: 56.25%] [G loss: 0.013133] [C loss: 0.790093]\n",
      "165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012260] [C loss: 0.737785]\n",
      "166 [D loss: 0.793731, acc.: 62.50%] [G loss: 0.010839] [C loss: 0.752957]\n",
      "167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010168] [C loss: 0.766404]\n",
      "168 [D loss: 0.834733, acc.: 56.25%] [G loss: 0.014113] [C loss: 0.789851]\n",
      "169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017429] [C loss: 0.773515]\n",
      "170 [D loss: 0.783743, acc.: 50.00%] [G loss: 0.016371] [C loss: 0.728873]\n",
      "171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015312] [C loss: 0.856271]\n",
      "172 [D loss: 0.805054, acc.: 50.00%] [G loss: 0.015652] [C loss: 0.790023]\n",
      "173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011871] [C loss: 0.816458]\n",
      "174 [D loss: 0.857636, acc.: 56.25%] [G loss: 0.012114] [C loss: 0.818638]\n",
      "175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012603] [C loss: 0.824503]\n",
      "176 [D loss: 0.766499, acc.: 62.50%] [G loss: 0.011917] [C loss: 0.752204]\n",
      "177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012613] [C loss: 0.830895]\n",
      "178 [D loss: 0.823201, acc.: 37.50%] [G loss: 0.009125] [C loss: 0.759019]\n",
      "179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010105] [C loss: 0.791299]\n",
      "180 [D loss: 0.796632, acc.: 50.00%] [G loss: 0.014090] [C loss: 0.780455]\n",
      "181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016258] [C loss: 0.889843]\n",
      "182 [D loss: 0.797771, acc.: 50.00%] [G loss: 0.010712] [C loss: 0.747752]\n",
      "183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014613] [C loss: 0.750686]\n",
      "184 [D loss: 0.796770, acc.: 50.00%] [G loss: 0.012670] [C loss: 0.731030]\n",
      "185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010509] [C loss: 0.748709]\n",
      "186 [D loss: 0.817037, acc.: 62.50%] [G loss: 0.010774] [C loss: 0.751984]\n",
      "187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010531] [C loss: 0.731464]\n",
      "188 [D loss: 0.765964, acc.: 37.50%] [G loss: 0.012609] [C loss: 0.748246]\n",
      "189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009320] [C loss: 0.729648]\n",
      "190 [D loss: 0.770492, acc.: 43.75%] [G loss: 0.011266] [C loss: 0.754386]\n",
      "191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011153] [C loss: 0.744321]\n",
      "192 [D loss: 0.966300, acc.: 62.50%] [G loss: 0.015104] [C loss: 0.891139]\n",
      "193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014270] [C loss: 0.980128]\n",
      "194 [D loss: 0.916082, acc.: 62.50%] [G loss: 0.015409] [C loss: 0.777820]\n",
      "195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009081] [C loss: 0.881490]\n",
      "196 [D loss: 0.831474, acc.: 43.75%] [G loss: 0.010940] [C loss: 0.807252]\n",
      "197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014115] [C loss: 0.939941]\n",
      "198 [D loss: 0.911938, acc.: 43.75%] [G loss: 0.026701] [C loss: 0.878804]\n",
      "199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012441] [C loss: 0.956911]\n",
      "200 [D loss: 0.826775, acc.: 50.00%] [G loss: 0.011999] [C loss: 0.759571]\n",
      "201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009079] [C loss: 0.947895]\n",
      "202 [D loss: 0.897395, acc.: 56.25%] [G loss: 0.012901] [C loss: 0.939193]\n",
      "203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008368] [C loss: 1.204129]\n",
      "204 [D loss: 1.062751, acc.: 56.25%] [G loss: 0.017174] [C loss: 0.961142]\n",
      "205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013915] [C loss: 0.833971]\n",
      "206 [D loss: 0.881711, acc.: 50.00%] [G loss: 0.010833] [C loss: 0.775015]\n",
      "207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013192] [C loss: 0.827110]\n",
      "208 [D loss: 0.780353, acc.: 56.25%] [G loss: 0.012543] [C loss: 0.938409]\n",
      "209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.025299] [C loss: 0.882735]\n",
      "210 [D loss: 0.866809, acc.: 50.00%] [G loss: 0.012291] [C loss: 0.807029]\n",
      "211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010726] [C loss: 1.002090]\n",
      "212 [D loss: 0.925968, acc.: 50.00%] [G loss: 0.011871] [C loss: 0.809286]\n",
      "213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010006] [C loss: 0.829734]\n",
      "214 [D loss: 0.845257, acc.: 56.25%] [G loss: 0.011224] [C loss: 0.906823]\n",
      "215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011918] [C loss: 0.882983]\n",
      "216 [D loss: 0.926533, acc.: 62.50%] [G loss: 0.012652] [C loss: 0.783258]\n",
      "217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013174] [C loss: 0.991223]\n",
      "218 [D loss: 0.973601, acc.: 43.75%] [G loss: 0.013746] [C loss: 0.926625]\n",
      "219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010079] [C loss: 0.817198]\n",
      "220 [D loss: 0.909731, acc.: 56.25%] [G loss: 0.013677] [C loss: 0.716875]\n",
      "221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008543] [C loss: 1.026459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 [D loss: 0.904926, acc.: 43.75%] [G loss: 0.010673] [C loss: 0.810794]\n",
      "223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009869] [C loss: 0.879747]\n",
      "224 [D loss: 0.809239, acc.: 56.25%] [G loss: 0.011888] [C loss: 0.765745]\n",
      "225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013020] [C loss: 0.763656]\n",
      "226 [D loss: 0.872645, acc.: 37.50%] [G loss: 0.014350] [C loss: 0.908266]\n",
      "227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009797] [C loss: 1.003183]\n",
      "228 [D loss: 0.856265, acc.: 43.75%] [G loss: 0.009353] [C loss: 0.878975]\n",
      "229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008958] [C loss: 1.041059]\n",
      "230 [D loss: 0.894331, acc.: 50.00%] [G loss: 0.010394] [C loss: 0.813949]\n",
      "231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012456] [C loss: 0.899333]\n",
      "232 [D loss: 1.030101, acc.: 37.50%] [G loss: 0.014539] [C loss: 0.931491]\n",
      "233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015559] [C loss: 0.936323]\n",
      "234 [D loss: 0.940906, acc.: 50.00%] [G loss: 0.011232] [C loss: 1.006791]\n",
      "235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011372] [C loss: 0.861130]\n",
      "236 [D loss: 0.923895, acc.: 43.75%] [G loss: 0.018029] [C loss: 0.810398]\n",
      "237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011876] [C loss: 0.869850]\n",
      "238 [D loss: 0.968386, acc.: 43.75%] [G loss: 0.010085] [C loss: 0.858000]\n",
      "239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009860] [C loss: 1.100539]\n",
      "240 [D loss: 1.009271, acc.: 50.00%] [G loss: 0.012067] [C loss: 0.780071]\n",
      "241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012206] [C loss: 1.014839]\n",
      "242 [D loss: 0.943896, acc.: 50.00%] [G loss: 0.012929] [C loss: 0.789354]\n",
      "243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010078] [C loss: 0.781554]\n",
      "244 [D loss: 0.937499, acc.: 62.50%] [G loss: 0.010588] [C loss: 0.861153]\n",
      "245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012209] [C loss: 0.966007]\n",
      "246 [D loss: 0.944618, acc.: 56.25%] [G loss: 0.012129] [C loss: 0.853956]\n",
      "247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012335] [C loss: 1.145589]\n",
      "248 [D loss: 0.817873, acc.: 31.25%] [G loss: 0.012558] [C loss: 0.779669]\n",
      "249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013200] [C loss: 0.865203]\n",
      "250 [D loss: 0.783125, acc.: 56.25%] [G loss: 0.007629] [C loss: 0.764243]\n",
      "251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006275] [C loss: 0.813150]\n",
      "252 [D loss: 0.775271, acc.: 43.75%] [G loss: 0.011817] [C loss: 0.762211]\n",
      "253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014020] [C loss: 0.878595]\n",
      "254 [D loss: 0.935349, acc.: 37.50%] [G loss: 0.012727] [C loss: 0.815461]\n",
      "255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008218] [C loss: 0.913846]\n",
      "256 [D loss: 0.840359, acc.: 62.50%] [G loss: 0.011425] [C loss: 0.768271]\n",
      "257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009082] [C loss: 0.813028]\n",
      "258 [D loss: 0.808557, acc.: 56.25%] [G loss: 0.010313] [C loss: 0.752615]\n",
      "259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009327] [C loss: 0.753416]\n",
      "260 [D loss: 0.729795, acc.: 37.50%] [G loss: 0.014286] [C loss: 0.745282]\n",
      "261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013048] [C loss: 0.804880]\n",
      "262 [D loss: 0.799761, acc.: 56.25%] [G loss: 0.011626] [C loss: 0.963666]\n",
      "263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011152] [C loss: 0.732391]\n",
      "264 [D loss: 0.732992, acc.: 43.75%] [G loss: 0.008201] [C loss: 0.731269]\n",
      "265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011080] [C loss: 0.826100]\n",
      "266 [D loss: 0.885681, acc.: 37.50%] [G loss: 0.009622] [C loss: 0.747497]\n",
      "267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009983] [C loss: 0.786214]\n",
      "268 [D loss: 0.838586, acc.: 56.25%] [G loss: 0.009236] [C loss: 0.803336]\n",
      "269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013375] [C loss: 0.849108]\n",
      "270 [D loss: 0.816971, acc.: 43.75%] [G loss: 0.013009] [C loss: 0.728079]\n",
      "271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011231] [C loss: 0.845404]\n",
      "272 [D loss: 0.792333, acc.: 50.00%] [G loss: 0.010615] [C loss: 0.797335]\n",
      "273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012007] [C loss: 0.780887]\n",
      "274 [D loss: 0.888527, acc.: 43.75%] [G loss: 0.007815] [C loss: 0.758804]\n",
      "275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009101] [C loss: 0.739653]\n",
      "276 [D loss: 0.774325, acc.: 50.00%] [G loss: 0.016613] [C loss: 0.753935]\n",
      "277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016207] [C loss: 0.782608]\n",
      "278 [D loss: 0.740194, acc.: 56.25%] [G loss: 0.006821] [C loss: 0.827111]\n",
      "279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011041] [C loss: 0.833954]\n",
      "280 [D loss: 0.859269, acc.: 50.00%] [G loss: 0.010801] [C loss: 0.859210]\n",
      "281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008228] [C loss: 0.816037]\n",
      "282 [D loss: 0.814450, acc.: 56.25%] [G loss: 0.013986] [C loss: 0.779474]\n",
      "283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008202] [C loss: 0.818202]\n",
      "284 [D loss: 0.855644, acc.: 50.00%] [G loss: 0.011066] [C loss: 0.806671]\n",
      "285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009011] [C loss: 0.869275]\n",
      "286 [D loss: 0.828248, acc.: 68.75%] [G loss: 0.008068] [C loss: 0.760848]\n",
      "287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007921] [C loss: 0.792205]\n",
      "288 [D loss: 0.805711, acc.: 43.75%] [G loss: 0.009371] [C loss: 0.785039]\n",
      "289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013261] [C loss: 0.761025]\n",
      "290 [D loss: 0.742152, acc.: 37.50%] [G loss: 0.010131] [C loss: 0.736713]\n",
      "291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009253] [C loss: 0.746002]\n",
      "292 [D loss: 0.745443, acc.: 43.75%] [G loss: 0.008813] [C loss: 0.762330]\n",
      "293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010239] [C loss: 0.801428]\n",
      "294 [D loss: 0.806110, acc.: 50.00%] [G loss: 0.005800] [C loss: 0.737094]\n",
      "295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010040] [C loss: 0.792281]\n",
      "296 [D loss: 0.880155, acc.: 31.25%] [G loss: 0.020128] [C loss: 0.737918]\n",
      "297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008137] [C loss: 0.759805]\n",
      "298 [D loss: 0.829402, acc.: 43.75%] [G loss: 0.011324] [C loss: 0.738347]\n",
      "299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008416] [C loss: 0.712461]\n",
      "300 [D loss: 0.750996, acc.: 37.50%] [G loss: 0.013541] [C loss: 0.746046]\n",
      "301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017160] [C loss: 0.744930]\n",
      "302 [D loss: 0.768304, acc.: 50.00%] [G loss: 0.014193] [C loss: 0.756630]\n",
      "303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010276] [C loss: 0.755159]\n",
      "304 [D loss: 0.806993, acc.: 68.75%] [G loss: 0.009962] [C loss: 0.710992]\n",
      "305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011236] [C loss: 0.720762]\n",
      "306 [D loss: 0.783426, acc.: 50.00%] [G loss: 0.010586] [C loss: 0.731921]\n",
      "307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009483] [C loss: 0.736098]\n",
      "308 [D loss: 0.776587, acc.: 37.50%] [G loss: 0.010375] [C loss: 0.747505]\n",
      "309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015481] [C loss: 0.804273]\n",
      "310 [D loss: 0.830221, acc.: 56.25%] [G loss: 0.012474] [C loss: 0.754141]\n",
      "311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008199] [C loss: 0.756705]\n",
      "312 [D loss: 0.762555, acc.: 56.25%] [G loss: 0.009020] [C loss: 0.768247]\n",
      "313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010988] [C loss: 0.836591]\n",
      "314 [D loss: 0.782721, acc.: 62.50%] [G loss: 0.008611] [C loss: 0.899531]\n",
      "315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011239] [C loss: 0.904192]\n",
      "316 [D loss: 0.819674, acc.: 43.75%] [G loss: 0.013939] [C loss: 0.801386]\n",
      "317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009411] [C loss: 0.783034]\n",
      "318 [D loss: 0.752790, acc.: 50.00%] [G loss: 0.013515] [C loss: 0.771435]\n",
      "319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009714] [C loss: 0.852806]\n",
      "320 [D loss: 0.771551, acc.: 50.00%] [G loss: 0.009851] [C loss: 0.735056]\n",
      "321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010394] [C loss: 0.807248]\n",
      "322 [D loss: 0.880634, acc.: 50.00%] [G loss: 0.008921] [C loss: 0.832113]\n",
      "323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008988] [C loss: 0.759738]\n",
      "324 [D loss: 0.802643, acc.: 56.25%] [G loss: 0.011145] [C loss: 0.735111]\n",
      "325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007630] [C loss: 0.793687]\n",
      "326 [D loss: 0.819898, acc.: 62.50%] [G loss: 0.011692] [C loss: 0.849551]\n",
      "327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008767] [C loss: 0.762861]\n",
      "328 [D loss: 0.774997, acc.: 56.25%] [G loss: 0.008552] [C loss: 0.765689]\n",
      "329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008726] [C loss: 0.855562]\n",
      "330 [D loss: 0.770648, acc.: 50.00%] [G loss: 0.011245] [C loss: 0.789050]\n",
      "331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007913] [C loss: 0.765476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332 [D loss: 0.797302, acc.: 56.25%] [G loss: 0.007532] [C loss: 0.730813]\n",
      "333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011131] [C loss: 0.737861]\n",
      "334 [D loss: 0.801744, acc.: 37.50%] [G loss: 0.011687] [C loss: 0.753614]\n",
      "335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022249] [C loss: 0.772365]\n",
      "336 [D loss: 0.778511, acc.: 43.75%] [G loss: 0.008825] [C loss: 0.753961]\n",
      "337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008436] [C loss: 0.742647]\n",
      "338 [D loss: 0.767202, acc.: 37.50%] [G loss: 0.012979] [C loss: 0.765080]\n",
      "339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011261] [C loss: 0.773913]\n",
      "340 [D loss: 0.798169, acc.: 43.75%] [G loss: 0.008337] [C loss: 0.772697]\n",
      "341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015821] [C loss: 0.832234]\n",
      "342 [D loss: 0.787067, acc.: 56.25%] [G loss: 0.014360] [C loss: 0.797877]\n",
      "343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007484] [C loss: 0.819704]\n",
      "344 [D loss: 0.800640, acc.: 37.50%] [G loss: 0.011848] [C loss: 0.723264]\n",
      "345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011175] [C loss: 0.767131]\n",
      "346 [D loss: 0.773468, acc.: 56.25%] [G loss: 0.009557] [C loss: 0.802835]\n",
      "347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010106] [C loss: 0.881623]\n",
      "348 [D loss: 0.907170, acc.: 56.25%] [G loss: 0.009632] [C loss: 0.783819]\n",
      "349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010953] [C loss: 0.867985]\n",
      "350 [D loss: 0.758600, acc.: 56.25%] [G loss: 0.008831] [C loss: 0.779098]\n",
      "351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007154] [C loss: 0.745674]\n",
      "352 [D loss: 0.755609, acc.: 50.00%] [G loss: 0.007897] [C loss: 0.737811]\n",
      "353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008741] [C loss: 0.923759]\n",
      "354 [D loss: 0.788175, acc.: 43.75%] [G loss: 0.010301] [C loss: 0.794204]\n",
      "355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012177] [C loss: 1.096658]\n",
      "356 [D loss: 0.889237, acc.: 37.50%] [G loss: 0.010374] [C loss: 0.811958]\n",
      "357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008775] [C loss: 0.940947]\n",
      "358 [D loss: 0.758081, acc.: 50.00%] [G loss: 0.007106] [C loss: 0.838911]\n",
      "359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006979] [C loss: 0.922603]\n",
      "360 [D loss: 0.966106, acc.: 56.25%] [G loss: 0.012918] [C loss: 0.859298]\n",
      "361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008341] [C loss: 0.779561]\n",
      "362 [D loss: 0.832727, acc.: 50.00%] [G loss: 0.012457] [C loss: 0.772977]\n",
      "363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017524] [C loss: 0.780054]\n",
      "364 [D loss: 0.777109, acc.: 43.75%] [G loss: 0.009632] [C loss: 0.822451]\n",
      "365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014001] [C loss: 0.954522]\n",
      "366 [D loss: 0.868438, acc.: 50.00%] [G loss: 0.010048] [C loss: 0.883905]\n",
      "367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010964] [C loss: 1.074065]\n",
      "368 [D loss: 0.847012, acc.: 37.50%] [G loss: 0.011012] [C loss: 0.765313]\n",
      "369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008956] [C loss: 0.775283]\n",
      "370 [D loss: 0.858376, acc.: 50.00%] [G loss: 0.009625] [C loss: 0.841898]\n",
      "371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009779] [C loss: 0.764052]\n",
      "372 [D loss: 0.820716, acc.: 43.75%] [G loss: 0.007280] [C loss: 0.792284]\n",
      "373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009698] [C loss: 0.767842]\n",
      "374 [D loss: 0.769192, acc.: 43.75%] [G loss: 0.009793] [C loss: 0.732590]\n",
      "375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010760] [C loss: 0.745212]\n",
      "376 [D loss: 0.814502, acc.: 50.00%] [G loss: 0.009189] [C loss: 0.785974]\n",
      "377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012663] [C loss: 0.756260]\n",
      "378 [D loss: 0.826390, acc.: 56.25%] [G loss: 0.009405] [C loss: 0.757744]\n",
      "379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008783] [C loss: 0.748963]\n",
      "380 [D loss: 0.791974, acc.: 31.25%] [G loss: 0.008332] [C loss: 0.771181]\n",
      "381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010060] [C loss: 0.744181]\n",
      "382 [D loss: 0.834810, acc.: 37.50%] [G loss: 0.008351] [C loss: 0.694240]\n",
      "383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007222] [C loss: 0.689986]\n",
      "384 [D loss: 0.782831, acc.: 50.00%] [G loss: 0.010320] [C loss: 0.770451]\n",
      "385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008465] [C loss: 0.893832]\n",
      "386 [D loss: 0.762069, acc.: 43.75%] [G loss: 0.011423] [C loss: 0.776863]\n",
      "387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007528] [C loss: 0.855459]\n",
      "388 [D loss: 0.875994, acc.: 56.25%] [G loss: 0.017465] [C loss: 0.738698]\n",
      "389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007992] [C loss: 0.735301]\n",
      "390 [D loss: 0.807989, acc.: 37.50%] [G loss: 0.011013] [C loss: 0.755431]\n",
      "391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.029066] [C loss: 0.808308]\n",
      "392 [D loss: 0.819793, acc.: 43.75%] [G loss: 0.009668] [C loss: 0.717723]\n",
      "393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013009] [C loss: 0.801586]\n",
      "394 [D loss: 0.823021, acc.: 43.75%] [G loss: 0.011113] [C loss: 0.723457]\n",
      "395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010268] [C loss: 0.815859]\n",
      "396 [D loss: 0.790196, acc.: 68.75%] [G loss: 0.012227] [C loss: 0.817680]\n",
      "397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011254] [C loss: 0.788072]\n",
      "398 [D loss: 0.797874, acc.: 37.50%] [G loss: 0.008382] [C loss: 0.720290]\n",
      "399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014378] [C loss: 0.816167]\n",
      "400 [D loss: 0.865553, acc.: 43.75%] [G loss: 0.014429] [C loss: 0.813548]\n",
      "401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.025827] [C loss: 0.872839]\n",
      "402 [D loss: 0.821637, acc.: 37.50%] [G loss: 0.010078] [C loss: 0.761243]\n",
      "403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009486] [C loss: 0.831606]\n",
      "404 [D loss: 0.764776, acc.: 43.75%] [G loss: 0.010013] [C loss: 0.755090]\n",
      "405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009762] [C loss: 0.915239]\n",
      "406 [D loss: 0.788776, acc.: 50.00%] [G loss: 0.011477] [C loss: 0.749248]\n",
      "407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009026] [C loss: 0.841835]\n",
      "408 [D loss: 0.877812, acc.: 43.75%] [G loss: 0.014888] [C loss: 0.817292]\n",
      "409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009011] [C loss: 0.910805]\n",
      "410 [D loss: 0.839092, acc.: 56.25%] [G loss: 0.010197] [C loss: 0.749698]\n",
      "411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009953] [C loss: 0.771414]\n",
      "412 [D loss: 0.871909, acc.: 56.25%] [G loss: 0.009210] [C loss: 0.799726]\n",
      "413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007510] [C loss: 0.790196]\n",
      "414 [D loss: 0.791148, acc.: 43.75%] [G loss: 0.011909] [C loss: 0.810756]\n",
      "415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009455] [C loss: 0.911425]\n",
      "416 [D loss: 0.794735, acc.: 50.00%] [G loss: 0.008088] [C loss: 0.754565]\n",
      "417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010716] [C loss: 0.789521]\n",
      "418 [D loss: 0.823906, acc.: 37.50%] [G loss: 0.007029] [C loss: 0.825883]\n",
      "419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009029] [C loss: 0.824807]\n",
      "420 [D loss: 0.784174, acc.: 62.50%] [G loss: 0.010911] [C loss: 0.793125]\n",
      "421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011752] [C loss: 0.814587]\n",
      "422 [D loss: 0.830446, acc.: 43.75%] [G loss: 0.011728] [C loss: 0.817854]\n",
      "423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008625] [C loss: 0.789675]\n",
      "424 [D loss: 0.787218, acc.: 56.25%] [G loss: 0.010230] [C loss: 0.721828]\n",
      "425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009962] [C loss: 0.790311]\n",
      "426 [D loss: 0.792902, acc.: 43.75%] [G loss: 0.012457] [C loss: 0.777694]\n",
      "427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010114] [C loss: 0.753605]\n",
      "428 [D loss: 0.838824, acc.: 56.25%] [G loss: 0.012007] [C loss: 0.753891]\n",
      "429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008195] [C loss: 0.883448]\n",
      "430 [D loss: 0.762723, acc.: 43.75%] [G loss: 0.009300] [C loss: 0.751321]\n",
      "431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007683] [C loss: 0.741251]\n",
      "432 [D loss: 0.756458, acc.: 43.75%] [G loss: 0.006968] [C loss: 0.755384]\n",
      "433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010485] [C loss: 0.924690]\n",
      "434 [D loss: 0.832745, acc.: 43.75%] [G loss: 0.008473] [C loss: 0.815971]\n",
      "435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007311] [C loss: 0.710466]\n",
      "436 [D loss: 0.760871, acc.: 50.00%] [G loss: 0.007844] [C loss: 0.741431]\n",
      "437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008316] [C loss: 0.737033]\n",
      "438 [D loss: 0.796688, acc.: 50.00%] [G loss: 0.011496] [C loss: 0.732859]\n",
      "439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009436] [C loss: 0.761684]\n",
      "440 [D loss: 0.805955, acc.: 56.25%] [G loss: 0.011556] [C loss: 0.802926]\n",
      "441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009707] [C loss: 0.756258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 [D loss: 0.840924, acc.: 50.00%] [G loss: 0.011738] [C loss: 0.742006]\n",
      "443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010271] [C loss: 0.790789]\n",
      "444 [D loss: 0.774412, acc.: 50.00%] [G loss: 0.012055] [C loss: 0.812094]\n",
      "445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006431] [C loss: 0.723645]\n",
      "446 [D loss: 0.844526, acc.: 62.50%] [G loss: 0.014582] [C loss: 0.741034]\n",
      "447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009013] [C loss: 0.758736]\n",
      "448 [D loss: 0.751173, acc.: 43.75%] [G loss: 0.008206] [C loss: 0.816396]\n",
      "449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010070] [C loss: 0.759019]\n",
      "450 [D loss: 0.794538, acc.: 56.25%] [G loss: 0.007264] [C loss: 0.742890]\n",
      "451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007647] [C loss: 0.716354]\n",
      "452 [D loss: 0.751816, acc.: 56.25%] [G loss: 0.010177] [C loss: 0.738742]\n",
      "453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009931] [C loss: 0.775949]\n",
      "454 [D loss: 0.762602, acc.: 50.00%] [G loss: 0.008092] [C loss: 0.717282]\n",
      "455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008266] [C loss: 0.745474]\n",
      "456 [D loss: 0.753549, acc.: 50.00%] [G loss: 0.008912] [C loss: 0.727533]\n",
      "457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009559] [C loss: 0.780728]\n",
      "458 [D loss: 0.785565, acc.: 37.50%] [G loss: 0.007954] [C loss: 0.715188]\n",
      "459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008930] [C loss: 0.735613]\n",
      "460 [D loss: 0.744581, acc.: 50.00%] [G loss: 0.006509] [C loss: 0.724795]\n",
      "461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009261] [C loss: 0.749367]\n",
      "462 [D loss: 0.767130, acc.: 43.75%] [G loss: 0.006217] [C loss: 0.728356]\n",
      "463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006549] [C loss: 0.751401]\n",
      "464 [D loss: 0.724959, acc.: 43.75%] [G loss: 0.008633] [C loss: 0.734429]\n",
      "465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009429] [C loss: 0.735854]\n",
      "466 [D loss: 0.749762, acc.: 43.75%] [G loss: 0.008382] [C loss: 0.709375]\n",
      "467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008570] [C loss: 0.733784]\n",
      "468 [D loss: 0.744939, acc.: 43.75%] [G loss: 0.011111] [C loss: 0.719766]\n",
      "469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009010] [C loss: 0.745954]\n",
      "470 [D loss: 0.742085, acc.: 56.25%] [G loss: 0.006504] [C loss: 0.708288]\n",
      "471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007558] [C loss: 0.819207]\n",
      "472 [D loss: 0.769829, acc.: 50.00%] [G loss: 0.006411] [C loss: 0.741734]\n",
      "473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009447] [C loss: 0.800759]\n",
      "474 [D loss: 0.769783, acc.: 50.00%] [G loss: 0.010996] [C loss: 0.724558]\n",
      "475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008557] [C loss: 0.725132]\n",
      "476 [D loss: 0.769247, acc.: 50.00%] [G loss: 0.008025] [C loss: 0.734914]\n",
      "477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005963] [C loss: 0.790673]\n",
      "478 [D loss: 0.752702, acc.: 50.00%] [G loss: 0.008279] [C loss: 0.723520]\n",
      "479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010847] [C loss: 0.763318]\n",
      "480 [D loss: 0.779336, acc.: 43.75%] [G loss: 0.010341] [C loss: 0.766739]\n",
      "481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006792] [C loss: 0.824894]\n",
      "482 [D loss: 0.781445, acc.: 50.00%] [G loss: 0.007180] [C loss: 0.765860]\n",
      "483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007873] [C loss: 0.764314]\n",
      "484 [D loss: 0.727993, acc.: 50.00%] [G loss: 0.008947] [C loss: 0.716143]\n",
      "485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006344] [C loss: 0.745336]\n",
      "486 [D loss: 0.740039, acc.: 43.75%] [G loss: 0.006637] [C loss: 0.717028]\n",
      "487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006177] [C loss: 0.748657]\n",
      "488 [D loss: 0.715610, acc.: 50.00%] [G loss: 0.013694] [C loss: 0.717651]\n",
      "489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008077] [C loss: 0.794788]\n",
      "490 [D loss: 0.758203, acc.: 50.00%] [G loss: 0.008069] [C loss: 0.712295]\n",
      "491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008124] [C loss: 0.800420]\n",
      "492 [D loss: 0.744945, acc.: 43.75%] [G loss: 0.009280] [C loss: 0.752064]\n",
      "493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008361] [C loss: 0.820534]\n",
      "494 [D loss: 0.778678, acc.: 43.75%] [G loss: 0.009026] [C loss: 0.733171]\n",
      "495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008001] [C loss: 0.721943]\n",
      "496 [D loss: 0.720061, acc.: 43.75%] [G loss: 0.011398] [C loss: 0.699036]\n",
      "497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007488] [C loss: 0.779985]\n",
      "498 [D loss: 0.770267, acc.: 50.00%] [G loss: 0.009569] [C loss: 0.736173]\n",
      "499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005869] [C loss: 0.718735]\n",
      "500 [D loss: 0.786232, acc.: 50.00%] [G loss: 0.007232] [C loss: 0.794981]\n",
      "501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005843] [C loss: 0.732212]\n",
      "502 [D loss: 0.751249, acc.: 56.25%] [G loss: 0.010694] [C loss: 0.732760]\n",
      "503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009237] [C loss: 0.725725]\n",
      "504 [D loss: 0.722610, acc.: 50.00%] [G loss: 0.015918] [C loss: 0.715353]\n",
      "505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005656] [C loss: 0.726014]\n",
      "506 [D loss: 0.735815, acc.: 37.50%] [G loss: 0.009201] [C loss: 0.725611]\n",
      "507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007293] [C loss: 0.754770]\n",
      "508 [D loss: 0.730105, acc.: 37.50%] [G loss: 0.012600] [C loss: 0.718064]\n",
      "509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.738400]\n",
      "510 [D loss: 0.753277, acc.: 43.75%] [G loss: 0.008227] [C loss: 0.718066]\n",
      "511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012386] [C loss: 0.733963]\n",
      "512 [D loss: 0.731972, acc.: 37.50%] [G loss: 0.010387] [C loss: 0.710609]\n",
      "513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008902] [C loss: 0.735527]\n",
      "514 [D loss: 0.787914, acc.: 43.75%] [G loss: 0.007813] [C loss: 0.755953]\n",
      "515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009439] [C loss: 0.768328]\n",
      "516 [D loss: 0.768895, acc.: 50.00%] [G loss: 0.010676] [C loss: 0.704114]\n",
      "517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010473] [C loss: 0.762468]\n",
      "518 [D loss: 0.740629, acc.: 50.00%] [G loss: 0.009325] [C loss: 0.739458]\n",
      "519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008369] [C loss: 0.731286]\n",
      "520 [D loss: 0.726631, acc.: 56.25%] [G loss: 0.007729] [C loss: 0.729662]\n",
      "521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007401] [C loss: 0.750478]\n",
      "522 [D loss: 0.764070, acc.: 56.25%] [G loss: 0.007503] [C loss: 0.753756]\n",
      "523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009468] [C loss: 0.934656]\n",
      "524 [D loss: 0.818589, acc.: 50.00%] [G loss: 0.008300] [C loss: 0.732028]\n",
      "525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009672] [C loss: 0.729899]\n",
      "526 [D loss: 0.733392, acc.: 50.00%] [G loss: 0.006005] [C loss: 0.725677]\n",
      "527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006545] [C loss: 0.740427]\n",
      "528 [D loss: 0.732206, acc.: 50.00%] [G loss: 0.012354] [C loss: 0.746049]\n",
      "529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023634] [C loss: 0.745452]\n",
      "530 [D loss: 0.768596, acc.: 56.25%] [G loss: 0.006480] [C loss: 0.726287]\n",
      "531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011340] [C loss: 0.807516]\n",
      "532 [D loss: 0.762321, acc.: 62.50%] [G loss: 0.010654] [C loss: 0.762720]\n",
      "533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008979] [C loss: 0.737166]\n",
      "534 [D loss: 0.739692, acc.: 50.00%] [G loss: 0.008258] [C loss: 0.725759]\n",
      "535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009910] [C loss: 0.754475]\n",
      "536 [D loss: 0.790827, acc.: 43.75%] [G loss: 0.012931] [C loss: 0.713545]\n",
      "537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008578] [C loss: 0.741694]\n",
      "538 [D loss: 0.829490, acc.: 56.25%] [G loss: 0.007249] [C loss: 0.745734]\n",
      "539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010067] [C loss: 0.741207]\n",
      "540 [D loss: 0.734123, acc.: 43.75%] [G loss: 0.014047] [C loss: 0.718458]\n",
      "541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011683] [C loss: 0.813639]\n",
      "542 [D loss: 0.759841, acc.: 62.50%] [G loss: 0.011746] [C loss: 0.736040]\n",
      "543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009086] [C loss: 0.752099]\n",
      "544 [D loss: 0.725594, acc.: 68.75%] [G loss: 0.007691] [C loss: 0.710302]\n",
      "545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006744] [C loss: 0.708497]\n",
      "546 [D loss: 0.753739, acc.: 50.00%] [G loss: 0.006313] [C loss: 0.727818]\n",
      "547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006091] [C loss: 0.768105]\n",
      "548 [D loss: 0.769527, acc.: 56.25%] [G loss: 0.006538] [C loss: 0.731697]\n",
      "549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008702] [C loss: 0.785331]\n",
      "550 [D loss: 0.716275, acc.: 43.75%] [G loss: 0.007599] [C loss: 0.712896]\n",
      "551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009917] [C loss: 0.764334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552 [D loss: 0.756090, acc.: 50.00%] [G loss: 0.007460] [C loss: 0.751030]\n",
      "553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007680] [C loss: 0.774030]\n",
      "554 [D loss: 0.759334, acc.: 43.75%] [G loss: 0.009199] [C loss: 0.746316]\n",
      "555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010865] [C loss: 0.741903]\n",
      "556 [D loss: 0.817465, acc.: 50.00%] [G loss: 0.007635] [C loss: 0.757043]\n",
      "557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008565] [C loss: 0.766026]\n",
      "558 [D loss: 0.778245, acc.: 50.00%] [G loss: 0.008620] [C loss: 0.753732]\n",
      "559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008574] [C loss: 0.730077]\n",
      "560 [D loss: 0.730986, acc.: 37.50%] [G loss: 0.011288] [C loss: 0.710623]\n",
      "561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011523] [C loss: 0.712428]\n",
      "562 [D loss: 0.748011, acc.: 37.50%] [G loss: 0.007833] [C loss: 0.744757]\n",
      "563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007201] [C loss: 0.767029]\n",
      "564 [D loss: 0.776277, acc.: 50.00%] [G loss: 0.007640] [C loss: 0.728121]\n",
      "565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008368] [C loss: 0.734266]\n",
      "566 [D loss: 0.750378, acc.: 43.75%] [G loss: 0.007159] [C loss: 0.727043]\n",
      "567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007372] [C loss: 0.746382]\n",
      "568 [D loss: 0.748847, acc.: 56.25%] [G loss: 0.009119] [C loss: 0.725859]\n",
      "569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007895] [C loss: 0.761125]\n",
      "570 [D loss: 0.793964, acc.: 43.75%] [G loss: 0.006482] [C loss: 0.723998]\n",
      "571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022740] [C loss: 0.738733]\n",
      "572 [D loss: 0.754230, acc.: 56.25%] [G loss: 0.011811] [C loss: 0.760859]\n",
      "573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010796] [C loss: 0.749633]\n",
      "574 [D loss: 0.735869, acc.: 62.50%] [G loss: 0.007815] [C loss: 0.719578]\n",
      "575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007992] [C loss: 0.710832]\n",
      "576 [D loss: 0.756241, acc.: 56.25%] [G loss: 0.009811] [C loss: 0.744864]\n",
      "577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022423] [C loss: 0.765132]\n",
      "578 [D loss: 0.772594, acc.: 56.25%] [G loss: 0.006117] [C loss: 0.722183]\n",
      "579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007469] [C loss: 0.814980]\n",
      "580 [D loss: 0.743100, acc.: 62.50%] [G loss: 0.009029] [C loss: 0.710741]\n",
      "581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011567] [C loss: 0.760304]\n",
      "582 [D loss: 0.749470, acc.: 56.25%] [G loss: 0.008988] [C loss: 0.734249]\n",
      "583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008923] [C loss: 0.821869]\n",
      "584 [D loss: 0.745154, acc.: 43.75%] [G loss: 0.008761] [C loss: 0.730153]\n",
      "585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009426] [C loss: 0.722721]\n",
      "586 [D loss: 0.722229, acc.: 43.75%] [G loss: 0.008208] [C loss: 0.723049]\n",
      "587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008077] [C loss: 0.796431]\n",
      "588 [D loss: 0.770360, acc.: 37.50%] [G loss: 0.011638] [C loss: 0.757535]\n",
      "589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011787] [C loss: 0.795724]\n",
      "590 [D loss: 0.798980, acc.: 56.25%] [G loss: 0.006945] [C loss: 0.771219]\n",
      "591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008175] [C loss: 0.858535]\n",
      "592 [D loss: 0.796314, acc.: 50.00%] [G loss: 0.010424] [C loss: 0.737923]\n",
      "593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012671] [C loss: 0.746048]\n",
      "594 [D loss: 0.810171, acc.: 62.50%] [G loss: 0.007772] [C loss: 0.730904]\n",
      "595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007403] [C loss: 0.814780]\n",
      "596 [D loss: 0.713010, acc.: 56.25%] [G loss: 0.009937] [C loss: 0.714816]\n",
      "597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007681] [C loss: 0.766213]\n",
      "598 [D loss: 0.733670, acc.: 50.00%] [G loss: 0.009106] [C loss: 0.729258]\n",
      "599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008066] [C loss: 0.765898]\n",
      "600 [D loss: 0.776205, acc.: 56.25%] [G loss: 0.013678] [C loss: 0.741388]\n",
      "601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008479] [C loss: 0.761306]\n",
      "602 [D loss: 0.749084, acc.: 43.75%] [G loss: 0.008636] [C loss: 0.727753]\n",
      "603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007307] [C loss: 0.916953]\n",
      "604 [D loss: 0.754642, acc.: 43.75%] [G loss: 0.006966] [C loss: 0.730124]\n",
      "605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011196] [C loss: 0.819418]\n",
      "606 [D loss: 0.734200, acc.: 50.00%] [G loss: 0.014374] [C loss: 0.734871]\n",
      "607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008124] [C loss: 0.789510]\n",
      "608 [D loss: 0.857988, acc.: 43.75%] [G loss: 0.009332] [C loss: 0.782606]\n",
      "609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009648] [C loss: 0.793342]\n",
      "610 [D loss: 0.789475, acc.: 50.00%] [G loss: 0.008864] [C loss: 0.734663]\n",
      "611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010105] [C loss: 0.747376]\n",
      "612 [D loss: 0.733975, acc.: 43.75%] [G loss: 0.008169] [C loss: 0.716652]\n",
      "613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009380] [C loss: 0.734592]\n",
      "614 [D loss: 0.831208, acc.: 50.00%] [G loss: 0.007886] [C loss: 0.737029]\n",
      "615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012078] [C loss: 0.750297]\n",
      "616 [D loss: 0.786484, acc.: 50.00%] [G loss: 0.007711] [C loss: 0.716165]\n",
      "617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012606] [C loss: 0.793089]\n",
      "618 [D loss: 0.774950, acc.: 50.00%] [G loss: 0.010593] [C loss: 0.722586]\n",
      "619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009254] [C loss: 0.783302]\n",
      "620 [D loss: 0.807968, acc.: 56.25%] [G loss: 0.011169] [C loss: 0.727076]\n",
      "621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008776] [C loss: 0.711711]\n",
      "622 [D loss: 0.747735, acc.: 50.00%] [G loss: 0.009798] [C loss: 0.707407]\n",
      "623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023880] [C loss: 0.723892]\n",
      "624 [D loss: 0.731346, acc.: 50.00%] [G loss: 0.005841] [C loss: 0.728082]\n",
      "625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006890] [C loss: 0.701645]\n",
      "626 [D loss: 0.752756, acc.: 56.25%] [G loss: 0.008699] [C loss: 0.723346]\n",
      "627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009214] [C loss: 0.718427]\n",
      "628 [D loss: 0.746436, acc.: 43.75%] [G loss: 0.014618] [C loss: 0.704997]\n",
      "629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007111] [C loss: 0.724333]\n",
      "630 [D loss: 0.722258, acc.: 56.25%] [G loss: 0.009860] [C loss: 0.704795]\n",
      "631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007041] [C loss: 0.713058]\n",
      "632 [D loss: 0.783123, acc.: 50.00%] [G loss: 0.006309] [C loss: 0.746161]\n",
      "633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005991] [C loss: 0.767101]\n",
      "634 [D loss: 0.730987, acc.: 43.75%] [G loss: 0.006353] [C loss: 0.724870]\n",
      "635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015752] [C loss: 0.862572]\n",
      "636 [D loss: 0.780038, acc.: 56.25%] [G loss: 0.010163] [C loss: 0.730006]\n",
      "637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007165] [C loss: 0.751442]\n",
      "638 [D loss: 0.731137, acc.: 43.75%] [G loss: 0.005848] [C loss: 0.723361]\n",
      "639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008472] [C loss: 0.769217]\n",
      "640 [D loss: 0.794773, acc.: 43.75%] [G loss: 0.010770] [C loss: 0.737464]\n",
      "641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012637] [C loss: 0.728413]\n",
      "642 [D loss: 0.725851, acc.: 50.00%] [G loss: 0.008150] [C loss: 0.714406]\n",
      "643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010601] [C loss: 0.828639]\n",
      "644 [D loss: 0.785373, acc.: 43.75%] [G loss: 0.007956] [C loss: 0.774599]\n",
      "645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006491] [C loss: 0.881478]\n",
      "646 [D loss: 0.781900, acc.: 50.00%] [G loss: 0.007227] [C loss: 0.752401]\n",
      "647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007912] [C loss: 0.774396]\n",
      "648 [D loss: 0.752782, acc.: 62.50%] [G loss: 0.009130] [C loss: 0.753595]\n",
      "649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009747] [C loss: 0.832591]\n",
      "650 [D loss: 0.894006, acc.: 56.25%] [G loss: 0.006347] [C loss: 0.837361]\n",
      "651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008974] [C loss: 0.808824]\n",
      "652 [D loss: 0.824762, acc.: 56.25%] [G loss: 0.006638] [C loss: 0.761764]\n",
      "653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007528] [C loss: 0.758205]\n",
      "654 [D loss: 0.763605, acc.: 56.25%] [G loss: 0.008431] [C loss: 0.728199]\n",
      "655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007250] [C loss: 0.849279]\n",
      "656 [D loss: 0.847148, acc.: 37.50%] [G loss: 0.009354] [C loss: 0.847730]\n",
      "657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010358] [C loss: 0.857322]\n",
      "658 [D loss: 0.840022, acc.: 56.25%] [G loss: 0.006849] [C loss: 0.778040]\n",
      "659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007190] [C loss: 0.858171]\n",
      "660 [D loss: 0.790284, acc.: 56.25%] [G loss: 0.007500] [C loss: 0.788297]\n",
      "661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006547] [C loss: 0.828500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 [D loss: 0.840278, acc.: 43.75%] [G loss: 0.008284] [C loss: 0.732001]\n",
      "663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021796] [C loss: 0.752473]\n",
      "664 [D loss: 0.764219, acc.: 43.75%] [G loss: 0.011908] [C loss: 0.743688]\n",
      "665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010620] [C loss: 0.796645]\n",
      "666 [D loss: 0.795858, acc.: 62.50%] [G loss: 0.008187] [C loss: 0.782975]\n",
      "667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010831] [C loss: 0.838442]\n",
      "668 [D loss: 0.810011, acc.: 43.75%] [G loss: 0.011003] [C loss: 0.766068]\n",
      "669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009649] [C loss: 0.779403]\n",
      "670 [D loss: 0.777748, acc.: 50.00%] [G loss: 0.007884] [C loss: 0.764425]\n",
      "671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011012] [C loss: 0.768970]\n",
      "672 [D loss: 0.757313, acc.: 43.75%] [G loss: 0.009151] [C loss: 0.746487]\n",
      "673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008145] [C loss: 0.835351]\n",
      "674 [D loss: 0.784250, acc.: 43.75%] [G loss: 0.010172] [C loss: 0.782528]\n",
      "675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010607] [C loss: 0.837901]\n",
      "676 [D loss: 0.773147, acc.: 50.00%] [G loss: 0.009272] [C loss: 0.742472]\n",
      "677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008617] [C loss: 0.746145]\n",
      "678 [D loss: 0.778094, acc.: 50.00%] [G loss: 0.009483] [C loss: 0.762425]\n",
      "679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005900] [C loss: 0.793893]\n",
      "680 [D loss: 0.767265, acc.: 43.75%] [G loss: 0.010885] [C loss: 0.754225]\n",
      "681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005870] [C loss: 0.736826]\n",
      "682 [D loss: 0.796191, acc.: 37.50%] [G loss: 0.009119] [C loss: 0.739303]\n",
      "683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010728] [C loss: 0.754315]\n",
      "684 [D loss: 0.746524, acc.: 50.00%] [G loss: 0.012266] [C loss: 0.714344]\n",
      "685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010200] [C loss: 0.844167]\n",
      "686 [D loss: 0.770587, acc.: 56.25%] [G loss: 0.009710] [C loss: 0.758045]\n",
      "687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006211] [C loss: 0.848367]\n",
      "688 [D loss: 0.871801, acc.: 37.50%] [G loss: 0.009599] [C loss: 0.805594]\n",
      "689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009475] [C loss: 0.814177]\n",
      "690 [D loss: 0.879318, acc.: 50.00%] [G loss: 0.008002] [C loss: 0.792375]\n",
      "691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009307] [C loss: 0.793394]\n",
      "692 [D loss: 0.772024, acc.: 56.25%] [G loss: 0.007399] [C loss: 0.753411]\n",
      "693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008167] [C loss: 0.762272]\n",
      "694 [D loss: 0.770408, acc.: 56.25%] [G loss: 0.007973] [C loss: 0.740126]\n",
      "695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007028] [C loss: 0.761388]\n",
      "696 [D loss: 0.750536, acc.: 56.25%] [G loss: 0.007857] [C loss: 0.735161]\n",
      "697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008855] [C loss: 0.827995]\n",
      "698 [D loss: 0.764555, acc.: 43.75%] [G loss: 0.007844] [C loss: 0.734250]\n",
      "699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010842] [C loss: 0.802068]\n",
      "700 [D loss: 0.831851, acc.: 56.25%] [G loss: 0.010082] [C loss: 0.783993]\n",
      "701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008149] [C loss: 0.717607]\n",
      "702 [D loss: 0.752800, acc.: 50.00%] [G loss: 0.017291] [C loss: 0.726074]\n",
      "703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010033] [C loss: 0.744052]\n",
      "704 [D loss: 0.756759, acc.: 37.50%] [G loss: 0.009626] [C loss: 0.707054]\n",
      "705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005680] [C loss: 0.702125]\n",
      "706 [D loss: 0.721751, acc.: 43.75%] [G loss: 0.013736] [C loss: 0.697159]\n",
      "707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006515] [C loss: 0.853219]\n",
      "708 [D loss: 0.724754, acc.: 56.25%] [G loss: 0.006746] [C loss: 0.758815]\n",
      "709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.850485]\n",
      "710 [D loss: 0.811055, acc.: 50.00%] [G loss: 0.009885] [C loss: 0.767041]\n",
      "711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007086] [C loss: 0.739014]\n",
      "712 [D loss: 0.859056, acc.: 50.00%] [G loss: 0.007680] [C loss: 0.711592]\n",
      "713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006595] [C loss: 0.746330]\n",
      "714 [D loss: 0.773530, acc.: 43.75%] [G loss: 0.008601] [C loss: 0.749051]\n",
      "715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006913] [C loss: 0.790840]\n",
      "716 [D loss: 0.844145, acc.: 56.25%] [G loss: 0.006078] [C loss: 0.781679]\n",
      "717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007194] [C loss: 0.849734]\n",
      "718 [D loss: 0.809979, acc.: 43.75%] [G loss: 0.007646] [C loss: 0.706394]\n",
      "719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008023] [C loss: 0.776781]\n",
      "720 [D loss: 0.752902, acc.: 50.00%] [G loss: 0.010406] [C loss: 0.731855]\n",
      "721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011037] [C loss: 0.753507]\n",
      "722 [D loss: 0.824501, acc.: 50.00%] [G loss: 0.006492] [C loss: 0.747551]\n",
      "723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010720] [C loss: 0.732800]\n",
      "724 [D loss: 0.839915, acc.: 56.25%] [G loss: 0.009225] [C loss: 0.794844]\n",
      "725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008152] [C loss: 0.769669]\n",
      "726 [D loss: 0.759413, acc.: 43.75%] [G loss: 0.008803] [C loss: 0.743543]\n",
      "727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007621] [C loss: 0.714381]\n",
      "728 [D loss: 0.829552, acc.: 56.25%] [G loss: 0.007916] [C loss: 0.747836]\n",
      "729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008623] [C loss: 0.750809]\n",
      "730 [D loss: 0.746984, acc.: 56.25%] [G loss: 0.007489] [C loss: 0.709971]\n",
      "731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005989] [C loss: 0.744206]\n",
      "732 [D loss: 0.770007, acc.: 50.00%] [G loss: 0.007277] [C loss: 0.756289]\n",
      "733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008415] [C loss: 0.727129]\n",
      "734 [D loss: 0.817867, acc.: 43.75%] [G loss: 0.010563] [C loss: 0.724343]\n",
      "735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009327] [C loss: 0.792404]\n",
      "736 [D loss: 0.798513, acc.: 50.00%] [G loss: 0.026080] [C loss: 0.773822]\n",
      "737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007814] [C loss: 0.726068]\n",
      "738 [D loss: 0.791444, acc.: 62.50%] [G loss: 0.010994] [C loss: 0.739116]\n",
      "739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014910] [C loss: 0.831828]\n",
      "740 [D loss: 0.789359, acc.: 43.75%] [G loss: 0.009354] [C loss: 0.759547]\n",
      "741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007817] [C loss: 0.928438]\n",
      "742 [D loss: 0.752969, acc.: 50.00%] [G loss: 0.008961] [C loss: 0.734752]\n",
      "743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011237] [C loss: 0.742012]\n",
      "744 [D loss: 0.764347, acc.: 56.25%] [G loss: 0.020086] [C loss: 0.720650]\n",
      "745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010292] [C loss: 0.793124]\n",
      "746 [D loss: 0.753394, acc.: 37.50%] [G loss: 0.011598] [C loss: 0.782940]\n",
      "747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010594] [C loss: 0.744741]\n",
      "748 [D loss: 0.756586, acc.: 56.25%] [G loss: 0.008644] [C loss: 0.731904]\n",
      "749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010421] [C loss: 0.859674]\n",
      "750 [D loss: 0.756083, acc.: 62.50%] [G loss: 0.011810] [C loss: 0.747970]\n",
      "751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007388] [C loss: 0.725320]\n",
      "752 [D loss: 0.744617, acc.: 43.75%] [G loss: 0.007715] [C loss: 0.732834]\n",
      "753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010279] [C loss: 0.756693]\n",
      "754 [D loss: 0.751646, acc.: 43.75%] [G loss: 0.008950] [C loss: 0.744116]\n",
      "755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007830] [C loss: 0.784059]\n",
      "756 [D loss: 0.856625, acc.: 31.25%] [G loss: 0.009259] [C loss: 0.790452]\n",
      "757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006742] [C loss: 0.759360]\n",
      "758 [D loss: 0.739633, acc.: 50.00%] [G loss: 0.009102] [C loss: 0.726964]\n",
      "759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010758] [C loss: 0.735489]\n",
      "760 [D loss: 0.784438, acc.: 43.75%] [G loss: 0.007798] [C loss: 0.777038]\n",
      "761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007056] [C loss: 0.885158]\n",
      "762 [D loss: 0.753994, acc.: 43.75%] [G loss: 0.010477] [C loss: 0.720529]\n",
      "763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006110] [C loss: 0.757619]\n",
      "764 [D loss: 0.802046, acc.: 43.75%] [G loss: 0.009213] [C loss: 0.706934]\n",
      "765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008173] [C loss: 0.731462]\n",
      "766 [D loss: 0.759615, acc.: 43.75%] [G loss: 0.006266] [C loss: 0.760741]\n",
      "767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006588] [C loss: 0.764999]\n",
      "768 [D loss: 0.854447, acc.: 56.25%] [G loss: 0.008981] [C loss: 0.770097]\n",
      "769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010556] [C loss: 0.772322]\n",
      "770 [D loss: 0.838255, acc.: 50.00%] [G loss: 0.006749] [C loss: 0.756334]\n",
      "771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010149] [C loss: 0.753257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772 [D loss: 0.729375, acc.: 56.25%] [G loss: 0.009905] [C loss: 0.749824]\n",
      "773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008731] [C loss: 0.822250]\n",
      "774 [D loss: 0.759941, acc.: 37.50%] [G loss: 0.008068] [C loss: 0.779813]\n",
      "775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011895] [C loss: 1.020824]\n",
      "776 [D loss: 0.889831, acc.: 50.00%] [G loss: 0.008518] [C loss: 0.764327]\n",
      "777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009020] [C loss: 0.906778]\n",
      "778 [D loss: 0.760679, acc.: 43.75%] [G loss: 0.007949] [C loss: 0.738157]\n",
      "779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009229] [C loss: 0.750618]\n",
      "780 [D loss: 0.767186, acc.: 50.00%] [G loss: 0.010447] [C loss: 0.755297]\n",
      "781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007127] [C loss: 0.740900]\n",
      "782 [D loss: 0.839139, acc.: 75.00%] [G loss: 0.009054] [C loss: 0.816675]\n",
      "783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009074] [C loss: 0.861293]\n",
      "784 [D loss: 0.843528, acc.: 50.00%] [G loss: 0.010781] [C loss: 0.800191]\n",
      "785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008901] [C loss: 0.788357]\n",
      "786 [D loss: 0.780225, acc.: 68.75%] [G loss: 0.013185] [C loss: 0.760400]\n",
      "787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010031] [C loss: 0.895228]\n",
      "788 [D loss: 0.841577, acc.: 37.50%] [G loss: 0.007549] [C loss: 0.740129]\n",
      "789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011096] [C loss: 0.929603]\n",
      "790 [D loss: 0.806366, acc.: 62.50%] [G loss: 0.014805] [C loss: 0.746670]\n",
      "791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009002] [C loss: 0.849274]\n",
      "792 [D loss: 0.751053, acc.: 50.00%] [G loss: 0.008827] [C loss: 0.791578]\n",
      "793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009062] [C loss: 0.792963]\n",
      "794 [D loss: 0.761382, acc.: 56.25%] [G loss: 0.009235] [C loss: 0.775034]\n",
      "795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010664] [C loss: 0.877760]\n",
      "796 [D loss: 0.926805, acc.: 50.00%] [G loss: 0.009299] [C loss: 0.763908]\n",
      "797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009432] [C loss: 0.859399]\n",
      "798 [D loss: 0.866350, acc.: 56.25%] [G loss: 0.007143] [C loss: 0.806322]\n",
      "799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009618] [C loss: 0.941766]\n",
      "800 [D loss: 0.952643, acc.: 43.75%] [G loss: 0.006803] [C loss: 0.905537]\n",
      "801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011202] [C loss: 0.882015]\n",
      "802 [D loss: 0.934972, acc.: 50.00%] [G loss: 0.008783] [C loss: 0.899595]\n",
      "803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007563] [C loss: 0.882392]\n",
      "804 [D loss: 0.824606, acc.: 56.25%] [G loss: 0.009680] [C loss: 0.848558]\n",
      "805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010410] [C loss: 1.027993]\n",
      "806 [D loss: 0.795908, acc.: 50.00%] [G loss: 0.011493] [C loss: 0.804897]\n",
      "807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007226] [C loss: 0.802743]\n",
      "808 [D loss: 0.787454, acc.: 43.75%] [G loss: 0.009110] [C loss: 0.731664]\n",
      "809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007640] [C loss: 0.807102]\n",
      "810 [D loss: 0.798954, acc.: 37.50%] [G loss: 0.009971] [C loss: 0.781049]\n",
      "811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008525] [C loss: 0.738241]\n",
      "812 [D loss: 0.871314, acc.: 50.00%] [G loss: 0.006575] [C loss: 0.753146]\n",
      "813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008221] [C loss: 0.746182]\n",
      "814 [D loss: 0.739223, acc.: 56.25%] [G loss: 0.010096] [C loss: 0.721071]\n",
      "815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007808] [C loss: 0.731375]\n",
      "816 [D loss: 0.794076, acc.: 43.75%] [G loss: 0.008425] [C loss: 0.734000]\n",
      "817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006355] [C loss: 0.729553]\n",
      "818 [D loss: 0.739506, acc.: 56.25%] [G loss: 0.007220] [C loss: 0.705003]\n",
      "819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009158] [C loss: 0.808623]\n",
      "820 [D loss: 0.747121, acc.: 56.25%] [G loss: 0.008035] [C loss: 0.770202]\n",
      "821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008893] [C loss: 0.820307]\n",
      "822 [D loss: 0.778153, acc.: 50.00%] [G loss: 0.007545] [C loss: 0.725063]\n",
      "823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007978] [C loss: 0.754028]\n",
      "824 [D loss: 0.770026, acc.: 43.75%] [G loss: 0.007912] [C loss: 0.740228]\n",
      "825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011526] [C loss: 0.747087]\n",
      "826 [D loss: 0.731296, acc.: 43.75%] [G loss: 0.005920] [C loss: 0.738852]\n",
      "827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007115] [C loss: 0.717614]\n",
      "828 [D loss: 0.755482, acc.: 50.00%] [G loss: 0.007332] [C loss: 0.722396]\n",
      "829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008514] [C loss: 0.763007]\n",
      "830 [D loss: 0.834652, acc.: 50.00%] [G loss: 0.005633] [C loss: 0.742648]\n",
      "831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009460] [C loss: 0.734172]\n",
      "832 [D loss: 0.758431, acc.: 43.75%] [G loss: 0.006296] [C loss: 0.746675]\n",
      "833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009113] [C loss: 0.730675]\n",
      "834 [D loss: 0.853510, acc.: 50.00%] [G loss: 0.007576] [C loss: 0.752551]\n",
      "835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009302] [C loss: 0.773696]\n",
      "836 [D loss: 0.805978, acc.: 50.00%] [G loss: 0.011378] [C loss: 0.780935]\n",
      "837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007399] [C loss: 0.817416]\n",
      "838 [D loss: 0.727231, acc.: 50.00%] [G loss: 0.008170] [C loss: 0.727789]\n",
      "839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006792] [C loss: 0.737921]\n",
      "840 [D loss: 0.774277, acc.: 50.00%] [G loss: 0.006805] [C loss: 0.711254]\n",
      "841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006157] [C loss: 0.717430]\n",
      "842 [D loss: 0.782173, acc.: 62.50%] [G loss: 0.007512] [C loss: 0.764348]\n",
      "843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011447] [C loss: 0.735879]\n",
      "844 [D loss: 0.731300, acc.: 50.00%] [G loss: 0.006219] [C loss: 0.734597]\n",
      "845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010683] [C loss: 0.805958]\n",
      "846 [D loss: 0.809606, acc.: 50.00%] [G loss: 0.008541] [C loss: 0.759521]\n",
      "847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006777] [C loss: 0.764988]\n",
      "848 [D loss: 0.739419, acc.: 56.25%] [G loss: 0.007789] [C loss: 0.722486]\n",
      "849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013968] [C loss: 0.804194]\n",
      "850 [D loss: 0.783875, acc.: 56.25%] [G loss: 0.009593] [C loss: 0.747513]\n",
      "851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008760] [C loss: 0.829995]\n",
      "852 [D loss: 0.791767, acc.: 50.00%] [G loss: 0.008093] [C loss: 0.769848]\n",
      "853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007114] [C loss: 0.808302]\n",
      "854 [D loss: 0.791587, acc.: 37.50%] [G loss: 0.009230] [C loss: 0.719103]\n",
      "855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008928] [C loss: 0.705034]\n",
      "856 [D loss: 0.776767, acc.: 50.00%] [G loss: 0.009033] [C loss: 0.722180]\n",
      "857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006837] [C loss: 0.823910]\n",
      "858 [D loss: 0.818833, acc.: 43.75%] [G loss: 0.007503] [C loss: 0.786056]\n",
      "859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019033] [C loss: 0.781941]\n",
      "860 [D loss: 0.809011, acc.: 50.00%] [G loss: 0.006412] [C loss: 0.766594]\n",
      "861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009778] [C loss: 0.826733]\n",
      "862 [D loss: 0.786456, acc.: 50.00%] [G loss: 0.006898] [C loss: 0.783204]\n",
      "863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008367] [C loss: 0.806032]\n",
      "864 [D loss: 0.779120, acc.: 43.75%] [G loss: 0.011069] [C loss: 0.714041]\n",
      "865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009163] [C loss: 0.757442]\n",
      "866 [D loss: 0.751128, acc.: 50.00%] [G loss: 0.006960] [C loss: 0.766221]\n",
      "867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007417] [C loss: 0.894323]\n",
      "868 [D loss: 0.759930, acc.: 37.50%] [G loss: 0.006347] [C loss: 0.733429]\n",
      "869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022588] [C loss: 0.781787]\n",
      "870 [D loss: 0.787074, acc.: 50.00%] [G loss: 0.007609] [C loss: 0.739861]\n",
      "871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005767] [C loss: 0.769109]\n",
      "872 [D loss: 0.740083, acc.: 43.75%] [G loss: 0.011564] [C loss: 0.775104]\n",
      "873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012587] [C loss: 0.786958]\n",
      "874 [D loss: 0.721140, acc.: 50.00%] [G loss: 0.010075] [C loss: 0.723842]\n",
      "875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009352] [C loss: 0.816239]\n",
      "876 [D loss: 0.813751, acc.: 37.50%] [G loss: 0.009270] [C loss: 0.761608]\n",
      "877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009598] [C loss: 0.747502]\n",
      "878 [D loss: 0.735748, acc.: 50.00%] [G loss: 0.007772] [C loss: 0.732770]\n",
      "879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008150] [C loss: 0.752987]\n",
      "880 [D loss: 0.731396, acc.: 43.75%] [G loss: 0.010527] [C loss: 0.732848]\n",
      "881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006696] [C loss: 0.755086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882 [D loss: 0.759138, acc.: 43.75%] [G loss: 0.010519] [C loss: 0.714997]\n",
      "883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009873] [C loss: 0.764160]\n",
      "884 [D loss: 0.784890, acc.: 50.00%] [G loss: 0.006793] [C loss: 0.750888]\n",
      "885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011967] [C loss: 0.731789]\n",
      "886 [D loss: 0.719328, acc.: 43.75%] [G loss: 0.006437] [C loss: 0.723483]\n",
      "887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008524] [C loss: 0.782268]\n",
      "888 [D loss: 0.760224, acc.: 62.50%] [G loss: 0.006797] [C loss: 0.723619]\n",
      "889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008566] [C loss: 0.842907]\n",
      "890 [D loss: 0.777991, acc.: 43.75%] [G loss: 0.007520] [C loss: 0.735776]\n",
      "891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007423] [C loss: 0.758420]\n",
      "892 [D loss: 0.782554, acc.: 43.75%] [G loss: 0.010139] [C loss: 0.736496]\n",
      "893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008162] [C loss: 0.731409]\n",
      "894 [D loss: 0.751575, acc.: 43.75%] [G loss: 0.007155] [C loss: 0.740101]\n",
      "895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010212] [C loss: 0.717522]\n",
      "896 [D loss: 0.737363, acc.: 43.75%] [G loss: 0.011903] [C loss: 0.743891]\n",
      "897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006623] [C loss: 0.725141]\n",
      "898 [D loss: 0.750363, acc.: 37.50%] [G loss: 0.008048] [C loss: 0.716643]\n",
      "899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021956] [C loss: 0.757848]\n",
      "900 [D loss: 0.728659, acc.: 50.00%] [G loss: 0.006917] [C loss: 0.727079]\n",
      "901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006141] [C loss: 0.815420]\n",
      "902 [D loss: 0.785339, acc.: 43.75%] [G loss: 0.008848] [C loss: 0.778625]\n",
      "903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008733] [C loss: 0.863471]\n",
      "904 [D loss: 0.815166, acc.: 43.75%] [G loss: 0.006835] [C loss: 0.771538]\n",
      "905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008429] [C loss: 0.759022]\n",
      "906 [D loss: 0.757071, acc.: 43.75%] [G loss: 0.010724] [C loss: 0.712804]\n",
      "907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006816] [C loss: 0.733021]\n",
      "908 [D loss: 0.783587, acc.: 62.50%] [G loss: 0.008064] [C loss: 0.730958]\n",
      "909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009243] [C loss: 0.746347]\n",
      "910 [D loss: 0.730065, acc.: 50.00%] [G loss: 0.007654] [C loss: 0.731020]\n",
      "911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009054] [C loss: 0.724796]\n",
      "912 [D loss: 0.760999, acc.: 50.00%] [G loss: 0.011419] [C loss: 0.719570]\n",
      "913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006398] [C loss: 0.733486]\n",
      "914 [D loss: 0.959067, acc.: 37.50%] [G loss: 0.010524] [C loss: 0.721938]\n",
      "915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009622] [C loss: 0.761093]\n",
      "916 [D loss: 0.754926, acc.: 56.25%] [G loss: 0.008453] [C loss: 0.744915]\n",
      "917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007310] [C loss: 0.793791]\n",
      "918 [D loss: 0.828846, acc.: 56.25%] [G loss: 0.010854] [C loss: 0.827384]\n",
      "919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013093] [C loss: 0.764395]\n",
      "920 [D loss: 0.733165, acc.: 50.00%] [G loss: 0.007846] [C loss: 0.767252]\n",
      "921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006800] [C loss: 0.777159]\n",
      "922 [D loss: 0.799554, acc.: 50.00%] [G loss: 0.010560] [C loss: 0.818026]\n",
      "923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012952] [C loss: 0.790576]\n",
      "924 [D loss: 0.781813, acc.: 56.25%] [G loss: 0.009659] [C loss: 0.778195]\n",
      "925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007742] [C loss: 0.846258]\n",
      "926 [D loss: 0.729918, acc.: 37.50%] [G loss: 0.008608] [C loss: 0.754620]\n",
      "927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007582] [C loss: 0.811931]\n",
      "928 [D loss: 0.785333, acc.: 50.00%] [G loss: 0.007772] [C loss: 0.736953]\n",
      "929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009227] [C loss: 0.890299]\n",
      "930 [D loss: 0.801552, acc.: 43.75%] [G loss: 0.013991] [C loss: 0.765272]\n",
      "931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008053] [C loss: 0.758486]\n",
      "932 [D loss: 0.761607, acc.: 43.75%] [G loss: 0.005111] [C loss: 0.731813]\n",
      "933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007943] [C loss: 0.771234]\n",
      "934 [D loss: 0.785058, acc.: 43.75%] [G loss: 0.010938] [C loss: 0.768847]\n",
      "935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010011] [C loss: 0.730625]\n",
      "936 [D loss: 0.735646, acc.: 62.50%] [G loss: 0.006847] [C loss: 0.712710]\n",
      "937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007005] [C loss: 0.749318]\n",
      "938 [D loss: 0.766346, acc.: 50.00%] [G loss: 0.006839] [C loss: 0.716612]\n",
      "939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007531] [C loss: 0.745796]\n",
      "940 [D loss: 0.717078, acc.: 62.50%] [G loss: 0.010923] [C loss: 0.699626]\n",
      "941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.774857]\n",
      "942 [D loss: 0.744678, acc.: 43.75%] [G loss: 0.007305] [C loss: 0.713233]\n",
      "943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006116] [C loss: 0.721314]\n",
      "944 [D loss: 0.728853, acc.: 43.75%] [G loss: 0.006977] [C loss: 0.714795]\n",
      "945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009203] [C loss: 0.728429]\n",
      "946 [D loss: 0.764472, acc.: 50.00%] [G loss: 0.008589] [C loss: 0.761228]\n",
      "947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005733] [C loss: 0.761486]\n",
      "948 [D loss: 0.727407, acc.: 50.00%] [G loss: 0.006259] [C loss: 0.723995]\n",
      "949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006468] [C loss: 0.736599]\n",
      "950 [D loss: 0.854515, acc.: 43.75%] [G loss: 0.009881] [C loss: 0.737857]\n",
      "951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011811] [C loss: 0.712968]\n",
      "952 [D loss: 0.744036, acc.: 37.50%] [G loss: 0.011123] [C loss: 0.730141]\n",
      "953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009423] [C loss: 0.756393]\n",
      "954 [D loss: 0.746124, acc.: 62.50%] [G loss: 0.011865] [C loss: 0.739322]\n",
      "955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009394] [C loss: 0.725652]\n",
      "956 [D loss: 0.760710, acc.: 50.00%] [G loss: 0.006590] [C loss: 0.722935]\n",
      "957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008067] [C loss: 0.796804]\n",
      "958 [D loss: 0.733222, acc.: 43.75%] [G loss: 0.008248] [C loss: 0.732317]\n",
      "959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007098] [C loss: 0.702279]\n",
      "960 [D loss: 0.753337, acc.: 62.50%] [G loss: 0.005394] [C loss: 0.696629]\n",
      "961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008263] [C loss: 0.695558]\n",
      "962 [D loss: 0.709242, acc.: 37.50%] [G loss: 0.007258] [C loss: 0.694913]\n",
      "963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009315] [C loss: 0.697216]\n",
      "964 [D loss: 0.702472, acc.: 31.25%] [G loss: 0.008446] [C loss: 0.698916]\n",
      "965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006246] [C loss: 0.713570]\n",
      "966 [D loss: 0.702232, acc.: 43.75%] [G loss: 0.006591] [C loss: 0.698528]\n",
      "967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006939] [C loss: 0.699678]\n",
      "968 [D loss: 0.701258, acc.: 12.50%] [G loss: 0.008465] [C loss: 0.698168]\n",
      "969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006852] [C loss: 0.698052]\n",
      "970 [D loss: 0.698043, acc.: 6.25%] [G loss: 0.009511] [C loss: 0.698828]\n",
      "971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009758] [C loss: 0.698828]\n",
      "972 [D loss: 0.698087, acc.: 0.00%] [G loss: 0.008010] [C loss: 0.699413]\n",
      "973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008159] [C loss: 0.699413]\n",
      "974 [D loss: 0.698135, acc.: 0.00%] [G loss: 0.006831] [C loss: 0.699801]\n",
      "975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006138] [C loss: 0.699801]\n",
      "976 [D loss: 0.698183, acc.: 0.00%] [G loss: 0.005597] [C loss: 0.700001]\n",
      "977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009990] [C loss: 0.700001]\n",
      "978 [D loss: 0.698223, acc.: 0.00%] [G loss: 0.007265] [C loss: 0.700033]\n",
      "979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006669] [C loss: 0.700033]\n",
      "980 [D loss: 0.698257, acc.: 0.00%] [G loss: 0.007199] [C loss: 0.699927]\n",
      "981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006957] [C loss: 0.699927]\n",
      "982 [D loss: 0.698281, acc.: 0.00%] [G loss: 0.008607] [C loss: 0.699718]\n",
      "983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007262] [C loss: 0.699718]\n",
      "984 [D loss: 0.698297, acc.: 0.00%] [G loss: 0.007396] [C loss: 0.699439]\n",
      "985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011101] [C loss: 0.699439]\n",
      "986 [D loss: 0.698304, acc.: 0.00%] [G loss: 0.006125] [C loss: 0.699128]\n",
      "987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008437] [C loss: 0.699128]\n",
      "988 [D loss: 0.698304, acc.: 0.00%] [G loss: 0.011717] [C loss: 0.698808]\n",
      "989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007648] [C loss: 0.698808]\n",
      "990 [D loss: 0.698297, acc.: 0.00%] [G loss: 0.008036] [C loss: 0.698504]\n",
      "991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005761] [C loss: 0.698504]\n",
      "992 [D loss: 0.698284, acc.: 0.00%] [G loss: 0.008652] [C loss: 0.698237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007972] [C loss: 0.698237]\n",
      "994 [D loss: 0.698268, acc.: 0.00%] [G loss: 0.007403] [C loss: 0.698020]\n",
      "995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006654] [C loss: 0.698020]\n",
      "996 [D loss: 0.698252, acc.: 0.00%] [G loss: 0.009067] [C loss: 0.697855]\n",
      "997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005686] [C loss: 0.697855]\n",
      "998 [D loss: 0.698233, acc.: 0.00%] [G loss: 0.012088] [C loss: 0.697746]\n",
      "999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005976] [C loss: 0.697746]\n",
      "1000 [D loss: 0.698215, acc.: 0.00%] [G loss: 0.009076] [C loss: 0.697689]\n",
      "1001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006116] [C loss: 0.697689]\n",
      "1002 [D loss: 0.698199, acc.: 0.00%] [G loss: 0.006717] [C loss: 0.697679]\n",
      "1003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005179] [C loss: 0.697679]\n",
      "1004 [D loss: 0.698184, acc.: 0.00%] [G loss: 0.009135] [C loss: 0.697709]\n",
      "1005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008694] [C loss: 0.697709]\n",
      "1006 [D loss: 0.698174, acc.: 0.00%] [G loss: 0.008708] [C loss: 0.697766]\n",
      "1007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007743] [C loss: 0.697766]\n",
      "1008 [D loss: 0.698165, acc.: 0.00%] [G loss: 0.007851] [C loss: 0.697840]\n",
      "1009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009355] [C loss: 0.697840]\n",
      "1010 [D loss: 0.698157, acc.: 0.00%] [G loss: 0.006691] [C loss: 0.697925]\n",
      "1011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019324] [C loss: 0.697925]\n",
      "1012 [D loss: 0.698153, acc.: 0.00%] [G loss: 0.007474] [C loss: 0.698009]\n",
      "1013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010931] [C loss: 0.698009]\n",
      "1014 [D loss: 0.698149, acc.: 0.00%] [G loss: 0.006287] [C loss: 0.698087]\n",
      "1015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008344] [C loss: 0.698087]\n",
      "1016 [D loss: 0.698148, acc.: 0.00%] [G loss: 0.005990] [C loss: 0.698158]\n",
      "1017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007473] [C loss: 0.698158]\n",
      "1018 [D loss: 0.698147, acc.: 0.00%] [G loss: 0.006338] [C loss: 0.698212]\n",
      "1019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007675] [C loss: 0.698212]\n",
      "1020 [D loss: 0.698146, acc.: 0.00%] [G loss: 0.006212] [C loss: 0.698254]\n",
      "1021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007043] [C loss: 0.698254]\n",
      "1022 [D loss: 0.698146, acc.: 0.00%] [G loss: 0.004398] [C loss: 0.698277]\n",
      "1023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006366] [C loss: 0.698277]\n",
      "1024 [D loss: 0.698145, acc.: 0.00%] [G loss: 0.008143] [C loss: 0.698288]\n",
      "1025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009348] [C loss: 0.698288]\n",
      "1026 [D loss: 0.698145, acc.: 0.00%] [G loss: 0.008123] [C loss: 0.698286]\n",
      "1027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011640] [C loss: 0.698286]\n",
      "1028 [D loss: 0.698145, acc.: 0.00%] [G loss: 0.007535] [C loss: 0.698270]\n",
      "1029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006646] [C loss: 0.698270]\n",
      "1030 [D loss: 0.698141, acc.: 0.00%] [G loss: 0.006211] [C loss: 0.698252]\n",
      "1031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012444] [C loss: 0.698252]\n",
      "1032 [D loss: 0.698141, acc.: 0.00%] [G loss: 0.006960] [C loss: 0.698225]\n",
      "1033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007290] [C loss: 0.698225]\n",
      "1034 [D loss: 0.698136, acc.: 0.00%] [G loss: 0.007304] [C loss: 0.698195]\n",
      "1035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010250] [C loss: 0.698195]\n",
      "1036 [D loss: 0.698133, acc.: 0.00%] [G loss: 0.007224] [C loss: 0.698166]\n",
      "1037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011638] [C loss: 0.698166]\n",
      "1038 [D loss: 0.698127, acc.: 0.00%] [G loss: 0.008283] [C loss: 0.698140]\n",
      "1039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005449] [C loss: 0.698140]\n",
      "1040 [D loss: 0.698124, acc.: 0.00%] [G loss: 0.006250] [C loss: 0.698115]\n",
      "1041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.698115]\n",
      "1042 [D loss: 0.698118, acc.: 0.00%] [G loss: 0.007935] [C loss: 0.698095]\n",
      "1043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008300] [C loss: 0.698095]\n",
      "1044 [D loss: 0.698115, acc.: 0.00%] [G loss: 0.009258] [C loss: 0.698078]\n",
      "1045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006504] [C loss: 0.698078]\n",
      "1046 [D loss: 0.698109, acc.: 0.00%] [G loss: 0.005343] [C loss: 0.698067]\n",
      "1047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007801] [C loss: 0.698067]\n",
      "1048 [D loss: 0.698105, acc.: 0.00%] [G loss: 0.009469] [C loss: 0.698060]\n",
      "1049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007797] [C loss: 0.698060]\n",
      "1050 [D loss: 0.698102, acc.: 0.00%] [G loss: 0.023857] [C loss: 0.698056]\n",
      "1051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005987] [C loss: 0.698056]\n",
      "1052 [D loss: 0.698096, acc.: 0.00%] [G loss: 0.006309] [C loss: 0.698054]\n",
      "1053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005859] [C loss: 0.698054]\n",
      "1054 [D loss: 0.698091, acc.: 0.00%] [G loss: 0.011230] [C loss: 0.698056]\n",
      "1055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006588] [C loss: 0.698056]\n",
      "1056 [D loss: 0.698087, acc.: 0.00%] [G loss: 0.004228] [C loss: 0.698059]\n",
      "1057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009115] [C loss: 0.698059]\n",
      "1058 [D loss: 0.698091, acc.: 0.00%] [G loss: 0.008228] [C loss: 0.698063]\n",
      "1059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016309] [C loss: 0.698063]\n",
      "1060 [D loss: 0.698080, acc.: 0.00%] [G loss: 0.010515] [C loss: 0.698063]\n",
      "1061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006713] [C loss: 0.698063]\n",
      "1062 [D loss: 0.698076, acc.: 0.00%] [G loss: 0.019260] [C loss: 0.698067]\n",
      "1063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009023] [C loss: 0.698067]\n",
      "1064 [D loss: 0.698072, acc.: 0.00%] [G loss: 0.005918] [C loss: 0.698067]\n",
      "1065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007048] [C loss: 0.698067]\n",
      "1066 [D loss: 0.698069, acc.: 0.00%] [G loss: 0.021393] [C loss: 0.698067]\n",
      "1067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009714] [C loss: 0.698067]\n",
      "1068 [D loss: 0.698066, acc.: 0.00%] [G loss: 0.010461] [C loss: 0.698068]\n",
      "1069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006284] [C loss: 0.698068]\n",
      "1070 [D loss: 0.698062, acc.: 0.00%] [G loss: 0.009814] [C loss: 0.698067]\n",
      "1071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007874] [C loss: 0.698067]\n",
      "1072 [D loss: 0.698058, acc.: 0.00%] [G loss: 0.006453] [C loss: 0.698068]\n",
      "1073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007077] [C loss: 0.698068]\n",
      "1074 [D loss: 0.698056, acc.: 0.00%] [G loss: 0.007702] [C loss: 0.698063]\n",
      "1075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007401] [C loss: 0.698063]\n",
      "1076 [D loss: 0.698053, acc.: 0.00%] [G loss: 0.004877] [C loss: 0.698059]\n",
      "1077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005156] [C loss: 0.698059]\n",
      "1078 [D loss: 0.698049, acc.: 0.00%] [G loss: 0.006099] [C loss: 0.698056]\n",
      "1079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010919] [C loss: 0.698056]\n",
      "1080 [D loss: 0.698047, acc.: 0.00%] [G loss: 0.011741] [C loss: 0.698048]\n",
      "1081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011092] [C loss: 0.698048]\n",
      "1082 [D loss: 0.698042, acc.: 0.00%] [G loss: 0.008351] [C loss: 0.698044]\n",
      "1083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007721] [C loss: 0.698044]\n",
      "1084 [D loss: 0.698038, acc.: 0.00%] [G loss: 0.008784] [C loss: 0.698038]\n",
      "1085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008508] [C loss: 0.698038]\n",
      "1086 [D loss: 0.698035, acc.: 0.00%] [G loss: 0.009855] [C loss: 0.698034]\n",
      "1087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008293] [C loss: 0.698034]\n",
      "1088 [D loss: 0.698033, acc.: 0.00%] [G loss: 0.006757] [C loss: 0.698028]\n",
      "1089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007374] [C loss: 0.698028]\n",
      "1090 [D loss: 0.698028, acc.: 0.00%] [G loss: 0.008072] [C loss: 0.698023]\n",
      "1091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006485] [C loss: 0.698023]\n",
      "1092 [D loss: 0.698026, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.698018]\n",
      "1093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012108] [C loss: 0.698018]\n",
      "1094 [D loss: 0.698021, acc.: 0.00%] [G loss: 0.006615] [C loss: 0.698014]\n",
      "1095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006678] [C loss: 0.698014]\n",
      "1096 [D loss: 0.698018, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.698008]\n",
      "1097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009914] [C loss: 0.698008]\n",
      "1098 [D loss: 0.698013, acc.: 0.00%] [G loss: 0.006753] [C loss: 0.698007]\n",
      "1099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007372] [C loss: 0.698007]\n",
      "1100 [D loss: 0.698010, acc.: 0.00%] [G loss: 0.008648] [C loss: 0.698001]\n",
      "1101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010519] [C loss: 0.698001]\n",
      "1102 [D loss: 0.698006, acc.: 0.00%] [G loss: 0.007120] [C loss: 0.697998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006788] [C loss: 0.697998]\n",
      "1104 [D loss: 0.698002, acc.: 0.00%] [G loss: 0.005883] [C loss: 0.697995]\n",
      "1105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006822] [C loss: 0.697995]\n",
      "1106 [D loss: 0.697999, acc.: 0.00%] [G loss: 0.022715] [C loss: 0.697996]\n",
      "1107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010957] [C loss: 0.697996]\n",
      "1108 [D loss: 0.697998, acc.: 0.00%] [G loss: 0.005869] [C loss: 0.697991]\n",
      "1109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009164] [C loss: 0.697991]\n",
      "1110 [D loss: 0.697993, acc.: 0.00%] [G loss: 0.007223] [C loss: 0.697988]\n",
      "1111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007048] [C loss: 0.697988]\n",
      "1112 [D loss: 0.697991, acc.: 0.00%] [G loss: 0.008143] [C loss: 0.697986]\n",
      "1113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009560] [C loss: 0.697986]\n",
      "1114 [D loss: 0.697987, acc.: 0.00%] [G loss: 0.006284] [C loss: 0.697984]\n",
      "1115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007471] [C loss: 0.697984]\n",
      "1116 [D loss: 0.697984, acc.: 0.00%] [G loss: 0.012635] [C loss: 0.697980]\n",
      "1117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009414] [C loss: 0.697980]\n",
      "1118 [D loss: 0.697980, acc.: 0.00%] [G loss: 0.023847] [C loss: 0.697977]\n",
      "1119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007153] [C loss: 0.697977]\n",
      "1120 [D loss: 0.697978, acc.: 0.00%] [G loss: 0.007831] [C loss: 0.697974]\n",
      "1121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007801] [C loss: 0.697974]\n",
      "1122 [D loss: 0.697973, acc.: 0.00%] [G loss: 0.006754] [C loss: 0.697972]\n",
      "1123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007256] [C loss: 0.697972]\n",
      "1124 [D loss: 0.697972, acc.: 0.00%] [G loss: 0.010895] [C loss: 0.697969]\n",
      "1125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005554] [C loss: 0.697969]\n",
      "1126 [D loss: 0.697968, acc.: 0.00%] [G loss: 0.007478] [C loss: 0.697965]\n",
      "1127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007646] [C loss: 0.697965]\n",
      "1128 [D loss: 0.697965, acc.: 0.00%] [G loss: 0.007384] [C loss: 0.697960]\n",
      "1129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006855] [C loss: 0.697960]\n",
      "1130 [D loss: 0.697962, acc.: 0.00%] [G loss: 0.012139] [C loss: 0.697958]\n",
      "1131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010068] [C loss: 0.697958]\n",
      "1132 [D loss: 0.697959, acc.: 0.00%] [G loss: 0.009558] [C loss: 0.697953]\n",
      "1133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007393] [C loss: 0.697953]\n",
      "1134 [D loss: 0.697954, acc.: 0.00%] [G loss: 0.007583] [C loss: 0.697951]\n",
      "1135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007255] [C loss: 0.697951]\n",
      "1136 [D loss: 0.697951, acc.: 0.00%] [G loss: 0.006324] [C loss: 0.697949]\n",
      "1137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005835] [C loss: 0.697949]\n",
      "1138 [D loss: 0.697949, acc.: 0.00%] [G loss: 0.009697] [C loss: 0.697947]\n",
      "1139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008601] [C loss: 0.697947]\n",
      "1140 [D loss: 0.697948, acc.: 0.00%] [G loss: 0.006165] [C loss: 0.697944]\n",
      "1141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007254] [C loss: 0.697944]\n",
      "1142 [D loss: 0.697943, acc.: 0.00%] [G loss: 0.005706] [C loss: 0.697938]\n",
      "1143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007745] [C loss: 0.697938]\n",
      "1144 [D loss: 0.697939, acc.: 0.00%] [G loss: 0.009575] [C loss: 0.697936]\n",
      "1145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009755] [C loss: 0.697936]\n",
      "1146 [D loss: 0.697936, acc.: 0.00%] [G loss: 0.010849] [C loss: 0.697934]\n",
      "1147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009590] [C loss: 0.697934]\n",
      "1148 [D loss: 0.697935, acc.: 0.00%] [G loss: 0.007135] [C loss: 0.697930]\n",
      "1149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006607] [C loss: 0.697930]\n",
      "1150 [D loss: 0.697931, acc.: 0.00%] [G loss: 0.007955] [C loss: 0.697926]\n",
      "1151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006952] [C loss: 0.697926]\n",
      "1152 [D loss: 0.697927, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.697925]\n",
      "1153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007362] [C loss: 0.697925]\n",
      "1154 [D loss: 0.697925, acc.: 0.00%] [G loss: 0.007516] [C loss: 0.697921]\n",
      "1155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009504] [C loss: 0.697921]\n",
      "1156 [D loss: 0.697922, acc.: 0.00%] [G loss: 0.007295] [C loss: 0.697919]\n",
      "1157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007656] [C loss: 0.697919]\n",
      "1158 [D loss: 0.697919, acc.: 0.00%] [G loss: 0.007196] [C loss: 0.697915]\n",
      "1159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009867] [C loss: 0.697915]\n",
      "1160 [D loss: 0.697916, acc.: 0.00%] [G loss: 0.007593] [C loss: 0.697911]\n",
      "1161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007277] [C loss: 0.697911]\n",
      "1162 [D loss: 0.697912, acc.: 0.00%] [G loss: 0.007310] [C loss: 0.697908]\n",
      "1163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009108] [C loss: 0.697908]\n",
      "1164 [D loss: 0.697911, acc.: 0.00%] [G loss: 0.007845] [C loss: 0.697905]\n",
      "1165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011373] [C loss: 0.697905]\n",
      "1166 [D loss: 0.697906, acc.: 0.00%] [G loss: 0.008945] [C loss: 0.697902]\n",
      "1167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005827] [C loss: 0.697902]\n",
      "1168 [D loss: 0.697902, acc.: 0.00%] [G loss: 0.007957] [C loss: 0.697899]\n",
      "1169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008633] [C loss: 0.697899]\n",
      "1170 [D loss: 0.697901, acc.: 0.00%] [G loss: 0.006519] [C loss: 0.697895]\n",
      "1171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007283] [C loss: 0.697895]\n",
      "1172 [D loss: 0.697897, acc.: 0.00%] [G loss: 0.011791] [C loss: 0.697895]\n",
      "1173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011791] [C loss: 0.697895]\n",
      "1174 [D loss: 0.697895, acc.: 0.00%] [G loss: 0.011392] [C loss: 0.697892]\n",
      "1175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007102] [C loss: 0.697892]\n",
      "1176 [D loss: 0.697893, acc.: 0.00%] [G loss: 0.005434] [C loss: 0.697887]\n",
      "1177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008170] [C loss: 0.697887]\n",
      "1178 [D loss: 0.697888, acc.: 0.00%] [G loss: 0.009083] [C loss: 0.697885]\n",
      "1179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009428] [C loss: 0.697885]\n",
      "1180 [D loss: 0.697886, acc.: 0.00%] [G loss: 0.008184] [C loss: 0.697881]\n",
      "1181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005354] [C loss: 0.697881]\n",
      "1182 [D loss: 0.697884, acc.: 6.25%] [G loss: 0.008518] [C loss: 0.697879]\n",
      "1183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008342] [C loss: 0.697879]\n",
      "1184 [D loss: 0.697881, acc.: 0.00%] [G loss: 0.007535] [C loss: 0.697878]\n",
      "1185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006436] [C loss: 0.697878]\n",
      "1186 [D loss: 0.697878, acc.: 0.00%] [G loss: 0.008805] [C loss: 0.697874]\n",
      "1187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009360] [C loss: 0.697877]\n",
      "1188 [D loss: 0.697875, acc.: 0.00%] [G loss: 0.010247] [C loss: 0.697870]\n",
      "1189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007576] [C loss: 0.697870]\n",
      "1190 [D loss: 0.697873, acc.: 0.00%] [G loss: 0.006793] [C loss: 0.697867]\n",
      "1191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023386] [C loss: 0.697867]\n",
      "1192 [D loss: 0.697868, acc.: 0.00%] [G loss: 0.006088] [C loss: 0.697863]\n",
      "1193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007845] [C loss: 0.697863]\n",
      "1194 [D loss: 0.697864, acc.: 0.00%] [G loss: 0.007314] [C loss: 0.697863]\n",
      "1195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007771] [C loss: 0.697863]\n",
      "1196 [D loss: 0.697863, acc.: 0.00%] [G loss: 0.007396] [C loss: 0.697859]\n",
      "1197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.697859]\n",
      "1198 [D loss: 0.697859, acc.: 0.00%] [G loss: 0.011095] [C loss: 0.697854]\n",
      "1199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007227] [C loss: 0.697854]\n",
      "1200 [D loss: 0.697855, acc.: 0.00%] [G loss: 0.007852] [C loss: 0.697852]\n",
      "1201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007024] [C loss: 0.697852]\n",
      "1202 [D loss: 0.697854, acc.: 0.00%] [G loss: 0.007434] [C loss: 0.697850]\n",
      "1203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007989] [C loss: 0.697850]\n",
      "1204 [D loss: 0.697851, acc.: 0.00%] [G loss: 0.007110] [C loss: 0.697847]\n",
      "1205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009501] [C loss: 0.697847]\n",
      "1206 [D loss: 0.697848, acc.: 0.00%] [G loss: 0.007275] [C loss: 0.697844]\n",
      "1207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006478] [C loss: 0.697844]\n",
      "1208 [D loss: 0.697845, acc.: 0.00%] [G loss: 0.006741] [C loss: 0.697841]\n",
      "1209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006399] [C loss: 0.697841]\n",
      "1210 [D loss: 0.697842, acc.: 0.00%] [G loss: 0.006136] [C loss: 0.697839]\n",
      "1211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007009] [C loss: 0.697839]\n",
      "1212 [D loss: 0.697839, acc.: 0.00%] [G loss: 0.006955] [C loss: 0.697836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009243] [C loss: 0.697836]\n",
      "1214 [D loss: 0.697836, acc.: 0.00%] [G loss: 0.007589] [C loss: 0.697830]\n",
      "1215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007006] [C loss: 0.697830]\n",
      "1216 [D loss: 0.697833, acc.: 0.00%] [G loss: 0.006731] [C loss: 0.697832]\n",
      "1217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006670] [C loss: 0.697832]\n",
      "1218 [D loss: 0.697831, acc.: 0.00%] [G loss: 0.006159] [C loss: 0.697829]\n",
      "1219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015444] [C loss: 0.697829]\n",
      "1220 [D loss: 0.697829, acc.: 0.00%] [G loss: 0.006630] [C loss: 0.697826]\n",
      "1221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007717] [C loss: 0.697826]\n",
      "1222 [D loss: 0.697826, acc.: 0.00%] [G loss: 0.006811] [C loss: 0.697823]\n",
      "1223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007568] [C loss: 0.697823]\n",
      "1224 [D loss: 0.697822, acc.: 0.00%] [G loss: 0.007685] [C loss: 0.697820]\n",
      "1225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006432] [C loss: 0.697820]\n",
      "1226 [D loss: 0.697821, acc.: 0.00%] [G loss: 0.006830] [C loss: 0.697817]\n",
      "1227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006694] [C loss: 0.697817]\n",
      "1228 [D loss: 0.697818, acc.: 0.00%] [G loss: 0.006932] [C loss: 0.697816]\n",
      "1229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019970] [C loss: 0.697816]\n",
      "1230 [D loss: 0.697816, acc.: 0.00%] [G loss: 0.005413] [C loss: 0.697812]\n",
      "1231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008628] [C loss: 0.697812]\n",
      "1232 [D loss: 0.697812, acc.: 0.00%] [G loss: 0.005744] [C loss: 0.697810]\n",
      "1233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005559] [C loss: 0.697810]\n",
      "1234 [D loss: 0.697811, acc.: 0.00%] [G loss: 0.007776] [C loss: 0.697806]\n",
      "1235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004902] [C loss: 0.697806]\n",
      "1236 [D loss: 0.697806, acc.: 0.00%] [G loss: 0.008328] [C loss: 0.697804]\n",
      "1237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005022] [C loss: 0.697804]\n",
      "1238 [D loss: 0.697805, acc.: 0.00%] [G loss: 0.004942] [C loss: 0.697801]\n",
      "1239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005505] [C loss: 0.697801]\n",
      "1240 [D loss: 0.697802, acc.: 0.00%] [G loss: 0.007042] [C loss: 0.697798]\n",
      "1241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005058] [C loss: 0.697798]\n",
      "1242 [D loss: 0.697799, acc.: 0.00%] [G loss: 0.007592] [C loss: 0.697796]\n",
      "1243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006524] [C loss: 0.697796]\n",
      "1244 [D loss: 0.697797, acc.: 0.00%] [G loss: 0.007991] [C loss: 0.697793]\n",
      "1245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005904] [C loss: 0.697793]\n",
      "1246 [D loss: 0.697793, acc.: 0.00%] [G loss: 0.006452] [C loss: 0.697792]\n",
      "1247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007974] [C loss: 0.697792]\n",
      "1248 [D loss: 0.697792, acc.: 0.00%] [G loss: 0.006358] [C loss: 0.697787]\n",
      "1249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008904] [C loss: 0.697787]\n",
      "1250 [D loss: 0.697788, acc.: 0.00%] [G loss: 0.006134] [C loss: 0.697785]\n",
      "1251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005353] [C loss: 0.697785]\n",
      "1252 [D loss: 0.697786, acc.: 0.00%] [G loss: 0.007678] [C loss: 0.697782]\n",
      "1253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008009] [C loss: 0.697782]\n",
      "1254 [D loss: 0.697783, acc.: 0.00%] [G loss: 0.010187] [C loss: 0.697779]\n",
      "1255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008084] [C loss: 0.697779]\n",
      "1256 [D loss: 0.697781, acc.: 0.00%] [G loss: 0.005761] [C loss: 0.697776]\n",
      "1257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005161] [C loss: 0.697776]\n",
      "1258 [D loss: 0.697778, acc.: 0.00%] [G loss: 0.006417] [C loss: 0.697774]\n",
      "1259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008360] [C loss: 0.697774]\n",
      "1260 [D loss: 0.697775, acc.: 0.00%] [G loss: 0.009655] [C loss: 0.697770]\n",
      "1261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005574] [C loss: 0.697770]\n",
      "1262 [D loss: 0.697771, acc.: 0.00%] [G loss: 0.005058] [C loss: 0.697769]\n",
      "1263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009013] [C loss: 0.697769]\n",
      "1264 [D loss: 0.697769, acc.: 0.00%] [G loss: 0.007978] [C loss: 0.697766]\n",
      "1265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007787] [C loss: 0.697766]\n",
      "1266 [D loss: 0.697766, acc.: 0.00%] [G loss: 0.009069] [C loss: 0.697765]\n",
      "1267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006353] [C loss: 0.697765]\n",
      "1268 [D loss: 0.697764, acc.: 0.00%] [G loss: 0.007533] [C loss: 0.697763]\n",
      "1269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005970] [C loss: 0.697763]\n",
      "1270 [D loss: 0.697764, acc.: 0.00%] [G loss: 0.005964] [C loss: 0.697760]\n",
      "1271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008359] [C loss: 0.697760]\n",
      "1272 [D loss: 0.697759, acc.: 0.00%] [G loss: 0.007171] [C loss: 0.697755]\n",
      "1273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009972] [C loss: 0.697755]\n",
      "1274 [D loss: 0.697756, acc.: 0.00%] [G loss: 0.005638] [C loss: 0.697753]\n",
      "1275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017148] [C loss: 0.697753]\n",
      "1276 [D loss: 0.697754, acc.: 0.00%] [G loss: 0.023312] [C loss: 0.697751]\n",
      "1277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007411] [C loss: 0.697751]\n",
      "1278 [D loss: 0.697752, acc.: 0.00%] [G loss: 0.007160] [C loss: 0.697747]\n",
      "1279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006977] [C loss: 0.697747]\n",
      "1280 [D loss: 0.697748, acc.: 0.00%] [G loss: 0.015645] [C loss: 0.697745]\n",
      "1281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005685] [C loss: 0.697745]\n",
      "1282 [D loss: 0.697746, acc.: 0.00%] [G loss: 0.006480] [C loss: 0.697742]\n",
      "1283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008309] [C loss: 0.697742]\n",
      "1284 [D loss: 0.697744, acc.: 0.00%] [G loss: 0.007382] [C loss: 0.697740]\n",
      "1285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010426] [C loss: 0.697740]\n",
      "1286 [D loss: 0.697742, acc.: 0.00%] [G loss: 0.008089] [C loss: 0.697737]\n",
      "1287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012609] [C loss: 0.697737]\n",
      "1288 [D loss: 0.697739, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.697736]\n",
      "1289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009995] [C loss: 0.697736]\n",
      "1290 [D loss: 0.697737, acc.: 0.00%] [G loss: 0.007095] [C loss: 0.697734]\n",
      "1291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008283] [C loss: 0.697734]\n",
      "1292 [D loss: 0.697735, acc.: 0.00%] [G loss: 0.006682] [C loss: 0.697731]\n",
      "1293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009745] [C loss: 0.697731]\n",
      "1294 [D loss: 0.697731, acc.: 0.00%] [G loss: 0.008830] [C loss: 0.697729]\n",
      "1295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006809] [C loss: 0.697729]\n",
      "1296 [D loss: 0.697729, acc.: 0.00%] [G loss: 0.010227] [C loss: 0.697727]\n",
      "1297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008332] [C loss: 0.697727]\n",
      "1298 [D loss: 0.697727, acc.: 0.00%] [G loss: 0.007441] [C loss: 0.697725]\n",
      "1299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008810] [C loss: 0.697725]\n",
      "1300 [D loss: 0.697725, acc.: 0.00%] [G loss: 0.008031] [C loss: 0.697719]\n",
      "1301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.697719]\n",
      "1302 [D loss: 0.697721, acc.: 0.00%] [G loss: 0.007327] [C loss: 0.697718]\n",
      "1303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005746] [C loss: 0.697718]\n",
      "1304 [D loss: 0.697719, acc.: 0.00%] [G loss: 0.006212] [C loss: 0.697716]\n",
      "1305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006510] [C loss: 0.697716]\n",
      "1306 [D loss: 0.697716, acc.: 0.00%] [G loss: 0.008474] [C loss: 0.697714]\n",
      "1307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006253] [C loss: 0.697714]\n",
      "1308 [D loss: 0.697715, acc.: 0.00%] [G loss: 0.005847] [C loss: 0.697710]\n",
      "1309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005167] [C loss: 0.697710]\n",
      "1310 [D loss: 0.697713, acc.: 0.00%] [G loss: 0.006534] [C loss: 0.697708]\n",
      "1311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007271] [C loss: 0.697708]\n",
      "1312 [D loss: 0.697709, acc.: 0.00%] [G loss: 0.007120] [C loss: 0.697707]\n",
      "1313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005728] [C loss: 0.697707]\n",
      "1314 [D loss: 0.697707, acc.: 0.00%] [G loss: 0.006352] [C loss: 0.697703]\n",
      "1315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011944] [C loss: 0.697703]\n",
      "1316 [D loss: 0.697704, acc.: 0.00%] [G loss: 0.008448] [C loss: 0.697702]\n",
      "1317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007917] [C loss: 0.697702]\n",
      "1318 [D loss: 0.697702, acc.: 0.00%] [G loss: 0.006474] [C loss: 0.697701]\n",
      "1319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008948] [C loss: 0.697701]\n",
      "1320 [D loss: 0.697700, acc.: 0.00%] [G loss: 0.007267] [C loss: 0.697697]\n",
      "1321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010520] [C loss: 0.697697]\n",
      "1322 [D loss: 0.697698, acc.: 0.00%] [G loss: 0.018897] [C loss: 0.697693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016487] [C loss: 0.697693]\n",
      "1324 [D loss: 0.697696, acc.: 0.00%] [G loss: 0.007204] [C loss: 0.697693]\n",
      "1325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006336] [C loss: 0.697693]\n",
      "1326 [D loss: 0.697694, acc.: 0.00%] [G loss: 0.009262] [C loss: 0.697689]\n",
      "1327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005009] [C loss: 0.697689]\n",
      "1328 [D loss: 0.697691, acc.: 0.00%] [G loss: 0.007547] [C loss: 0.697688]\n",
      "1329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011042] [C loss: 0.697688]\n",
      "1330 [D loss: 0.697688, acc.: 0.00%] [G loss: 0.006997] [C loss: 0.697685]\n",
      "1331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007272] [C loss: 0.697685]\n",
      "1332 [D loss: 0.697686, acc.: 0.00%] [G loss: 0.006913] [C loss: 0.697680]\n",
      "1333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006780] [C loss: 0.697680]\n",
      "1334 [D loss: 0.697682, acc.: 0.00%] [G loss: 0.008044] [C loss: 0.697680]\n",
      "1335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006102] [C loss: 0.697680]\n",
      "1336 [D loss: 0.697681, acc.: 0.00%] [G loss: 0.011691] [C loss: 0.697677]\n",
      "1337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007482] [C loss: 0.697677]\n",
      "1338 [D loss: 0.697678, acc.: 0.00%] [G loss: 0.006354] [C loss: 0.697675]\n",
      "1339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007216] [C loss: 0.697675]\n",
      "1340 [D loss: 0.697676, acc.: 0.00%] [G loss: 0.006622] [C loss: 0.697672]\n",
      "1341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006996] [C loss: 0.697672]\n",
      "1342 [D loss: 0.697674, acc.: 0.00%] [G loss: 0.004953] [C loss: 0.697672]\n",
      "1343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006044] [C loss: 0.697672]\n",
      "1344 [D loss: 0.697672, acc.: 0.00%] [G loss: 0.010310] [C loss: 0.697669]\n",
      "1345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008431] [C loss: 0.697669]\n",
      "1346 [D loss: 0.697669, acc.: 0.00%] [G loss: 0.005864] [C loss: 0.697667]\n",
      "1347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005933] [C loss: 0.697667]\n",
      "1348 [D loss: 0.697667, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.697664]\n",
      "1349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006833] [C loss: 0.697664]\n",
      "1350 [D loss: 0.697664, acc.: 0.00%] [G loss: 0.005298] [C loss: 0.697661]\n",
      "1351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005096] [C loss: 0.697661]\n",
      "1352 [D loss: 0.697661, acc.: 0.00%] [G loss: 0.008435] [C loss: 0.697659]\n",
      "1353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007118] [C loss: 0.697659]\n",
      "1354 [D loss: 0.697658, acc.: 0.00%] [G loss: 0.006555] [C loss: 0.697658]\n",
      "1355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006824] [C loss: 0.697658]\n",
      "1356 [D loss: 0.697659, acc.: 0.00%] [G loss: 0.004863] [C loss: 0.697655]\n",
      "1357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005168] [C loss: 0.697656]\n",
      "1358 [D loss: 0.697655, acc.: 0.00%] [G loss: 0.010081] [C loss: 0.697652]\n",
      "1359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.024819] [C loss: 0.697652]\n",
      "1360 [D loss: 0.697653, acc.: 0.00%] [G loss: 0.022503] [C loss: 0.697650]\n",
      "1361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006823] [C loss: 0.697650]\n",
      "1362 [D loss: 0.697650, acc.: 0.00%] [G loss: 0.009871] [C loss: 0.697648]\n",
      "1363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009926] [C loss: 0.697648]\n",
      "1364 [D loss: 0.697648, acc.: 0.00%] [G loss: 0.007532] [C loss: 0.697644]\n",
      "1365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006094] [C loss: 0.697644]\n",
      "1366 [D loss: 0.697644, acc.: 0.00%] [G loss: 0.007855] [C loss: 0.697642]\n",
      "1367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008858] [C loss: 0.697642]\n",
      "1368 [D loss: 0.697643, acc.: 0.00%] [G loss: 0.007152] [C loss: 0.697641]\n",
      "1369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009938] [C loss: 0.697641]\n",
      "1370 [D loss: 0.697642, acc.: 0.00%] [G loss: 0.006748] [C loss: 0.697637]\n",
      "1371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005211] [C loss: 0.697637]\n",
      "1372 [D loss: 0.697639, acc.: 0.00%] [G loss: 0.006972] [C loss: 0.697636]\n",
      "1373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.697636]\n",
      "1374 [D loss: 0.697637, acc.: 0.00%] [G loss: 0.009162] [C loss: 0.697632]\n",
      "1375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022639] [C loss: 0.697632]\n",
      "1376 [D loss: 0.697634, acc.: 0.00%] [G loss: 0.007087] [C loss: 0.697631]\n",
      "1377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005871] [C loss: 0.697631]\n",
      "1378 [D loss: 0.697632, acc.: 0.00%] [G loss: 0.007590] [C loss: 0.697630]\n",
      "1379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006768] [C loss: 0.697630]\n",
      "1380 [D loss: 0.697630, acc.: 0.00%] [G loss: 0.006116] [C loss: 0.697626]\n",
      "1381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007073] [C loss: 0.697626]\n",
      "1382 [D loss: 0.697626, acc.: 0.00%] [G loss: 0.007624] [C loss: 0.697625]\n",
      "1383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021692] [C loss: 0.697625]\n",
      "1384 [D loss: 0.697624, acc.: 0.00%] [G loss: 0.009310] [C loss: 0.697622]\n",
      "1385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008694] [C loss: 0.697622]\n",
      "1386 [D loss: 0.697622, acc.: 0.00%] [G loss: 0.007101] [C loss: 0.697620]\n",
      "1387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009830] [C loss: 0.697620]\n",
      "1388 [D loss: 0.697620, acc.: 0.00%] [G loss: 0.005582] [C loss: 0.697618]\n",
      "1389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005446] [C loss: 0.697618]\n",
      "1390 [D loss: 0.697618, acc.: 0.00%] [G loss: 0.007046] [C loss: 0.697614]\n",
      "1391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007740] [C loss: 0.697614]\n",
      "1392 [D loss: 0.697615, acc.: 0.00%] [G loss: 0.008016] [C loss: 0.697612]\n",
      "1393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005191] [C loss: 0.697612]\n",
      "1394 [D loss: 0.697613, acc.: 0.00%] [G loss: 0.009218] [C loss: 0.697609]\n",
      "1395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006137] [C loss: 0.697609]\n",
      "1396 [D loss: 0.697611, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.697609]\n",
      "1397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005590] [C loss: 0.697609]\n",
      "1398 [D loss: 0.697609, acc.: 0.00%] [G loss: 0.005549] [C loss: 0.697607]\n",
      "1399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007827] [C loss: 0.697607]\n",
      "1400 [D loss: 0.697607, acc.: 0.00%] [G loss: 0.006388] [C loss: 0.697604]\n",
      "1401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005465] [C loss: 0.697604]\n",
      "1402 [D loss: 0.697605, acc.: 0.00%] [G loss: 0.008730] [C loss: 0.697603]\n",
      "1403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006662] [C loss: 0.697603]\n",
      "1404 [D loss: 0.697603, acc.: 0.00%] [G loss: 0.006918] [C loss: 0.697600]\n",
      "1405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007261] [C loss: 0.697600]\n",
      "1406 [D loss: 0.697602, acc.: 0.00%] [G loss: 0.007868] [C loss: 0.697596]\n",
      "1407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007676] [C loss: 0.697596]\n",
      "1408 [D loss: 0.697598, acc.: 0.00%] [G loss: 0.010044] [C loss: 0.697596]\n",
      "1409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011205] [C loss: 0.697596]\n",
      "1410 [D loss: 0.697598, acc.: 0.00%] [G loss: 0.007559] [C loss: 0.697593]\n",
      "1411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007057] [C loss: 0.697593]\n",
      "1412 [D loss: 0.697595, acc.: 0.00%] [G loss: 0.006821] [C loss: 0.697592]\n",
      "1413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006919] [C loss: 0.697592]\n",
      "1414 [D loss: 0.697593, acc.: 0.00%] [G loss: 0.008860] [C loss: 0.697589]\n",
      "1415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008031] [C loss: 0.697589]\n",
      "1416 [D loss: 0.697591, acc.: 0.00%] [G loss: 0.007617] [C loss: 0.697585]\n",
      "1417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007536] [C loss: 0.697585]\n",
      "1418 [D loss: 0.697588, acc.: 6.25%] [G loss: 0.006764] [C loss: 0.697585]\n",
      "1419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006917] [C loss: 0.697585]\n",
      "1420 [D loss: 0.697586, acc.: 0.00%] [G loss: 0.006516] [C loss: 0.697583]\n",
      "1421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005902] [C loss: 0.697583]\n",
      "1422 [D loss: 0.697584, acc.: 0.00%] [G loss: 0.006090] [C loss: 0.697581]\n",
      "1423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006843] [C loss: 0.697581]\n",
      "1424 [D loss: 0.697582, acc.: 0.00%] [G loss: 0.008211] [C loss: 0.697578]\n",
      "1425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007938] [C loss: 0.697578]\n",
      "1426 [D loss: 0.697580, acc.: 0.00%] [G loss: 0.007937] [C loss: 0.697578]\n",
      "1427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006512] [C loss: 0.697578]\n",
      "1428 [D loss: 0.697578, acc.: 0.00%] [G loss: 0.009534] [C loss: 0.697574]\n",
      "1429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009381] [C loss: 0.697574]\n",
      "1430 [D loss: 0.697574, acc.: 0.00%] [G loss: 0.010650] [C loss: 0.697572]\n",
      "1431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008186] [C loss: 0.697572]\n",
      "1432 [D loss: 0.697572, acc.: 0.00%] [G loss: 0.007597] [C loss: 0.697571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010273] [C loss: 0.697571]\n",
      "1434 [D loss: 0.697570, acc.: 0.00%] [G loss: 0.009889] [C loss: 0.697568]\n",
      "1435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006764] [C loss: 0.697568]\n",
      "1436 [D loss: 0.697568, acc.: 0.00%] [G loss: 0.010417] [C loss: 0.697566]\n",
      "1437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007879] [C loss: 0.697566]\n",
      "1438 [D loss: 0.697566, acc.: 0.00%] [G loss: 0.006242] [C loss: 0.697566]\n",
      "1439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006922] [C loss: 0.697566]\n",
      "1440 [D loss: 0.697564, acc.: 0.00%] [G loss: 0.007745] [C loss: 0.697564]\n",
      "1441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007483] [C loss: 0.697564]\n",
      "1442 [D loss: 0.697564, acc.: 0.00%] [G loss: 0.008049] [C loss: 0.697561]\n",
      "1443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005437] [C loss: 0.697561]\n",
      "1444 [D loss: 0.697561, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.697558]\n",
      "1445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004495] [C loss: 0.697558]\n",
      "1446 [D loss: 0.697559, acc.: 0.00%] [G loss: 0.007393] [C loss: 0.697556]\n",
      "1447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006061] [C loss: 0.697556]\n",
      "1448 [D loss: 0.697556, acc.: 0.00%] [G loss: 0.006724] [C loss: 0.697552]\n",
      "1449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006190] [C loss: 0.697552]\n",
      "1450 [D loss: 0.697554, acc.: 0.00%] [G loss: 0.005765] [C loss: 0.697551]\n",
      "1451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011270] [C loss: 0.697551]\n",
      "1452 [D loss: 0.697551, acc.: 0.00%] [G loss: 0.005692] [C loss: 0.697548]\n",
      "1453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006778] [C loss: 0.697548]\n",
      "1454 [D loss: 0.697549, acc.: 0.00%] [G loss: 0.006759] [C loss: 0.697547]\n",
      "1455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006123] [C loss: 0.697547]\n",
      "1456 [D loss: 0.697548, acc.: 0.00%] [G loss: 0.007663] [C loss: 0.697544]\n",
      "1457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004892] [C loss: 0.697544]\n",
      "1458 [D loss: 0.697546, acc.: 0.00%] [G loss: 0.005155] [C loss: 0.697541]\n",
      "1459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005321] [C loss: 0.697541]\n",
      "1460 [D loss: 0.697543, acc.: 0.00%] [G loss: 0.006652] [C loss: 0.697540]\n",
      "1461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004322] [C loss: 0.697540]\n",
      "1462 [D loss: 0.697541, acc.: 0.00%] [G loss: 0.008740] [C loss: 0.697539]\n",
      "1463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007419] [C loss: 0.697539]\n",
      "1464 [D loss: 0.697539, acc.: 0.00%] [G loss: 0.007516] [C loss: 0.697536]\n",
      "1465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009136] [C loss: 0.697536]\n",
      "1466 [D loss: 0.697537, acc.: 0.00%] [G loss: 0.008463] [C loss: 0.697534]\n",
      "1467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007580] [C loss: 0.697534]\n",
      "1468 [D loss: 0.697535, acc.: 0.00%] [G loss: 0.006148] [C loss: 0.697534]\n",
      "1469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008455] [C loss: 0.697534]\n",
      "1470 [D loss: 0.697533, acc.: 0.00%] [G loss: 0.004640] [C loss: 0.697531]\n",
      "1471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009848] [C loss: 0.697531]\n",
      "1472 [D loss: 0.697530, acc.: 0.00%] [G loss: 0.005895] [C loss: 0.697530]\n",
      "1473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006182] [C loss: 0.697530]\n",
      "1474 [D loss: 0.697529, acc.: 0.00%] [G loss: 0.007627] [C loss: 0.697527]\n",
      "1475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006603] [C loss: 0.697527]\n",
      "1476 [D loss: 0.697527, acc.: 0.00%] [G loss: 0.006461] [C loss: 0.697525]\n",
      "1477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006493] [C loss: 0.697525]\n",
      "1478 [D loss: 0.697526, acc.: 0.00%] [G loss: 0.006798] [C loss: 0.697523]\n",
      "1479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008789] [C loss: 0.697523]\n",
      "1480 [D loss: 0.697524, acc.: 0.00%] [G loss: 0.005806] [C loss: 0.697519]\n",
      "1481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006443] [C loss: 0.697519]\n",
      "1482 [D loss: 0.697520, acc.: 0.00%] [G loss: 0.006381] [C loss: 0.697515]\n",
      "1483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005994] [C loss: 0.697515]\n",
      "1484 [D loss: 0.697517, acc.: 0.00%] [G loss: 0.009944] [C loss: 0.697514]\n",
      "1485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006657] [C loss: 0.697514]\n",
      "1486 [D loss: 0.697515, acc.: 0.00%] [G loss: 0.009476] [C loss: 0.697514]\n",
      "1487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007416] [C loss: 0.697514]\n",
      "1488 [D loss: 0.697515, acc.: 0.00%] [G loss: 0.005640] [C loss: 0.697512]\n",
      "1489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005418] [C loss: 0.697512]\n",
      "1490 [D loss: 0.697512, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.697509]\n",
      "1491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014584] [C loss: 0.697509]\n",
      "1492 [D loss: 0.697510, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.697508]\n",
      "1493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005173] [C loss: 0.697508]\n",
      "1494 [D loss: 0.697508, acc.: 0.00%] [G loss: 0.006753] [C loss: 0.697505]\n",
      "1495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.697505]\n",
      "1496 [D loss: 0.697506, acc.: 0.00%] [G loss: 0.009236] [C loss: 0.697501]\n",
      "1497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007839] [C loss: 0.697501]\n",
      "1498 [D loss: 0.697504, acc.: 0.00%] [G loss: 0.006337] [C loss: 0.697502]\n",
      "1499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009804] [C loss: 0.697502]\n",
      "1500 [D loss: 0.697502, acc.: 0.00%] [G loss: 0.006931] [C loss: 0.697501]\n",
      "1501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007416] [C loss: 0.697501]\n",
      "1502 [D loss: 0.697501, acc.: 0.00%] [G loss: 0.008306] [C loss: 0.697496]\n",
      "1503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005340] [C loss: 0.697496]\n",
      "1504 [D loss: 0.697497, acc.: 0.00%] [G loss: 0.010442] [C loss: 0.697496]\n",
      "1505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006541] [C loss: 0.697496]\n",
      "1506 [D loss: 0.697497, acc.: 0.00%] [G loss: 0.006536] [C loss: 0.697493]\n",
      "1507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006558] [C loss: 0.697493]\n",
      "1508 [D loss: 0.697494, acc.: 0.00%] [G loss: 0.006325] [C loss: 0.697491]\n",
      "1509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007658] [C loss: 0.697491]\n",
      "1510 [D loss: 0.697492, acc.: 0.00%] [G loss: 0.006113] [C loss: 0.697488]\n",
      "1511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006505] [C loss: 0.697488]\n",
      "1512 [D loss: 0.697490, acc.: 0.00%] [G loss: 0.007502] [C loss: 0.697487]\n",
      "1513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007110] [C loss: 0.697487]\n",
      "1514 [D loss: 0.697488, acc.: 0.00%] [G loss: 0.008138] [C loss: 0.697486]\n",
      "1515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011752] [C loss: 0.697486]\n",
      "1516 [D loss: 0.697487, acc.: 0.00%] [G loss: 0.007790] [C loss: 0.697485]\n",
      "1517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004983] [C loss: 0.697485]\n",
      "1518 [D loss: 0.697485, acc.: 0.00%] [G loss: 0.004801] [C loss: 0.697482]\n",
      "1519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008033] [C loss: 0.697482]\n",
      "1520 [D loss: 0.697482, acc.: 0.00%] [G loss: 0.007640] [C loss: 0.697481]\n",
      "1521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007132] [C loss: 0.697481]\n",
      "1522 [D loss: 0.697481, acc.: 0.00%] [G loss: 0.012729] [C loss: 0.697477]\n",
      "1523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010590] [C loss: 0.697477]\n",
      "1524 [D loss: 0.697478, acc.: 0.00%] [G loss: 0.006043] [C loss: 0.697476]\n",
      "1525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005985] [C loss: 0.697476]\n",
      "1526 [D loss: 0.697476, acc.: 0.00%] [G loss: 0.008123] [C loss: 0.697474]\n",
      "1527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006986] [C loss: 0.697474]\n",
      "1528 [D loss: 0.697475, acc.: 0.00%] [G loss: 0.005717] [C loss: 0.697472]\n",
      "1529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004934] [C loss: 0.697472]\n",
      "1530 [D loss: 0.697473, acc.: 0.00%] [G loss: 0.007493] [C loss: 0.697471]\n",
      "1531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010799] [C loss: 0.697471]\n",
      "1532 [D loss: 0.697471, acc.: 0.00%] [G loss: 0.005660] [C loss: 0.697470]\n",
      "1533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006085] [C loss: 0.697470]\n",
      "1534 [D loss: 0.697470, acc.: 0.00%] [G loss: 0.006815] [C loss: 0.697466]\n",
      "1535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005302] [C loss: 0.697466]\n",
      "1536 [D loss: 0.697467, acc.: 0.00%] [G loss: 0.015629] [C loss: 0.697464]\n",
      "1537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006554] [C loss: 0.697464]\n",
      "1538 [D loss: 0.697465, acc.: 0.00%] [G loss: 0.006794] [C loss: 0.697463]\n",
      "1539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007997] [C loss: 0.697463]\n",
      "1540 [D loss: 0.697464, acc.: 0.00%] [G loss: 0.008831] [C loss: 0.697458]\n",
      "1541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007732] [C loss: 0.697458]\n",
      "1542 [D loss: 0.697460, acc.: 0.00%] [G loss: 0.006017] [C loss: 0.697459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006721] [C loss: 0.697459]\n",
      "1544 [D loss: 0.697460, acc.: 0.00%] [G loss: 0.010060] [C loss: 0.697458]\n",
      "1545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006470] [C loss: 0.697458]\n",
      "1546 [D loss: 0.697458, acc.: 0.00%] [G loss: 0.005370] [C loss: 0.697454]\n",
      "1547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008461] [C loss: 0.697454]\n",
      "1548 [D loss: 0.697455, acc.: 0.00%] [G loss: 0.007438] [C loss: 0.697453]\n",
      "1549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006267] [C loss: 0.697453]\n",
      "1550 [D loss: 0.697453, acc.: 0.00%] [G loss: 0.005939] [C loss: 0.697453]\n",
      "1551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006627] [C loss: 0.697453]\n",
      "1552 [D loss: 0.697452, acc.: 0.00%] [G loss: 0.005662] [C loss: 0.697446]\n",
      "1553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008363] [C loss: 0.697446]\n",
      "1554 [D loss: 0.697448, acc.: 0.00%] [G loss: 0.012281] [C loss: 0.697448]\n",
      "1555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006156] [C loss: 0.697448]\n",
      "1556 [D loss: 0.697447, acc.: 0.00%] [G loss: 0.006183] [C loss: 0.697445]\n",
      "1557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006353] [C loss: 0.697445]\n",
      "1558 [D loss: 0.697446, acc.: 0.00%] [G loss: 0.007603] [C loss: 0.697443]\n",
      "1559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.697443]\n",
      "1560 [D loss: 0.697444, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.697440]\n",
      "1561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.026667] [C loss: 0.697440]\n",
      "1562 [D loss: 0.697442, acc.: 0.00%] [G loss: 0.007301] [C loss: 0.697438]\n",
      "1563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006934] [C loss: 0.697438]\n",
      "1564 [D loss: 0.697439, acc.: 0.00%] [G loss: 0.006793] [C loss: 0.697436]\n",
      "1565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006303] [C loss: 0.697436]\n",
      "1566 [D loss: 0.697437, acc.: 0.00%] [G loss: 0.005489] [C loss: 0.697434]\n",
      "1567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006000] [C loss: 0.697434]\n",
      "1568 [D loss: 0.697436, acc.: 0.00%] [G loss: 0.005123] [C loss: 0.697435]\n",
      "1569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006545] [C loss: 0.697435]\n",
      "1570 [D loss: 0.697435, acc.: 0.00%] [G loss: 0.007570] [C loss: 0.697431]\n",
      "1571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008262] [C loss: 0.697431]\n",
      "1572 [D loss: 0.697432, acc.: 0.00%] [G loss: 0.004914] [C loss: 0.697429]\n",
      "1573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009383] [C loss: 0.697429]\n",
      "1574 [D loss: 0.697431, acc.: 0.00%] [G loss: 0.007654] [C loss: 0.697427]\n",
      "1575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006837] [C loss: 0.697427]\n",
      "1576 [D loss: 0.697428, acc.: 0.00%] [G loss: 0.007765] [C loss: 0.697427]\n",
      "1577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006834] [C loss: 0.697427]\n",
      "1578 [D loss: 0.697427, acc.: 0.00%] [G loss: 0.005868] [C loss: 0.697424]\n",
      "1579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006151] [C loss: 0.697424]\n",
      "1580 [D loss: 0.697424, acc.: 0.00%] [G loss: 0.007696] [C loss: 0.697422]\n",
      "1581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006744] [C loss: 0.697422]\n",
      "1582 [D loss: 0.697423, acc.: 0.00%] [G loss: 0.008857] [C loss: 0.697422]\n",
      "1583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007128] [C loss: 0.697422]\n",
      "1584 [D loss: 0.697421, acc.: 0.00%] [G loss: 0.006530] [C loss: 0.697420]\n",
      "1585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005180] [C loss: 0.697420]\n",
      "1586 [D loss: 0.697420, acc.: 0.00%] [G loss: 0.005454] [C loss: 0.697418]\n",
      "1587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004704] [C loss: 0.697418]\n",
      "1588 [D loss: 0.697417, acc.: 0.00%] [G loss: 0.008278] [C loss: 0.697414]\n",
      "1589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005862] [C loss: 0.697414]\n",
      "1590 [D loss: 0.697415, acc.: 0.00%] [G loss: 0.006671] [C loss: 0.697413]\n",
      "1591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006335] [C loss: 0.697413]\n",
      "1592 [D loss: 0.697414, acc.: 0.00%] [G loss: 0.006771] [C loss: 0.697412]\n",
      "1593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005869] [C loss: 0.697412]\n",
      "1594 [D loss: 0.697413, acc.: 0.00%] [G loss: 0.009901] [C loss: 0.697409]\n",
      "1595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006620] [C loss: 0.697409]\n",
      "1596 [D loss: 0.697410, acc.: 0.00%] [G loss: 0.009551] [C loss: 0.697407]\n",
      "1597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006416] [C loss: 0.697407]\n",
      "1598 [D loss: 0.697409, acc.: 0.00%] [G loss: 0.006493] [C loss: 0.697406]\n",
      "1599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004854] [C loss: 0.697406]\n",
      "1600 [D loss: 0.697406, acc.: 0.00%] [G loss: 0.007459] [C loss: 0.697404]\n",
      "1601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006985] [C loss: 0.697404]\n",
      "1602 [D loss: 0.697405, acc.: 0.00%] [G loss: 0.009300] [C loss: 0.697401]\n",
      "1603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007235] [C loss: 0.697401]\n",
      "1604 [D loss: 0.697403, acc.: 0.00%] [G loss: 0.005170] [C loss: 0.697400]\n",
      "1605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008639] [C loss: 0.697400]\n",
      "1606 [D loss: 0.697401, acc.: 0.00%] [G loss: 0.009784] [C loss: 0.697398]\n",
      "1607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005602] [C loss: 0.697398]\n",
      "1608 [D loss: 0.697399, acc.: 0.00%] [G loss: 0.007037] [C loss: 0.697396]\n",
      "1609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007397] [C loss: 0.697396]\n",
      "1610 [D loss: 0.697397, acc.: 0.00%] [G loss: 0.010606] [C loss: 0.697394]\n",
      "1611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004593] [C loss: 0.697394]\n",
      "1612 [D loss: 0.697395, acc.: 0.00%] [G loss: 0.006235] [C loss: 0.697391]\n",
      "1613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010982] [C loss: 0.697391]\n",
      "1614 [D loss: 0.697393, acc.: 0.00%] [G loss: 0.007342] [C loss: 0.697389]\n",
      "1615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007097] [C loss: 0.697389]\n",
      "1616 [D loss: 0.697391, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.697389]\n",
      "1617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008440] [C loss: 0.697389]\n",
      "1618 [D loss: 0.697390, acc.: 0.00%] [G loss: 0.005988] [C loss: 0.697387]\n",
      "1619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005217] [C loss: 0.697387]\n",
      "1620 [D loss: 0.697388, acc.: 0.00%] [G loss: 0.006432] [C loss: 0.697386]\n",
      "1621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013634] [C loss: 0.697386]\n",
      "1622 [D loss: 0.697386, acc.: 0.00%] [G loss: 0.004909] [C loss: 0.697384]\n",
      "1623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007368] [C loss: 0.697384]\n",
      "1624 [D loss: 0.697384, acc.: 0.00%] [G loss: 0.007508] [C loss: 0.697384]\n",
      "1625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006595] [C loss: 0.697384]\n",
      "1626 [D loss: 0.697383, acc.: 0.00%] [G loss: 0.008507] [C loss: 0.697380]\n",
      "1627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006598] [C loss: 0.697380]\n",
      "1628 [D loss: 0.697380, acc.: 0.00%] [G loss: 0.021300] [C loss: 0.697377]\n",
      "1629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006907] [C loss: 0.697377]\n",
      "1630 [D loss: 0.697377, acc.: 0.00%] [G loss: 0.006233] [C loss: 0.697375]\n",
      "1631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005316] [C loss: 0.697375]\n",
      "1632 [D loss: 0.697376, acc.: 0.00%] [G loss: 0.008842] [C loss: 0.697376]\n",
      "1633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012957] [C loss: 0.697376]\n",
      "1634 [D loss: 0.697375, acc.: 0.00%] [G loss: 0.016830] [C loss: 0.697373]\n",
      "1635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006580] [C loss: 0.697373]\n",
      "1636 [D loss: 0.697373, acc.: 0.00%] [G loss: 0.003866] [C loss: 0.697372]\n",
      "1637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006375] [C loss: 0.697372]\n",
      "1638 [D loss: 0.697372, acc.: 0.00%] [G loss: 0.011064] [C loss: 0.697370]\n",
      "1639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004971] [C loss: 0.697370]\n",
      "1640 [D loss: 0.697371, acc.: 0.00%] [G loss: 0.008245] [C loss: 0.697369]\n",
      "1641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004815] [C loss: 0.697369]\n",
      "1642 [D loss: 0.697368, acc.: 0.00%] [G loss: 0.008886] [C loss: 0.697366]\n",
      "1643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005619] [C loss: 0.697366]\n",
      "1644 [D loss: 0.697367, acc.: 0.00%] [G loss: 0.009061] [C loss: 0.697364]\n",
      "1645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005643] [C loss: 0.697364]\n",
      "1646 [D loss: 0.697364, acc.: 0.00%] [G loss: 0.006312] [C loss: 0.697361]\n",
      "1647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007813] [C loss: 0.697361]\n",
      "1648 [D loss: 0.697363, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.697360]\n",
      "1649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006312] [C loss: 0.697360]\n",
      "1650 [D loss: 0.697361, acc.: 0.00%] [G loss: 0.007355] [C loss: 0.697358]\n",
      "1651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007948] [C loss: 0.697358]\n",
      "1652 [D loss: 0.697358, acc.: 0.00%] [G loss: 0.007476] [C loss: 0.697357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006611] [C loss: 0.697357]\n",
      "1654 [D loss: 0.697358, acc.: 0.00%] [G loss: 0.012801] [C loss: 0.697354]\n",
      "1655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008027] [C loss: 0.697354]\n",
      "1656 [D loss: 0.697355, acc.: 0.00%] [G loss: 0.007041] [C loss: 0.697354]\n",
      "1657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006252] [C loss: 0.697354]\n",
      "1658 [D loss: 0.697354, acc.: 0.00%] [G loss: 0.006814] [C loss: 0.697351]\n",
      "1659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009377] [C loss: 0.697351]\n",
      "1660 [D loss: 0.697352, acc.: 0.00%] [G loss: 0.007566] [C loss: 0.697351]\n",
      "1661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004994] [C loss: 0.697351]\n",
      "1662 [D loss: 0.697351, acc.: 0.00%] [G loss: 0.010739] [C loss: 0.697349]\n",
      "1663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007389] [C loss: 0.697349]\n",
      "1664 [D loss: 0.697349, acc.: 0.00%] [G loss: 0.005411] [C loss: 0.697347]\n",
      "1665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006789] [C loss: 0.697347]\n",
      "1666 [D loss: 0.697348, acc.: 0.00%] [G loss: 0.009461] [C loss: 0.697344]\n",
      "1667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011242] [C loss: 0.697344]\n",
      "1668 [D loss: 0.697345, acc.: 0.00%] [G loss: 0.007789] [C loss: 0.697343]\n",
      "1669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011029] [C loss: 0.697343]\n",
      "1670 [D loss: 0.697344, acc.: 0.00%] [G loss: 0.007054] [C loss: 0.697341]\n",
      "1671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005888] [C loss: 0.697341]\n",
      "1672 [D loss: 0.697341, acc.: 0.00%] [G loss: 0.007470] [C loss: 0.697338]\n",
      "1673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005220] [C loss: 0.697338]\n",
      "1674 [D loss: 0.697340, acc.: 0.00%] [G loss: 0.005444] [C loss: 0.697338]\n",
      "1675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007378] [C loss: 0.697338]\n",
      "1676 [D loss: 0.697340, acc.: 0.00%] [G loss: 0.004985] [C loss: 0.697336]\n",
      "1677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005697] [C loss: 0.697336]\n",
      "1678 [D loss: 0.697337, acc.: 0.00%] [G loss: 0.007618] [C loss: 0.697334]\n",
      "1679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005470] [C loss: 0.697334]\n",
      "1680 [D loss: 0.697335, acc.: 0.00%] [G loss: 0.008575] [C loss: 0.697329]\n",
      "1681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013168] [C loss: 0.697329]\n",
      "1682 [D loss: 0.697334, acc.: 0.00%] [G loss: 0.006741] [C loss: 0.697331]\n",
      "1683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006312] [C loss: 0.697331]\n",
      "1684 [D loss: 0.697331, acc.: 0.00%] [G loss: 0.004934] [C loss: 0.697327]\n",
      "1685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005487] [C loss: 0.697327]\n",
      "1686 [D loss: 0.697329, acc.: 0.00%] [G loss: 0.006529] [C loss: 0.697328]\n",
      "1687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005456] [C loss: 0.697328]\n",
      "1688 [D loss: 0.697329, acc.: 0.00%] [G loss: 0.006947] [C loss: 0.697326]\n",
      "1689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007433] [C loss: 0.697326]\n",
      "1690 [D loss: 0.697325, acc.: 0.00%] [G loss: 0.015815] [C loss: 0.697325]\n",
      "1691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005670] [C loss: 0.697325]\n",
      "1692 [D loss: 0.697325, acc.: 0.00%] [G loss: 0.008900] [C loss: 0.697324]\n",
      "1693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008530] [C loss: 0.697324]\n",
      "1694 [D loss: 0.697323, acc.: 0.00%] [G loss: 0.016170] [C loss: 0.697320]\n",
      "1695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007692] [C loss: 0.697320]\n",
      "1696 [D loss: 0.697321, acc.: 0.00%] [G loss: 0.007329] [C loss: 0.697319]\n",
      "1697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007130] [C loss: 0.697319]\n",
      "1698 [D loss: 0.697319, acc.: 0.00%] [G loss: 0.006697] [C loss: 0.697316]\n",
      "1699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005358] [C loss: 0.697316]\n",
      "1700 [D loss: 0.697317, acc.: 0.00%] [G loss: 0.005964] [C loss: 0.697317]\n",
      "1701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008660] [C loss: 0.697317]\n",
      "1702 [D loss: 0.697316, acc.: 0.00%] [G loss: 0.006147] [C loss: 0.697315]\n",
      "1703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006165] [C loss: 0.697315]\n",
      "1704 [D loss: 0.697315, acc.: 0.00%] [G loss: 0.007231] [C loss: 0.697313]\n",
      "1705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005497] [C loss: 0.697313]\n",
      "1706 [D loss: 0.697314, acc.: 0.00%] [G loss: 0.008220] [C loss: 0.697310]\n",
      "1707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015048] [C loss: 0.697310]\n",
      "1708 [D loss: 0.697311, acc.: 0.00%] [G loss: 0.007093] [C loss: 0.697307]\n",
      "1709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010519] [C loss: 0.697307]\n",
      "1710 [D loss: 0.697309, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.697307]\n",
      "1711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006587] [C loss: 0.697307]\n",
      "1712 [D loss: 0.697307, acc.: 0.00%] [G loss: 0.007687] [C loss: 0.697305]\n",
      "1713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005819] [C loss: 0.697305]\n",
      "1714 [D loss: 0.697306, acc.: 0.00%] [G loss: 0.004270] [C loss: 0.697305]\n",
      "1715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006930] [C loss: 0.697305]\n",
      "1716 [D loss: 0.697305, acc.: 0.00%] [G loss: 0.005338] [C loss: 0.697300]\n",
      "1717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006442] [C loss: 0.697300]\n",
      "1718 [D loss: 0.697302, acc.: 0.00%] [G loss: 0.023015] [C loss: 0.697300]\n",
      "1719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007661] [C loss: 0.697300]\n",
      "1720 [D loss: 0.697301, acc.: 0.00%] [G loss: 0.007069] [C loss: 0.697298]\n",
      "1721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006662] [C loss: 0.697298]\n",
      "1722 [D loss: 0.697299, acc.: 0.00%] [G loss: 0.005230] [C loss: 0.697297]\n",
      "1723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008591] [C loss: 0.697297]\n",
      "1724 [D loss: 0.697298, acc.: 0.00%] [G loss: 0.006167] [C loss: 0.697296]\n",
      "1725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007036] [C loss: 0.697296]\n",
      "1726 [D loss: 0.697298, acc.: 0.00%] [G loss: 0.006353] [C loss: 0.697294]\n",
      "1727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006624] [C loss: 0.697294]\n",
      "1728 [D loss: 0.697294, acc.: 0.00%] [G loss: 0.014934] [C loss: 0.697292]\n",
      "1729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006828] [C loss: 0.697292]\n",
      "1730 [D loss: 0.697292, acc.: 0.00%] [G loss: 0.006148] [C loss: 0.697290]\n",
      "1731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007681] [C loss: 0.697290]\n",
      "1732 [D loss: 0.697291, acc.: 0.00%] [G loss: 0.007314] [C loss: 0.697290]\n",
      "1733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005656] [C loss: 0.697290]\n",
      "1734 [D loss: 0.697290, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.697286]\n",
      "1735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.697286]\n",
      "1736 [D loss: 0.697288, acc.: 0.00%] [G loss: 0.008859] [C loss: 0.697285]\n",
      "1737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007874] [C loss: 0.697285]\n",
      "1738 [D loss: 0.697286, acc.: 0.00%] [G loss: 0.006962] [C loss: 0.697284]\n",
      "1739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006280] [C loss: 0.697284]\n",
      "1740 [D loss: 0.697284, acc.: 0.00%] [G loss: 0.004767] [C loss: 0.697282]\n",
      "1741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011126] [C loss: 0.697282]\n",
      "1742 [D loss: 0.697282, acc.: 0.00%] [G loss: 0.005134] [C loss: 0.697281]\n",
      "1743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004550] [C loss: 0.697281]\n",
      "1744 [D loss: 0.697281, acc.: 0.00%] [G loss: 0.009931] [C loss: 0.697279]\n",
      "1745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005971] [C loss: 0.697279]\n",
      "1746 [D loss: 0.697279, acc.: 0.00%] [G loss: 0.005252] [C loss: 0.697277]\n",
      "1747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011154] [C loss: 0.697277]\n",
      "1748 [D loss: 0.697278, acc.: 0.00%] [G loss: 0.010081] [C loss: 0.697274]\n",
      "1749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009077] [C loss: 0.697274]\n",
      "1750 [D loss: 0.697276, acc.: 0.00%] [G loss: 0.004990] [C loss: 0.697273]\n",
      "1751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007678] [C loss: 0.697273]\n",
      "1752 [D loss: 0.697274, acc.: 0.00%] [G loss: 0.006395] [C loss: 0.697273]\n",
      "1753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005834] [C loss: 0.697273]\n",
      "1754 [D loss: 0.697273, acc.: 0.00%] [G loss: 0.005534] [C loss: 0.697271]\n",
      "1755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007570] [C loss: 0.697271]\n",
      "1756 [D loss: 0.697272, acc.: 0.00%] [G loss: 0.009257] [C loss: 0.697269]\n",
      "1757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008163] [C loss: 0.697269]\n",
      "1758 [D loss: 0.697270, acc.: 0.00%] [G loss: 0.005765] [C loss: 0.697268]\n",
      "1759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006128] [C loss: 0.697268]\n",
      "1760 [D loss: 0.697269, acc.: 0.00%] [G loss: 0.015184] [C loss: 0.697263]\n",
      "1761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005369] [C loss: 0.697263]\n",
      "1762 [D loss: 0.697266, acc.: 0.00%] [G loss: 0.006861] [C loss: 0.697264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008415] [C loss: 0.697264]\n",
      "1764 [D loss: 0.697265, acc.: 0.00%] [G loss: 0.010574] [C loss: 0.697262]\n",
      "1765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007752] [C loss: 0.697262]\n",
      "1766 [D loss: 0.697262, acc.: 0.00%] [G loss: 0.006657] [C loss: 0.697260]\n",
      "1767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007009] [C loss: 0.697260]\n",
      "1768 [D loss: 0.697261, acc.: 0.00%] [G loss: 0.007972] [C loss: 0.697258]\n",
      "1769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007066] [C loss: 0.697258]\n",
      "1770 [D loss: 0.697259, acc.: 0.00%] [G loss: 0.006524] [C loss: 0.697256]\n",
      "1771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005331] [C loss: 0.697256]\n",
      "1772 [D loss: 0.697258, acc.: 0.00%] [G loss: 0.005381] [C loss: 0.697254]\n",
      "1773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008806] [C loss: 0.697254]\n",
      "1774 [D loss: 0.697256, acc.: 0.00%] [G loss: 0.008823] [C loss: 0.697254]\n",
      "1775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007237] [C loss: 0.697254]\n",
      "1776 [D loss: 0.697255, acc.: 0.00%] [G loss: 0.004764] [C loss: 0.697252]\n",
      "1777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009758] [C loss: 0.697252]\n",
      "1778 [D loss: 0.697252, acc.: 0.00%] [G loss: 0.008295] [C loss: 0.697253]\n",
      "1779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006397] [C loss: 0.697253]\n",
      "1780 [D loss: 0.697252, acc.: 0.00%] [G loss: 0.007789] [C loss: 0.697250]\n",
      "1781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007484] [C loss: 0.697250]\n",
      "1782 [D loss: 0.697250, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.697247]\n",
      "1783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007585] [C loss: 0.697247]\n",
      "1784 [D loss: 0.697248, acc.: 0.00%] [G loss: 0.021950] [C loss: 0.697246]\n",
      "1785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008523] [C loss: 0.697246]\n",
      "1786 [D loss: 0.697246, acc.: 0.00%] [G loss: 0.006435] [C loss: 0.697242]\n",
      "1787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005721] [C loss: 0.697242]\n",
      "1788 [D loss: 0.697247, acc.: 6.25%] [G loss: 0.009679] [C loss: 0.697244]\n",
      "1789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009569] [C loss: 0.697244]\n",
      "1790 [D loss: 0.697243, acc.: 0.00%] [G loss: 0.008750] [C loss: 0.697241]\n",
      "1791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005383] [C loss: 0.697241]\n",
      "1792 [D loss: 0.697242, acc.: 0.00%] [G loss: 0.005518] [C loss: 0.697240]\n",
      "1793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007907] [C loss: 0.697240]\n",
      "1794 [D loss: 0.697240, acc.: 0.00%] [G loss: 0.007478] [C loss: 0.697237]\n",
      "1795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015018] [C loss: 0.697237]\n",
      "1796 [D loss: 0.697237, acc.: 0.00%] [G loss: 0.004323] [C loss: 0.697237]\n",
      "1797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006899] [C loss: 0.697237]\n",
      "1798 [D loss: 0.697238, acc.: 0.00%] [G loss: 0.008374] [C loss: 0.697233]\n",
      "1799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008071] [C loss: 0.697233]\n",
      "1800 [D loss: 0.697235, acc.: 0.00%] [G loss: 0.009072] [C loss: 0.697232]\n",
      "1801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006481] [C loss: 0.697232]\n",
      "1802 [D loss: 0.697234, acc.: 0.00%] [G loss: 0.009725] [C loss: 0.697232]\n",
      "1803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004503] [C loss: 0.697232]\n",
      "1804 [D loss: 0.697233, acc.: 0.00%] [G loss: 0.008017] [C loss: 0.697231]\n",
      "1805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009468] [C loss: 0.697231]\n",
      "1806 [D loss: 0.697231, acc.: 0.00%] [G loss: 0.007641] [C loss: 0.697228]\n",
      "1807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005449] [C loss: 0.697228]\n",
      "1808 [D loss: 0.697229, acc.: 0.00%] [G loss: 0.006075] [C loss: 0.697226]\n",
      "1809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006770] [C loss: 0.697226]\n",
      "1810 [D loss: 0.697227, acc.: 0.00%] [G loss: 0.004833] [C loss: 0.697226]\n",
      "1811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004922] [C loss: 0.697226]\n",
      "1812 [D loss: 0.697227, acc.: 0.00%] [G loss: 0.005809] [C loss: 0.697225]\n",
      "1813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.697225]\n",
      "1814 [D loss: 0.697225, acc.: 0.00%] [G loss: 0.005560] [C loss: 0.697223]\n",
      "1815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004668] [C loss: 0.697223]\n",
      "1816 [D loss: 0.697223, acc.: 0.00%] [G loss: 0.006039] [C loss: 0.697220]\n",
      "1817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013272] [C loss: 0.697220]\n",
      "1818 [D loss: 0.697222, acc.: 0.00%] [G loss: 0.007642] [C loss: 0.697219]\n",
      "1819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006876] [C loss: 0.697219]\n",
      "1820 [D loss: 0.697220, acc.: 0.00%] [G loss: 0.009559] [C loss: 0.697216]\n",
      "1821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007271] [C loss: 0.697216]\n",
      "1822 [D loss: 0.697218, acc.: 0.00%] [G loss: 0.003989] [C loss: 0.697216]\n",
      "1823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007496] [C loss: 0.697216]\n",
      "1824 [D loss: 0.697217, acc.: 0.00%] [G loss: 0.006645] [C loss: 0.697214]\n",
      "1825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007433] [C loss: 0.697214]\n",
      "1826 [D loss: 0.697215, acc.: 0.00%] [G loss: 0.006875] [C loss: 0.697212]\n",
      "1827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007058] [C loss: 0.697212]\n",
      "1828 [D loss: 0.697213, acc.: 0.00%] [G loss: 0.009920] [C loss: 0.697211]\n",
      "1829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007241] [C loss: 0.697211]\n",
      "1830 [D loss: 0.697212, acc.: 0.00%] [G loss: 0.008095] [C loss: 0.697211]\n",
      "1831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008149] [C loss: 0.697211]\n",
      "1832 [D loss: 0.697211, acc.: 0.00%] [G loss: 0.008289] [C loss: 0.697211]\n",
      "1833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004814] [C loss: 0.697211]\n",
      "1834 [D loss: 0.697210, acc.: 0.00%] [G loss: 0.006099] [C loss: 0.697206]\n",
      "1835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014056] [C loss: 0.697206]\n",
      "1836 [D loss: 0.697208, acc.: 0.00%] [G loss: 0.005692] [C loss: 0.697205]\n",
      "1837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004576] [C loss: 0.697205]\n",
      "1838 [D loss: 0.697206, acc.: 0.00%] [G loss: 0.005337] [C loss: 0.697206]\n",
      "1839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006241] [C loss: 0.697206]\n",
      "1840 [D loss: 0.697206, acc.: 0.00%] [G loss: 0.006997] [C loss: 0.697202]\n",
      "1841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004915] [C loss: 0.697202]\n",
      "1842 [D loss: 0.697202, acc.: 0.00%] [G loss: 0.006366] [C loss: 0.697202]\n",
      "1843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009115] [C loss: 0.697202]\n",
      "1844 [D loss: 0.697202, acc.: 0.00%] [G loss: 0.007126] [C loss: 0.697198]\n",
      "1845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008451] [C loss: 0.697198]\n",
      "1846 [D loss: 0.697199, acc.: 0.00%] [G loss: 0.006400] [C loss: 0.697198]\n",
      "1847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005028] [C loss: 0.697198]\n",
      "1848 [D loss: 0.697200, acc.: 0.00%] [G loss: 0.009480] [C loss: 0.697195]\n",
      "1849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006749] [C loss: 0.697195]\n",
      "1850 [D loss: 0.697197, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.697196]\n",
      "1851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006811] [C loss: 0.697196]\n",
      "1852 [D loss: 0.697196, acc.: 0.00%] [G loss: 0.007300] [C loss: 0.697193]\n",
      "1853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007736] [C loss: 0.697193]\n",
      "1854 [D loss: 0.697195, acc.: 0.00%] [G loss: 0.005072] [C loss: 0.697191]\n",
      "1855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.697191]\n",
      "1856 [D loss: 0.697192, acc.: 0.00%] [G loss: 0.007376] [C loss: 0.697187]\n",
      "1857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005519] [C loss: 0.697187]\n",
      "1858 [D loss: 0.697189, acc.: 0.00%] [G loss: 0.007090] [C loss: 0.697188]\n",
      "1859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007357] [C loss: 0.697188]\n",
      "1860 [D loss: 0.697189, acc.: 0.00%] [G loss: 0.005413] [C loss: 0.697188]\n",
      "1861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005254] [C loss: 0.697188]\n",
      "1862 [D loss: 0.697189, acc.: 0.00%] [G loss: 0.006807] [C loss: 0.697186]\n",
      "1863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.697186]\n",
      "1864 [D loss: 0.697187, acc.: 0.00%] [G loss: 0.006060] [C loss: 0.697182]\n",
      "1865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008890] [C loss: 0.697182]\n",
      "1866 [D loss: 0.697184, acc.: 0.00%] [G loss: 0.006198] [C loss: 0.697182]\n",
      "1867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006495] [C loss: 0.697182]\n",
      "1868 [D loss: 0.697183, acc.: 0.00%] [G loss: 0.007424] [C loss: 0.697180]\n",
      "1869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006947] [C loss: 0.697180]\n",
      "1870 [D loss: 0.697181, acc.: 0.00%] [G loss: 0.008596] [C loss: 0.697179]\n",
      "1871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006105] [C loss: 0.697179]\n",
      "1872 [D loss: 0.697180, acc.: 0.00%] [G loss: 0.007163] [C loss: 0.697179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005775] [C loss: 0.697179]\n",
      "1874 [D loss: 0.697178, acc.: 0.00%] [G loss: 0.006402] [C loss: 0.697177]\n",
      "1875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004107] [C loss: 0.697177]\n",
      "1876 [D loss: 0.697178, acc.: 0.00%] [G loss: 0.008365] [C loss: 0.697176]\n",
      "1877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.697176]\n",
      "1878 [D loss: 0.697176, acc.: 0.00%] [G loss: 0.011925] [C loss: 0.697173]\n",
      "1879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010931] [C loss: 0.697173]\n",
      "1880 [D loss: 0.697174, acc.: 0.00%] [G loss: 0.013679] [C loss: 0.697172]\n",
      "1881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019031] [C loss: 0.697172]\n",
      "1882 [D loss: 0.697173, acc.: 0.00%] [G loss: 0.008724] [C loss: 0.697169]\n",
      "1883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009146] [C loss: 0.697169]\n",
      "1884 [D loss: 0.697170, acc.: 0.00%] [G loss: 0.008625] [C loss: 0.697166]\n",
      "1885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005940] [C loss: 0.697166]\n",
      "1886 [D loss: 0.697167, acc.: 0.00%] [G loss: 0.008754] [C loss: 0.697165]\n",
      "1887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005936] [C loss: 0.697165]\n",
      "1888 [D loss: 0.697166, acc.: 0.00%] [G loss: 0.008045] [C loss: 0.697166]\n",
      "1889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008666] [C loss: 0.697166]\n",
      "1890 [D loss: 0.697167, acc.: 0.00%] [G loss: 0.010142] [C loss: 0.697161]\n",
      "1891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004811] [C loss: 0.697161]\n",
      "1892 [D loss: 0.697163, acc.: 0.00%] [G loss: 0.008615] [C loss: 0.697159]\n",
      "1893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006410] [C loss: 0.697159]\n",
      "1894 [D loss: 0.697161, acc.: 0.00%] [G loss: 0.006258] [C loss: 0.697161]\n",
      "1895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010006] [C loss: 0.697161]\n",
      "1896 [D loss: 0.697161, acc.: 0.00%] [G loss: 0.007945] [C loss: 0.697158]\n",
      "1897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007623] [C loss: 0.697158]\n",
      "1898 [D loss: 0.697159, acc.: 0.00%] [G loss: 0.009419] [C loss: 0.697158]\n",
      "1899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006673] [C loss: 0.697158]\n",
      "1900 [D loss: 0.697158, acc.: 0.00%] [G loss: 0.005450] [C loss: 0.697156]\n",
      "1901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008157] [C loss: 0.697156]\n",
      "1902 [D loss: 0.697156, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.697155]\n",
      "1903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006783] [C loss: 0.697155]\n",
      "1904 [D loss: 0.697155, acc.: 0.00%] [G loss: 0.005923] [C loss: 0.697154]\n",
      "1905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014985] [C loss: 0.697154]\n",
      "1906 [D loss: 0.697155, acc.: 0.00%] [G loss: 0.006572] [C loss: 0.697152]\n",
      "1907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005139] [C loss: 0.697152]\n",
      "1908 [D loss: 0.697152, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.697149]\n",
      "1909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005611] [C loss: 0.697149]\n",
      "1910 [D loss: 0.697150, acc.: 0.00%] [G loss: 0.003833] [C loss: 0.697148]\n",
      "1911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018532] [C loss: 0.697148]\n",
      "1912 [D loss: 0.697149, acc.: 0.00%] [G loss: 0.006841] [C loss: 0.697147]\n",
      "1913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006652] [C loss: 0.697147]\n",
      "1914 [D loss: 0.697148, acc.: 0.00%] [G loss: 0.005589] [C loss: 0.697144]\n",
      "1915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004525] [C loss: 0.697144]\n",
      "1916 [D loss: 0.697145, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.697143]\n",
      "1917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006903] [C loss: 0.697143]\n",
      "1918 [D loss: 0.697144, acc.: 0.00%] [G loss: 0.010143] [C loss: 0.697141]\n",
      "1919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005094] [C loss: 0.697141]\n",
      "1920 [D loss: 0.697143, acc.: 0.00%] [G loss: 0.006147] [C loss: 0.697140]\n",
      "1921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007047] [C loss: 0.697140]\n",
      "1922 [D loss: 0.697141, acc.: 0.00%] [G loss: 0.006427] [C loss: 0.697139]\n",
      "1923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006466] [C loss: 0.697139]\n",
      "1924 [D loss: 0.697140, acc.: 0.00%] [G loss: 0.006281] [C loss: 0.697138]\n",
      "1925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006078] [C loss: 0.697138]\n",
      "1926 [D loss: 0.697138, acc.: 0.00%] [G loss: 0.008169] [C loss: 0.697134]\n",
      "1927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006349] [C loss: 0.697134]\n",
      "1928 [D loss: 0.697137, acc.: 0.00%] [G loss: 0.010043] [C loss: 0.697133]\n",
      "1929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007082] [C loss: 0.697133]\n",
      "1930 [D loss: 0.697135, acc.: 0.00%] [G loss: 0.008268] [C loss: 0.697134]\n",
      "1931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007009] [C loss: 0.697134]\n",
      "1932 [D loss: 0.697135, acc.: 0.00%] [G loss: 0.006482] [C loss: 0.697132]\n",
      "1933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007624] [C loss: 0.697132]\n",
      "1934 [D loss: 0.697132, acc.: 0.00%] [G loss: 0.006265] [C loss: 0.697130]\n",
      "1935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005516] [C loss: 0.697130]\n",
      "1936 [D loss: 0.697131, acc.: 0.00%] [G loss: 0.008383] [C loss: 0.697128]\n",
      "1937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008377] [C loss: 0.697128]\n",
      "1938 [D loss: 0.697130, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.697127]\n",
      "1939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008152] [C loss: 0.697127]\n",
      "1940 [D loss: 0.697128, acc.: 0.00%] [G loss: 0.005151] [C loss: 0.697126]\n",
      "1941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004880] [C loss: 0.697126]\n",
      "1942 [D loss: 0.697127, acc.: 0.00%] [G loss: 0.005766] [C loss: 0.697124]\n",
      "1943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006720] [C loss: 0.697124]\n",
      "1944 [D loss: 0.697124, acc.: 0.00%] [G loss: 0.006193] [C loss: 0.697124]\n",
      "1945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009228] [C loss: 0.697124]\n",
      "1946 [D loss: 0.697125, acc.: 0.00%] [G loss: 0.005606] [C loss: 0.697120]\n",
      "1947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008248] [C loss: 0.697120]\n",
      "1948 [D loss: 0.697121, acc.: 0.00%] [G loss: 0.006579] [C loss: 0.697122]\n",
      "1949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005884] [C loss: 0.697122]\n",
      "1950 [D loss: 0.697121, acc.: 0.00%] [G loss: 0.006267] [C loss: 0.697119]\n",
      "1951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006813] [C loss: 0.697119]\n",
      "1952 [D loss: 0.697119, acc.: 0.00%] [G loss: 0.005729] [C loss: 0.697117]\n",
      "1953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005200] [C loss: 0.697117]\n",
      "1954 [D loss: 0.697117, acc.: 0.00%] [G loss: 0.005958] [C loss: 0.697114]\n",
      "1955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006686] [C loss: 0.697114]\n",
      "1956 [D loss: 0.697115, acc.: 0.00%] [G loss: 0.007013] [C loss: 0.697114]\n",
      "1957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.697114]\n",
      "1958 [D loss: 0.697116, acc.: 0.00%] [G loss: 0.006868] [C loss: 0.697112]\n",
      "1959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004664] [C loss: 0.697112]\n",
      "1960 [D loss: 0.697113, acc.: 0.00%] [G loss: 0.012692] [C loss: 0.697112]\n",
      "1961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009619] [C loss: 0.697112]\n",
      "1962 [D loss: 0.697113, acc.: 0.00%] [G loss: 0.012786] [C loss: 0.697109]\n",
      "1963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005835] [C loss: 0.697109]\n",
      "1964 [D loss: 0.697110, acc.: 0.00%] [G loss: 0.005975] [C loss: 0.697107]\n",
      "1965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005757] [C loss: 0.697107]\n",
      "1966 [D loss: 0.697109, acc.: 0.00%] [G loss: 0.005686] [C loss: 0.697107]\n",
      "1967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007265] [C loss: 0.697107]\n",
      "1968 [D loss: 0.697108, acc.: 0.00%] [G loss: 0.005993] [C loss: 0.697105]\n",
      "1969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005183] [C loss: 0.697105]\n",
      "1970 [D loss: 0.697107, acc.: 0.00%] [G loss: 0.015402] [C loss: 0.697103]\n",
      "1971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005801] [C loss: 0.697103]\n",
      "1972 [D loss: 0.697104, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.697101]\n",
      "1973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004641] [C loss: 0.697101]\n",
      "1974 [D loss: 0.697102, acc.: 0.00%] [G loss: 0.008745] [C loss: 0.697102]\n",
      "1975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007348] [C loss: 0.697102]\n",
      "1976 [D loss: 0.697103, acc.: 0.00%] [G loss: 0.007477] [C loss: 0.697098]\n",
      "1977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009504] [C loss: 0.697099]\n",
      "1978 [D loss: 0.697100, acc.: 0.00%] [G loss: 0.005273] [C loss: 0.697097]\n",
      "1979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004991] [C loss: 0.697097]\n",
      "1980 [D loss: 0.697098, acc.: 0.00%] [G loss: 0.006941] [C loss: 0.697098]\n",
      "1981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.697098]\n",
      "1982 [D loss: 0.697099, acc.: 0.00%] [G loss: 0.005975] [C loss: 0.697095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007345] [C loss: 0.697095]\n",
      "1984 [D loss: 0.697096, acc.: 0.00%] [G loss: 0.006882] [C loss: 0.697095]\n",
      "1985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.697095]\n",
      "1986 [D loss: 0.697094, acc.: 0.00%] [G loss: 0.007182] [C loss: 0.697093]\n",
      "1987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007261] [C loss: 0.697093]\n",
      "1988 [D loss: 0.697094, acc.: 0.00%] [G loss: 0.004621] [C loss: 0.697090]\n",
      "1989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005917] [C loss: 0.697090]\n",
      "1990 [D loss: 0.697092, acc.: 0.00%] [G loss: 0.005879] [C loss: 0.697089]\n",
      "1991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008802] [C loss: 0.697089]\n",
      "1992 [D loss: 0.697091, acc.: 0.00%] [G loss: 0.010544] [C loss: 0.697089]\n",
      "1993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007124] [C loss: 0.697089]\n",
      "1994 [D loss: 0.697089, acc.: 0.00%] [G loss: 0.007288] [C loss: 0.697088]\n",
      "1995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006258] [C loss: 0.697088]\n",
      "1996 [D loss: 0.697088, acc.: 0.00%] [G loss: 0.005340] [C loss: 0.697086]\n",
      "1997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006351] [C loss: 0.697086]\n",
      "1998 [D loss: 0.697087, acc.: 0.00%] [G loss: 0.005866] [C loss: 0.697084]\n",
      "1999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.697084]\n",
      "2000 [D loss: 0.697085, acc.: 0.00%] [G loss: 0.006307] [C loss: 0.697083]\n",
      "2001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009170] [C loss: 0.697083]\n",
      "2002 [D loss: 0.697084, acc.: 0.00%] [G loss: 0.005733] [C loss: 0.697079]\n",
      "2003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005480] [C loss: 0.697079]\n",
      "2004 [D loss: 0.697081, acc.: 0.00%] [G loss: 0.006077] [C loss: 0.697079]\n",
      "2005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008536] [C loss: 0.697079]\n",
      "2006 [D loss: 0.697081, acc.: 0.00%] [G loss: 0.008872] [C loss: 0.697078]\n",
      "2007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007769] [C loss: 0.697078]\n",
      "2008 [D loss: 0.697079, acc.: 0.00%] [G loss: 0.006244] [C loss: 0.697078]\n",
      "2009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006495] [C loss: 0.697078]\n",
      "2010 [D loss: 0.697078, acc.: 0.00%] [G loss: 0.006941] [C loss: 0.697075]\n",
      "2011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007083] [C loss: 0.697075]\n",
      "2012 [D loss: 0.697077, acc.: 0.00%] [G loss: 0.004322] [C loss: 0.697074]\n",
      "2013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004406] [C loss: 0.697074]\n",
      "2014 [D loss: 0.697074, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.697074]\n",
      "2015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005479] [C loss: 0.697074]\n",
      "2016 [D loss: 0.697075, acc.: 0.00%] [G loss: 0.006101] [C loss: 0.697072]\n",
      "2017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004532] [C loss: 0.697072]\n",
      "2018 [D loss: 0.697073, acc.: 0.00%] [G loss: 0.006797] [C loss: 0.697070]\n",
      "2019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007967] [C loss: 0.697070]\n",
      "2020 [D loss: 0.697070, acc.: 0.00%] [G loss: 0.004539] [C loss: 0.697066]\n",
      "2021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005031] [C loss: 0.697066]\n",
      "2022 [D loss: 0.697069, acc.: 0.00%] [G loss: 0.006288] [C loss: 0.697067]\n",
      "2023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006727] [C loss: 0.697067]\n",
      "2024 [D loss: 0.697068, acc.: 0.00%] [G loss: 0.006692] [C loss: 0.697066]\n",
      "2025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005400] [C loss: 0.697066]\n",
      "2026 [D loss: 0.697066, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.697063]\n",
      "2027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010560] [C loss: 0.697063]\n",
      "2028 [D loss: 0.697065, acc.: 0.00%] [G loss: 0.005646] [C loss: 0.697062]\n",
      "2029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.697062]\n",
      "2030 [D loss: 0.697063, acc.: 0.00%] [G loss: 0.006465] [C loss: 0.697061]\n",
      "2031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006608] [C loss: 0.697061]\n",
      "2032 [D loss: 0.697063, acc.: 0.00%] [G loss: 0.003885] [C loss: 0.697060]\n",
      "2033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007334] [C loss: 0.697060]\n",
      "2034 [D loss: 0.697060, acc.: 0.00%] [G loss: 0.005592] [C loss: 0.697059]\n",
      "2035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005430] [C loss: 0.697059]\n",
      "2036 [D loss: 0.697059, acc.: 0.00%] [G loss: 0.004348] [C loss: 0.697060]\n",
      "2037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010800] [C loss: 0.697060]\n",
      "2038 [D loss: 0.697058, acc.: 0.00%] [G loss: 0.008148] [C loss: 0.697055]\n",
      "2039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020109] [C loss: 0.697055]\n",
      "2040 [D loss: 0.697055, acc.: 0.00%] [G loss: 0.006374] [C loss: 0.697054]\n",
      "2041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005257] [C loss: 0.697054]\n",
      "2042 [D loss: 0.697055, acc.: 0.00%] [G loss: 0.006909] [C loss: 0.697052]\n",
      "2043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006600] [C loss: 0.697052]\n",
      "2044 [D loss: 0.697053, acc.: 0.00%] [G loss: 0.004492] [C loss: 0.697052]\n",
      "2045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005609] [C loss: 0.697052]\n",
      "2046 [D loss: 0.697053, acc.: 0.00%] [G loss: 0.008319] [C loss: 0.697052]\n",
      "2047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006761] [C loss: 0.697052]\n",
      "2048 [D loss: 0.697052, acc.: 0.00%] [G loss: 0.006762] [C loss: 0.697049]\n",
      "2049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004400] [C loss: 0.697049]\n",
      "2050 [D loss: 0.697049, acc.: 0.00%] [G loss: 0.008058] [C loss: 0.697047]\n",
      "2051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004944] [C loss: 0.697047]\n",
      "2052 [D loss: 0.697049, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.697046]\n",
      "2053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007205] [C loss: 0.697046]\n",
      "2054 [D loss: 0.697047, acc.: 0.00%] [G loss: 0.005840] [C loss: 0.697045]\n",
      "2055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005435] [C loss: 0.697045]\n",
      "2056 [D loss: 0.697045, acc.: 0.00%] [G loss: 0.006356] [C loss: 0.697044]\n",
      "2057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007405] [C loss: 0.697044]\n",
      "2058 [D loss: 0.697044, acc.: 0.00%] [G loss: 0.011614] [C loss: 0.697041]\n",
      "2059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004092] [C loss: 0.697041]\n",
      "2060 [D loss: 0.697042, acc.: 0.00%] [G loss: 0.007286] [C loss: 0.697039]\n",
      "2061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008822] [C loss: 0.697039]\n",
      "2062 [D loss: 0.697043, acc.: 0.00%] [G loss: 0.006034] [C loss: 0.697038]\n",
      "2063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006805] [C loss: 0.697038]\n",
      "2064 [D loss: 0.697040, acc.: 0.00%] [G loss: 0.005625] [C loss: 0.697038]\n",
      "2065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006820] [C loss: 0.697038]\n",
      "2066 [D loss: 0.697038, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.697036]\n",
      "2067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007414] [C loss: 0.697036]\n",
      "2068 [D loss: 0.697037, acc.: 0.00%] [G loss: 0.003983] [C loss: 0.697035]\n",
      "2069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007798] [C loss: 0.697035]\n",
      "2070 [D loss: 0.697035, acc.: 0.00%] [G loss: 0.006154] [C loss: 0.697033]\n",
      "2071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015461] [C loss: 0.697033]\n",
      "2072 [D loss: 0.697034, acc.: 0.00%] [G loss: 0.005400] [C loss: 0.697032]\n",
      "2073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.697032]\n",
      "2074 [D loss: 0.697033, acc.: 0.00%] [G loss: 0.006018] [C loss: 0.697029]\n",
      "2075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008940] [C loss: 0.697029]\n",
      "2076 [D loss: 0.697031, acc.: 0.00%] [G loss: 0.005753] [C loss: 0.697028]\n",
      "2077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007656] [C loss: 0.697028]\n",
      "2078 [D loss: 0.697029, acc.: 0.00%] [G loss: 0.006614] [C loss: 0.697028]\n",
      "2079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005271] [C loss: 0.697028]\n",
      "2080 [D loss: 0.697029, acc.: 0.00%] [G loss: 0.005529] [C loss: 0.697027]\n",
      "2081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006991] [C loss: 0.697027]\n",
      "2082 [D loss: 0.697028, acc.: 0.00%] [G loss: 0.025594] [C loss: 0.697025]\n",
      "2083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008170] [C loss: 0.697025]\n",
      "2084 [D loss: 0.697026, acc.: 0.00%] [G loss: 0.005139] [C loss: 0.697024]\n",
      "2085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005773] [C loss: 0.697024]\n",
      "2086 [D loss: 0.697024, acc.: 0.00%] [G loss: 0.007916] [C loss: 0.697024]\n",
      "2087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006292] [C loss: 0.697024]\n",
      "2088 [D loss: 0.697023, acc.: 0.00%] [G loss: 0.005397] [C loss: 0.697022]\n",
      "2089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005199] [C loss: 0.697022]\n",
      "2090 [D loss: 0.697022, acc.: 0.00%] [G loss: 0.005052] [C loss: 0.697021]\n",
      "2091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008145] [C loss: 0.697021]\n",
      "2092 [D loss: 0.697021, acc.: 0.00%] [G loss: 0.006974] [C loss: 0.697017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008375] [C loss: 0.697017]\n",
      "2094 [D loss: 0.697018, acc.: 0.00%] [G loss: 0.006490] [C loss: 0.697016]\n",
      "2095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004710] [C loss: 0.697016]\n",
      "2096 [D loss: 0.697018, acc.: 0.00%] [G loss: 0.004918] [C loss: 0.697016]\n",
      "2097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005221] [C loss: 0.697016]\n",
      "2098 [D loss: 0.697016, acc.: 0.00%] [G loss: 0.005802] [C loss: 0.697013]\n",
      "2099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005289] [C loss: 0.697013]\n",
      "2100 [D loss: 0.697014, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.697011]\n",
      "2101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.697011]\n",
      "2102 [D loss: 0.697013, acc.: 0.00%] [G loss: 0.006256] [C loss: 0.697010]\n",
      "2103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008697] [C loss: 0.697010]\n",
      "2104 [D loss: 0.697012, acc.: 0.00%] [G loss: 0.008974] [C loss: 0.697009]\n",
      "2105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005792] [C loss: 0.697009]\n",
      "2106 [D loss: 0.697011, acc.: 0.00%] [G loss: 0.005804] [C loss: 0.697008]\n",
      "2107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005470] [C loss: 0.697008]\n",
      "2108 [D loss: 0.697009, acc.: 0.00%] [G loss: 0.006934] [C loss: 0.697006]\n",
      "2109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007315] [C loss: 0.697006]\n",
      "2110 [D loss: 0.697007, acc.: 0.00%] [G loss: 0.004980] [C loss: 0.697006]\n",
      "2111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.697006]\n",
      "2112 [D loss: 0.697007, acc.: 0.00%] [G loss: 0.006321] [C loss: 0.697006]\n",
      "2113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005585] [C loss: 0.697006]\n",
      "2114 [D loss: 0.697006, acc.: 0.00%] [G loss: 0.007785] [C loss: 0.697003]\n",
      "2115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007724] [C loss: 0.697003]\n",
      "2116 [D loss: 0.697003, acc.: 0.00%] [G loss: 0.005462] [C loss: 0.697000]\n",
      "2117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006293] [C loss: 0.697000]\n",
      "2118 [D loss: 0.697002, acc.: 0.00%] [G loss: 0.007513] [C loss: 0.696998]\n",
      "2119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006695] [C loss: 0.696998]\n",
      "2120 [D loss: 0.697000, acc.: 0.00%] [G loss: 0.005942] [C loss: 0.696998]\n",
      "2121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009748] [C loss: 0.696998]\n",
      "2122 [D loss: 0.697000, acc.: 0.00%] [G loss: 0.004643] [C loss: 0.696998]\n",
      "2123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006292] [C loss: 0.696998]\n",
      "2124 [D loss: 0.696999, acc.: 0.00%] [G loss: 0.022189] [C loss: 0.696997]\n",
      "2125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005609] [C loss: 0.696997]\n",
      "2126 [D loss: 0.696997, acc.: 0.00%] [G loss: 0.007786] [C loss: 0.696995]\n",
      "2127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007443] [C loss: 0.696995]\n",
      "2128 [D loss: 0.696995, acc.: 0.00%] [G loss: 0.005920] [C loss: 0.696994]\n",
      "2129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007917] [C loss: 0.696994]\n",
      "2130 [D loss: 0.696994, acc.: 0.00%] [G loss: 0.009388] [C loss: 0.696990]\n",
      "2131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014464] [C loss: 0.696990]\n",
      "2132 [D loss: 0.696992, acc.: 0.00%] [G loss: 0.006961] [C loss: 0.696991]\n",
      "2133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007102] [C loss: 0.696991]\n",
      "2134 [D loss: 0.696992, acc.: 0.00%] [G loss: 0.008548] [C loss: 0.696989]\n",
      "2135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007603] [C loss: 0.696989]\n",
      "2136 [D loss: 0.696991, acc.: 0.00%] [G loss: 0.006092] [C loss: 0.696987]\n",
      "2137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005249] [C loss: 0.696987]\n",
      "2138 [D loss: 0.696988, acc.: 0.00%] [G loss: 0.011212] [C loss: 0.696987]\n",
      "2139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008878] [C loss: 0.696987]\n",
      "2140 [D loss: 0.696988, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.696986]\n",
      "2141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006919] [C loss: 0.696986]\n",
      "2142 [D loss: 0.696987, acc.: 0.00%] [G loss: 0.005322] [C loss: 0.696984]\n",
      "2143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007475] [C loss: 0.696984]\n",
      "2144 [D loss: 0.696986, acc.: 0.00%] [G loss: 0.006649] [C loss: 0.696981]\n",
      "2145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006418] [C loss: 0.696981]\n",
      "2146 [D loss: 0.696983, acc.: 0.00%] [G loss: 0.006225] [C loss: 0.696982]\n",
      "2147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011684] [C loss: 0.696982]\n",
      "2148 [D loss: 0.696983, acc.: 0.00%] [G loss: 0.009826] [C loss: 0.696979]\n",
      "2149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006864] [C loss: 0.696979]\n",
      "2150 [D loss: 0.696981, acc.: 0.00%] [G loss: 0.006764] [C loss: 0.696978]\n",
      "2151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004905] [C loss: 0.696978]\n",
      "2152 [D loss: 0.696979, acc.: 0.00%] [G loss: 0.007126] [C loss: 0.696977]\n",
      "2153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006475] [C loss: 0.696977]\n",
      "2154 [D loss: 0.696978, acc.: 0.00%] [G loss: 0.004907] [C loss: 0.696976]\n",
      "2155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005422] [C loss: 0.696976]\n",
      "2156 [D loss: 0.696976, acc.: 0.00%] [G loss: 0.007292] [C loss: 0.696973]\n",
      "2157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008699] [C loss: 0.696973]\n",
      "2158 [D loss: 0.696974, acc.: 0.00%] [G loss: 0.008716] [C loss: 0.696974]\n",
      "2159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007357] [C loss: 0.696974]\n",
      "2160 [D loss: 0.696974, acc.: 0.00%] [G loss: 0.006971] [C loss: 0.696971]\n",
      "2161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007881] [C loss: 0.696971]\n",
      "2162 [D loss: 0.696972, acc.: 0.00%] [G loss: 0.011085] [C loss: 0.696972]\n",
      "2163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006263] [C loss: 0.696972]\n",
      "2164 [D loss: 0.696972, acc.: 0.00%] [G loss: 0.007936] [C loss: 0.696969]\n",
      "2165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005558] [C loss: 0.696969]\n",
      "2166 [D loss: 0.696970, acc.: 0.00%] [G loss: 0.008710] [C loss: 0.696968]\n",
      "2167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005725] [C loss: 0.696968]\n",
      "2168 [D loss: 0.696968, acc.: 0.00%] [G loss: 0.006039] [C loss: 0.696968]\n",
      "2169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006013] [C loss: 0.696968]\n",
      "2170 [D loss: 0.696969, acc.: 0.00%] [G loss: 0.019820] [C loss: 0.696965]\n",
      "2171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007943] [C loss: 0.696965]\n",
      "2172 [D loss: 0.696965, acc.: 0.00%] [G loss: 0.005484] [C loss: 0.696963]\n",
      "2173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005967] [C loss: 0.696963]\n",
      "2174 [D loss: 0.696964, acc.: 0.00%] [G loss: 0.005567] [C loss: 0.696962]\n",
      "2175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007223] [C loss: 0.696962]\n",
      "2176 [D loss: 0.696962, acc.: 0.00%] [G loss: 0.007439] [C loss: 0.696961]\n",
      "2177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005391] [C loss: 0.696961]\n",
      "2178 [D loss: 0.696964, acc.: 0.00%] [G loss: 0.007033] [C loss: 0.696958]\n",
      "2179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006070] [C loss: 0.696958]\n",
      "2180 [D loss: 0.696960, acc.: 0.00%] [G loss: 0.003860] [C loss: 0.696957]\n",
      "2181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009154] [C loss: 0.696957]\n",
      "2182 [D loss: 0.696959, acc.: 0.00%] [G loss: 0.005765] [C loss: 0.696957]\n",
      "2183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006082] [C loss: 0.696957]\n",
      "2184 [D loss: 0.696958, acc.: 0.00%] [G loss: 0.005404] [C loss: 0.696955]\n",
      "2185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006986] [C loss: 0.696955]\n",
      "2186 [D loss: 0.696958, acc.: 0.00%] [G loss: 0.003912] [C loss: 0.696954]\n",
      "2187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009097] [C loss: 0.696954]\n",
      "2188 [D loss: 0.696956, acc.: 0.00%] [G loss: 0.007985] [C loss: 0.696953]\n",
      "2189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009584] [C loss: 0.696953]\n",
      "2190 [D loss: 0.696954, acc.: 0.00%] [G loss: 0.005714] [C loss: 0.696951]\n",
      "2191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006065] [C loss: 0.696951]\n",
      "2192 [D loss: 0.696952, acc.: 0.00%] [G loss: 0.006544] [C loss: 0.696951]\n",
      "2193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011949] [C loss: 0.696951]\n",
      "2194 [D loss: 0.696952, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.696949]\n",
      "2195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006151] [C loss: 0.696949]\n",
      "2196 [D loss: 0.696950, acc.: 0.00%] [G loss: 0.028396] [C loss: 0.696949]\n",
      "2197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007227] [C loss: 0.696949]\n",
      "2198 [D loss: 0.696949, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.696946]\n",
      "2199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008275] [C loss: 0.696946]\n",
      "2200 [D loss: 0.696948, acc.: 0.00%] [G loss: 0.007932] [C loss: 0.696945]\n",
      "2201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007135] [C loss: 0.696945]\n",
      "2202 [D loss: 0.696945, acc.: 0.00%] [G loss: 0.007519] [C loss: 0.696945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006256] [C loss: 0.696945]\n",
      "2204 [D loss: 0.696946, acc.: 0.00%] [G loss: 0.009165] [C loss: 0.696943]\n",
      "2205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007446] [C loss: 0.696943]\n",
      "2206 [D loss: 0.696944, acc.: 0.00%] [G loss: 0.007595] [C loss: 0.696941]\n",
      "2207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005345] [C loss: 0.696941]\n",
      "2208 [D loss: 0.696943, acc.: 0.00%] [G loss: 0.007053] [C loss: 0.696940]\n",
      "2209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011415] [C loss: 0.696940]\n",
      "2210 [D loss: 0.696941, acc.: 0.00%] [G loss: 0.011522] [C loss: 0.696939]\n",
      "2211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006444] [C loss: 0.696939]\n",
      "2212 [D loss: 0.696941, acc.: 0.00%] [G loss: 0.006866] [C loss: 0.696935]\n",
      "2213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012496] [C loss: 0.696935]\n",
      "2214 [D loss: 0.696939, acc.: 0.00%] [G loss: 0.004933] [C loss: 0.696938]\n",
      "2215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005653] [C loss: 0.696938]\n",
      "2216 [D loss: 0.696938, acc.: 0.00%] [G loss: 0.007118] [C loss: 0.696936]\n",
      "2217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005948] [C loss: 0.696936]\n",
      "2218 [D loss: 0.696936, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.696935]\n",
      "2219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008487] [C loss: 0.696935]\n",
      "2220 [D loss: 0.696935, acc.: 0.00%] [G loss: 0.006571] [C loss: 0.696932]\n",
      "2221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005308] [C loss: 0.696932]\n",
      "2222 [D loss: 0.696933, acc.: 0.00%] [G loss: 0.006102] [C loss: 0.696930]\n",
      "2223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004566] [C loss: 0.696930]\n",
      "2224 [D loss: 0.696931, acc.: 0.00%] [G loss: 0.005081] [C loss: 0.696930]\n",
      "2225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009977] [C loss: 0.696930]\n",
      "2226 [D loss: 0.696930, acc.: 0.00%] [G loss: 0.006588] [C loss: 0.696929]\n",
      "2227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005424] [C loss: 0.696929]\n",
      "2228 [D loss: 0.696929, acc.: 0.00%] [G loss: 0.005878] [C loss: 0.696927]\n",
      "2229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.696927]\n",
      "2230 [D loss: 0.696928, acc.: 0.00%] [G loss: 0.007162] [C loss: 0.696925]\n",
      "2231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006383] [C loss: 0.696925]\n",
      "2232 [D loss: 0.696926, acc.: 0.00%] [G loss: 0.005436] [C loss: 0.696924]\n",
      "2233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009058] [C loss: 0.696924]\n",
      "2234 [D loss: 0.696926, acc.: 0.00%] [G loss: 0.006059] [C loss: 0.696922]\n",
      "2235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007525] [C loss: 0.696922]\n",
      "2236 [D loss: 0.696923, acc.: 0.00%] [G loss: 0.008055] [C loss: 0.696921]\n",
      "2237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005923] [C loss: 0.696921]\n",
      "2238 [D loss: 0.696922, acc.: 0.00%] [G loss: 0.005556] [C loss: 0.696919]\n",
      "2239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006203] [C loss: 0.696919]\n",
      "2240 [D loss: 0.696921, acc.: 0.00%] [G loss: 0.007042] [C loss: 0.696917]\n",
      "2241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005373] [C loss: 0.696917]\n",
      "2242 [D loss: 0.696919, acc.: 0.00%] [G loss: 0.005473] [C loss: 0.696915]\n",
      "2243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008542] [C loss: 0.696915]\n",
      "2244 [D loss: 0.696918, acc.: 0.00%] [G loss: 0.007325] [C loss: 0.696913]\n",
      "2245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.696913]\n",
      "2246 [D loss: 0.696916, acc.: 0.00%] [G loss: 0.005477] [C loss: 0.696913]\n",
      "2247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009481] [C loss: 0.696913]\n",
      "2248 [D loss: 0.696915, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.696914]\n",
      "2249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004852] [C loss: 0.696914]\n",
      "2250 [D loss: 0.696914, acc.: 0.00%] [G loss: 0.008745] [C loss: 0.696912]\n",
      "2251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006567] [C loss: 0.696912]\n",
      "2252 [D loss: 0.696912, acc.: 0.00%] [G loss: 0.006182] [C loss: 0.696912]\n",
      "2253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007140] [C loss: 0.696912]\n",
      "2254 [D loss: 0.696912, acc.: 0.00%] [G loss: 0.006098] [C loss: 0.696909]\n",
      "2255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006877] [C loss: 0.696909]\n",
      "2256 [D loss: 0.696910, acc.: 0.00%] [G loss: 0.004254] [C loss: 0.696909]\n",
      "2257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014219] [C loss: 0.696909]\n",
      "2258 [D loss: 0.696909, acc.: 0.00%] [G loss: 0.005245] [C loss: 0.696906]\n",
      "2259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005537] [C loss: 0.696906]\n",
      "2260 [D loss: 0.696907, acc.: 0.00%] [G loss: 0.005965] [C loss: 0.696907]\n",
      "2261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004531] [C loss: 0.696907]\n",
      "2262 [D loss: 0.696907, acc.: 0.00%] [G loss: 0.004637] [C loss: 0.696904]\n",
      "2263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005170] [C loss: 0.696904]\n",
      "2264 [D loss: 0.696904, acc.: 0.00%] [G loss: 0.010827] [C loss: 0.696902]\n",
      "2265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010519] [C loss: 0.696902]\n",
      "2266 [D loss: 0.696904, acc.: 0.00%] [G loss: 0.006381] [C loss: 0.696899]\n",
      "2267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.696899]\n",
      "2268 [D loss: 0.696901, acc.: 0.00%] [G loss: 0.004824] [C loss: 0.696899]\n",
      "2269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008968] [C loss: 0.696899]\n",
      "2270 [D loss: 0.696901, acc.: 0.00%] [G loss: 0.008169] [C loss: 0.696897]\n",
      "2271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004756] [C loss: 0.696897]\n",
      "2272 [D loss: 0.696899, acc.: 0.00%] [G loss: 0.006284] [C loss: 0.696897]\n",
      "2273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007094] [C loss: 0.696897]\n",
      "2274 [D loss: 0.696900, acc.: 6.25%] [G loss: 0.005779] [C loss: 0.696897]\n",
      "2275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009334] [C loss: 0.696897]\n",
      "2276 [D loss: 0.696897, acc.: 0.00%] [G loss: 0.007398] [C loss: 0.696895]\n",
      "2277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007120] [C loss: 0.696895]\n",
      "2278 [D loss: 0.696896, acc.: 0.00%] [G loss: 0.008439] [C loss: 0.696894]\n",
      "2279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008147] [C loss: 0.696894]\n",
      "2280 [D loss: 0.696894, acc.: 0.00%] [G loss: 0.006327] [C loss: 0.696893]\n",
      "2281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006591] [C loss: 0.696893]\n",
      "2282 [D loss: 0.696893, acc.: 0.00%] [G loss: 0.007021] [C loss: 0.696893]\n",
      "2283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006589] [C loss: 0.696893]\n",
      "2284 [D loss: 0.696892, acc.: 0.00%] [G loss: 0.012977] [C loss: 0.696890]\n",
      "2285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010938] [C loss: 0.696890]\n",
      "2286 [D loss: 0.696890, acc.: 0.00%] [G loss: 0.005225] [C loss: 0.696888]\n",
      "2287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012380] [C loss: 0.696888]\n",
      "2288 [D loss: 0.696889, acc.: 0.00%] [G loss: 0.006604] [C loss: 0.696887]\n",
      "2289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004928] [C loss: 0.696887]\n",
      "2290 [D loss: 0.696888, acc.: 0.00%] [G loss: 0.006013] [C loss: 0.696886]\n",
      "2291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007428] [C loss: 0.696886]\n",
      "2292 [D loss: 0.696887, acc.: 0.00%] [G loss: 0.006360] [C loss: 0.696884]\n",
      "2293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010382] [C loss: 0.696884]\n",
      "2294 [D loss: 0.696886, acc.: 0.00%] [G loss: 0.006590] [C loss: 0.696883]\n",
      "2295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006610] [C loss: 0.696883]\n",
      "2296 [D loss: 0.696884, acc.: 0.00%] [G loss: 0.016691] [C loss: 0.696882]\n",
      "2297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005833] [C loss: 0.696880]\n",
      "2298 [D loss: 0.696883, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.696881]\n",
      "2299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005439] [C loss: 0.696881]\n",
      "2300 [D loss: 0.696882, acc.: 0.00%] [G loss: 0.010707] [C loss: 0.696879]\n",
      "2301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007560] [C loss: 0.696879]\n",
      "2302 [D loss: 0.696881, acc.: 0.00%] [G loss: 0.008196] [C loss: 0.696877]\n",
      "2303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006204] [C loss: 0.696877]\n",
      "2304 [D loss: 0.696879, acc.: 0.00%] [G loss: 0.006681] [C loss: 0.696877]\n",
      "2305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012672] [C loss: 0.696877]\n",
      "2306 [D loss: 0.696878, acc.: 0.00%] [G loss: 0.005637] [C loss: 0.696874]\n",
      "2307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009620] [C loss: 0.696874]\n",
      "2308 [D loss: 0.696876, acc.: 0.00%] [G loss: 0.007520] [C loss: 0.696873]\n",
      "2309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007144] [C loss: 0.696873]\n",
      "2310 [D loss: 0.696875, acc.: 0.00%] [G loss: 0.008127] [C loss: 0.696872]\n",
      "2311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005087] [C loss: 0.696872]\n",
      "2312 [D loss: 0.696874, acc.: 0.00%] [G loss: 0.004983] [C loss: 0.696872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007298] [C loss: 0.696872]\n",
      "2314 [D loss: 0.696873, acc.: 0.00%] [G loss: 0.005749] [C loss: 0.696870]\n",
      "2315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004670] [C loss: 0.696870]\n",
      "2316 [D loss: 0.696872, acc.: 0.00%] [G loss: 0.005351] [C loss: 0.696870]\n",
      "2317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006623] [C loss: 0.696870]\n",
      "2318 [D loss: 0.696870, acc.: 0.00%] [G loss: 0.004467] [C loss: 0.696868]\n",
      "2319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.696868]\n",
      "2320 [D loss: 0.696869, acc.: 0.00%] [G loss: 0.005246] [C loss: 0.696866]\n",
      "2321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005667] [C loss: 0.696866]\n",
      "2322 [D loss: 0.696867, acc.: 0.00%] [G loss: 0.009056] [C loss: 0.696865]\n",
      "2323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006569] [C loss: 0.696865]\n",
      "2324 [D loss: 0.696866, acc.: 0.00%] [G loss: 0.006079] [C loss: 0.696866]\n",
      "2325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009846] [C loss: 0.696866]\n",
      "2326 [D loss: 0.696865, acc.: 0.00%] [G loss: 0.006772] [C loss: 0.696862]\n",
      "2327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005485] [C loss: 0.696862]\n",
      "2328 [D loss: 0.696863, acc.: 0.00%] [G loss: 0.005598] [C loss: 0.696858]\n",
      "2329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008036] [C loss: 0.696858]\n",
      "2330 [D loss: 0.696860, acc.: 0.00%] [G loss: 0.007456] [C loss: 0.696860]\n",
      "2331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006263] [C loss: 0.696860]\n",
      "2332 [D loss: 0.696861, acc.: 0.00%] [G loss: 0.005663] [C loss: 0.696857]\n",
      "2333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005352] [C loss: 0.696857]\n",
      "2334 [D loss: 0.696859, acc.: 0.00%] [G loss: 0.007881] [C loss: 0.696857]\n",
      "2335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.696857]\n",
      "2336 [D loss: 0.696858, acc.: 0.00%] [G loss: 0.006222] [C loss: 0.696855]\n",
      "2337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010449] [C loss: 0.696855]\n",
      "2338 [D loss: 0.696856, acc.: 0.00%] [G loss: 0.007091] [C loss: 0.696856]\n",
      "2339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005112] [C loss: 0.696856]\n",
      "2340 [D loss: 0.696856, acc.: 0.00%] [G loss: 0.004925] [C loss: 0.696853]\n",
      "2341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006149] [C loss: 0.696853]\n",
      "2342 [D loss: 0.696855, acc.: 0.00%] [G loss: 0.004672] [C loss: 0.696852]\n",
      "2343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007736] [C loss: 0.696852]\n",
      "2344 [D loss: 0.696853, acc.: 0.00%] [G loss: 0.004108] [C loss: 0.696851]\n",
      "2345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.696851]\n",
      "2346 [D loss: 0.696851, acc.: 0.00%] [G loss: 0.004663] [C loss: 0.696850]\n",
      "2347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006201] [C loss: 0.696850]\n",
      "2348 [D loss: 0.696851, acc.: 0.00%] [G loss: 0.015418] [C loss: 0.696845]\n",
      "2349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003922] [C loss: 0.696845]\n",
      "2350 [D loss: 0.696848, acc.: 0.00%] [G loss: 0.006411] [C loss: 0.696845]\n",
      "2351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005565] [C loss: 0.696845]\n",
      "2352 [D loss: 0.696846, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.696845]\n",
      "2353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008217] [C loss: 0.696845]\n",
      "2354 [D loss: 0.696846, acc.: 0.00%] [G loss: 0.007038] [C loss: 0.696843]\n",
      "2355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008215] [C loss: 0.696843]\n",
      "2356 [D loss: 0.696846, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.696843]\n",
      "2357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009481] [C loss: 0.696843]\n",
      "2358 [D loss: 0.696844, acc.: 0.00%] [G loss: 0.015191] [C loss: 0.696841]\n",
      "2359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007289] [C loss: 0.696841]\n",
      "2360 [D loss: 0.696842, acc.: 0.00%] [G loss: 0.006642] [C loss: 0.696841]\n",
      "2361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005766] [C loss: 0.696841]\n",
      "2362 [D loss: 0.696843, acc.: 0.00%] [G loss: 0.007549] [C loss: 0.696840]\n",
      "2363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007185] [C loss: 0.696840]\n",
      "2364 [D loss: 0.696841, acc.: 0.00%] [G loss: 0.006930] [C loss: 0.696838]\n",
      "2365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005149] [C loss: 0.696838]\n",
      "2366 [D loss: 0.696840, acc.: 0.00%] [G loss: 0.005531] [C loss: 0.696836]\n",
      "2367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004955] [C loss: 0.696836]\n",
      "2368 [D loss: 0.696838, acc.: 0.00%] [G loss: 0.005499] [C loss: 0.696833]\n",
      "2369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008883] [C loss: 0.696833]\n",
      "2370 [D loss: 0.696836, acc.: 0.00%] [G loss: 0.005013] [C loss: 0.696833]\n",
      "2371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005828] [C loss: 0.696833]\n",
      "2372 [D loss: 0.696834, acc.: 0.00%] [G loss: 0.006963] [C loss: 0.696833]\n",
      "2373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006693] [C loss: 0.696833]\n",
      "2374 [D loss: 0.696834, acc.: 0.00%] [G loss: 0.007932] [C loss: 0.696831]\n",
      "2375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006064] [C loss: 0.696831]\n",
      "2376 [D loss: 0.696832, acc.: 0.00%] [G loss: 0.006914] [C loss: 0.696831]\n",
      "2377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005844] [C loss: 0.696831]\n",
      "2378 [D loss: 0.696833, acc.: 0.00%] [G loss: 0.005824] [C loss: 0.696828]\n",
      "2379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006709] [C loss: 0.696828]\n",
      "2380 [D loss: 0.696830, acc.: 0.00%] [G loss: 0.005500] [C loss: 0.696828]\n",
      "2381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004633] [C loss: 0.696828]\n",
      "2382 [D loss: 0.696829, acc.: 0.00%] [G loss: 0.007816] [C loss: 0.696826]\n",
      "2383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006429] [C loss: 0.696826]\n",
      "2384 [D loss: 0.696827, acc.: 0.00%] [G loss: 0.006801] [C loss: 0.696825]\n",
      "2385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.696825]\n",
      "2386 [D loss: 0.696826, acc.: 0.00%] [G loss: 0.005197] [C loss: 0.696825]\n",
      "2387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006765] [C loss: 0.696825]\n",
      "2388 [D loss: 0.696826, acc.: 0.00%] [G loss: 0.004864] [C loss: 0.696822]\n",
      "2389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.696822]\n",
      "2390 [D loss: 0.696824, acc.: 0.00%] [G loss: 0.008453] [C loss: 0.696824]\n",
      "2391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006277] [C loss: 0.696824]\n",
      "2392 [D loss: 0.696824, acc.: 0.00%] [G loss: 0.007674] [C loss: 0.696822]\n",
      "2393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.696822]\n",
      "2394 [D loss: 0.696823, acc.: 0.00%] [G loss: 0.013823] [C loss: 0.696819]\n",
      "2395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006499] [C loss: 0.696819]\n",
      "2396 [D loss: 0.696820, acc.: 0.00%] [G loss: 0.006044] [C loss: 0.696816]\n",
      "2397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006481] [C loss: 0.696816]\n",
      "2398 [D loss: 0.696818, acc.: 0.00%] [G loss: 0.006285] [C loss: 0.696818]\n",
      "2399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011108] [C loss: 0.696818]\n",
      "2400 [D loss: 0.696818, acc.: 0.00%] [G loss: 0.005978] [C loss: 0.696816]\n",
      "2401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006758] [C loss: 0.696816]\n",
      "2402 [D loss: 0.696816, acc.: 0.00%] [G loss: 0.008430] [C loss: 0.696815]\n",
      "2403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006055] [C loss: 0.696815]\n",
      "2404 [D loss: 0.696816, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.696815]\n",
      "2405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006739] [C loss: 0.696815]\n",
      "2406 [D loss: 0.696815, acc.: 0.00%] [G loss: 0.005541] [C loss: 0.696810]\n",
      "2407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005742] [C loss: 0.696810]\n",
      "2408 [D loss: 0.696812, acc.: 0.00%] [G loss: 0.013021] [C loss: 0.696811]\n",
      "2409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.696811]\n",
      "2410 [D loss: 0.696811, acc.: 0.00%] [G loss: 0.009856] [C loss: 0.696809]\n",
      "2411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008396] [C loss: 0.696809]\n",
      "2412 [D loss: 0.696810, acc.: 0.00%] [G loss: 0.005110] [C loss: 0.696807]\n",
      "2413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005443] [C loss: 0.696807]\n",
      "2414 [D loss: 0.696808, acc.: 0.00%] [G loss: 0.008559] [C loss: 0.696807]\n",
      "2415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008174] [C loss: 0.696807]\n",
      "2416 [D loss: 0.696808, acc.: 0.00%] [G loss: 0.005819] [C loss: 0.696804]\n",
      "2417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007308] [C loss: 0.696804]\n",
      "2418 [D loss: 0.696807, acc.: 0.00%] [G loss: 0.010575] [C loss: 0.696804]\n",
      "2419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005682] [C loss: 0.696804]\n",
      "2420 [D loss: 0.696805, acc.: 0.00%] [G loss: 0.008619] [C loss: 0.696802]\n",
      "2421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004247] [C loss: 0.696802]\n",
      "2422 [D loss: 0.696804, acc.: 0.00%] [G loss: 0.006138] [C loss: 0.696800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.696800]\n",
      "2424 [D loss: 0.696803, acc.: 0.00%] [G loss: 0.006003] [C loss: 0.696798]\n",
      "2425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005074] [C loss: 0.696798]\n",
      "2426 [D loss: 0.696801, acc.: 0.00%] [G loss: 0.006153] [C loss: 0.696800]\n",
      "2427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007997] [C loss: 0.696800]\n",
      "2428 [D loss: 0.696800, acc.: 0.00%] [G loss: 0.007003] [C loss: 0.696797]\n",
      "2429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005113] [C loss: 0.696797]\n",
      "2430 [D loss: 0.696798, acc.: 0.00%] [G loss: 0.005919] [C loss: 0.696796]\n",
      "2431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015578] [C loss: 0.696796]\n",
      "2432 [D loss: 0.696798, acc.: 0.00%] [G loss: 0.005116] [C loss: 0.696794]\n",
      "2433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006868] [C loss: 0.696794]\n",
      "2434 [D loss: 0.696796, acc.: 0.00%] [G loss: 0.004805] [C loss: 0.696795]\n",
      "2435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006239] [C loss: 0.696795]\n",
      "2436 [D loss: 0.696795, acc.: 0.00%] [G loss: 0.007516] [C loss: 0.696791]\n",
      "2437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007205] [C loss: 0.696791]\n",
      "2438 [D loss: 0.696793, acc.: 0.00%] [G loss: 0.009765] [C loss: 0.696791]\n",
      "2439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005947] [C loss: 0.696791]\n",
      "2440 [D loss: 0.696793, acc.: 0.00%] [G loss: 0.007732] [C loss: 0.696789]\n",
      "2441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008210] [C loss: 0.696789]\n",
      "2442 [D loss: 0.696790, acc.: 0.00%] [G loss: 0.005313] [C loss: 0.696789]\n",
      "2443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005346] [C loss: 0.696789]\n",
      "2444 [D loss: 0.696790, acc.: 0.00%] [G loss: 0.006604] [C loss: 0.696788]\n",
      "2445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006020] [C loss: 0.696788]\n",
      "2446 [D loss: 0.696788, acc.: 0.00%] [G loss: 0.021513] [C loss: 0.696788]\n",
      "2447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006206] [C loss: 0.696788]\n",
      "2448 [D loss: 0.696788, acc.: 0.00%] [G loss: 0.006047] [C loss: 0.696785]\n",
      "2449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006555] [C loss: 0.696785]\n",
      "2450 [D loss: 0.696786, acc.: 0.00%] [G loss: 0.005873] [C loss: 0.696785]\n",
      "2451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007142] [C loss: 0.696785]\n",
      "2452 [D loss: 0.696786, acc.: 0.00%] [G loss: 0.006437] [C loss: 0.696783]\n",
      "2453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006053] [C loss: 0.696783]\n",
      "2454 [D loss: 0.696784, acc.: 0.00%] [G loss: 0.017951] [C loss: 0.696783]\n",
      "2455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009567] [C loss: 0.696783]\n",
      "2456 [D loss: 0.696783, acc.: 0.00%] [G loss: 0.007898] [C loss: 0.696780]\n",
      "2457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005971] [C loss: 0.696780]\n",
      "2458 [D loss: 0.696782, acc.: 0.00%] [G loss: 0.004662] [C loss: 0.696778]\n",
      "2459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006433] [C loss: 0.696778]\n",
      "2460 [D loss: 0.696781, acc.: 0.00%] [G loss: 0.007418] [C loss: 0.696779]\n",
      "2461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004290] [C loss: 0.696779]\n",
      "2462 [D loss: 0.696780, acc.: 0.00%] [G loss: 0.004589] [C loss: 0.696778]\n",
      "2463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004493] [C loss: 0.696778]\n",
      "2464 [D loss: 0.696779, acc.: 0.00%] [G loss: 0.004759] [C loss: 0.696778]\n",
      "2465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007734] [C loss: 0.696778]\n",
      "2466 [D loss: 0.696778, acc.: 0.00%] [G loss: 0.003538] [C loss: 0.696776]\n",
      "2467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006359] [C loss: 0.696776]\n",
      "2468 [D loss: 0.696776, acc.: 0.00%] [G loss: 0.005226] [C loss: 0.696774]\n",
      "2469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004626] [C loss: 0.696774]\n",
      "2470 [D loss: 0.696775, acc.: 0.00%] [G loss: 0.003801] [C loss: 0.696774]\n",
      "2471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005621] [C loss: 0.696774]\n",
      "2472 [D loss: 0.696775, acc.: 0.00%] [G loss: 0.004952] [C loss: 0.696774]\n",
      "2473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009566] [C loss: 0.696774]\n",
      "2474 [D loss: 0.696774, acc.: 0.00%] [G loss: 0.007398] [C loss: 0.696771]\n",
      "2475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004091] [C loss: 0.696771]\n",
      "2476 [D loss: 0.696772, acc.: 0.00%] [G loss: 0.006319] [C loss: 0.696770]\n",
      "2477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005419] [C loss: 0.696770]\n",
      "2478 [D loss: 0.696771, acc.: 0.00%] [G loss: 0.006100] [C loss: 0.696767]\n",
      "2479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006239] [C loss: 0.696767]\n",
      "2480 [D loss: 0.696768, acc.: 0.00%] [G loss: 0.005948] [C loss: 0.696767]\n",
      "2481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005329] [C loss: 0.696767]\n",
      "2482 [D loss: 0.696767, acc.: 0.00%] [G loss: 0.007456] [C loss: 0.696766]\n",
      "2483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009403] [C loss: 0.696766]\n",
      "2484 [D loss: 0.696767, acc.: 0.00%] [G loss: 0.007393] [C loss: 0.696764]\n",
      "2485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005006] [C loss: 0.696764]\n",
      "2486 [D loss: 0.696765, acc.: 0.00%] [G loss: 0.006246] [C loss: 0.696761]\n",
      "2487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010264] [C loss: 0.696761]\n",
      "2488 [D loss: 0.696764, acc.: 0.00%] [G loss: 0.005944] [C loss: 0.696760]\n",
      "2489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006089] [C loss: 0.696760]\n",
      "2490 [D loss: 0.696761, acc.: 0.00%] [G loss: 0.005905] [C loss: 0.696758]\n",
      "2491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.696758]\n",
      "2492 [D loss: 0.696760, acc.: 0.00%] [G loss: 0.013810] [C loss: 0.696758]\n",
      "2493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005896] [C loss: 0.696758]\n",
      "2494 [D loss: 0.696761, acc.: 6.25%] [G loss: 0.006622] [C loss: 0.696758]\n",
      "2495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006925] [C loss: 0.696758]\n",
      "2496 [D loss: 0.696759, acc.: 0.00%] [G loss: 0.005399] [C loss: 0.696757]\n",
      "2497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006551] [C loss: 0.696757]\n",
      "2498 [D loss: 0.696757, acc.: 0.00%] [G loss: 0.009000] [C loss: 0.696754]\n",
      "2499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007290] [C loss: 0.696754]\n",
      "2500 [D loss: 0.696756, acc.: 0.00%] [G loss: 0.005533] [C loss: 0.696753]\n",
      "2501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004421] [C loss: 0.696753]\n",
      "2502 [D loss: 0.696754, acc.: 0.00%] [G loss: 0.006008] [C loss: 0.696752]\n",
      "2503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012049] [C loss: 0.696752]\n",
      "2504 [D loss: 0.696754, acc.: 0.00%] [G loss: 0.005762] [C loss: 0.696750]\n",
      "2505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006146] [C loss: 0.696750]\n",
      "2506 [D loss: 0.696752, acc.: 0.00%] [G loss: 0.004926] [C loss: 0.696749]\n",
      "2507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005113] [C loss: 0.696749]\n",
      "2508 [D loss: 0.696752, acc.: 0.00%] [G loss: 0.006343] [C loss: 0.696749]\n",
      "2509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006321] [C loss: 0.696749]\n",
      "2510 [D loss: 0.696750, acc.: 0.00%] [G loss: 0.007069] [C loss: 0.696746]\n",
      "2511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003619] [C loss: 0.696746]\n",
      "2512 [D loss: 0.696748, acc.: 0.00%] [G loss: 0.009617] [C loss: 0.696745]\n",
      "2513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005929] [C loss: 0.696745]\n",
      "2514 [D loss: 0.696748, acc.: 0.00%] [G loss: 0.005902] [C loss: 0.696744]\n",
      "2515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005283] [C loss: 0.696744]\n",
      "2516 [D loss: 0.696747, acc.: 0.00%] [G loss: 0.005434] [C loss: 0.696745]\n",
      "2517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007260] [C loss: 0.696745]\n",
      "2518 [D loss: 0.696746, acc.: 0.00%] [G loss: 0.011338] [C loss: 0.696743]\n",
      "2519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.696743]\n",
      "2520 [D loss: 0.696743, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.696740]\n",
      "2521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009218] [C loss: 0.696740]\n",
      "2522 [D loss: 0.696742, acc.: 0.00%] [G loss: 0.005908] [C loss: 0.696739]\n",
      "2523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017099] [C loss: 0.696739]\n",
      "2524 [D loss: 0.696740, acc.: 0.00%] [G loss: 0.020507] [C loss: 0.696739]\n",
      "2525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009286] [C loss: 0.696739]\n",
      "2526 [D loss: 0.696738, acc.: 0.00%] [G loss: 0.005295] [C loss: 0.696738]\n",
      "2527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007389] [C loss: 0.696738]\n",
      "2528 [D loss: 0.696739, acc.: 0.00%] [G loss: 0.006462] [C loss: 0.696734]\n",
      "2529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005126] [C loss: 0.696734]\n",
      "2530 [D loss: 0.696736, acc.: 0.00%] [G loss: 0.005497] [C loss: 0.696733]\n",
      "2531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007217] [C loss: 0.696733]\n",
      "2532 [D loss: 0.696735, acc.: 0.00%] [G loss: 0.005166] [C loss: 0.696733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008989] [C loss: 0.696733]\n",
      "2534 [D loss: 0.696734, acc.: 0.00%] [G loss: 0.009887] [C loss: 0.696734]\n",
      "2535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005424] [C loss: 0.696734]\n",
      "2536 [D loss: 0.696734, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.696730]\n",
      "2537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005769] [C loss: 0.696730]\n",
      "2538 [D loss: 0.696732, acc.: 0.00%] [G loss: 0.003996] [C loss: 0.696729]\n",
      "2539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005630] [C loss: 0.696729]\n",
      "2540 [D loss: 0.696729, acc.: 0.00%] [G loss: 0.006550] [C loss: 0.696727]\n",
      "2541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006230] [C loss: 0.696727]\n",
      "2542 [D loss: 0.696728, acc.: 0.00%] [G loss: 0.004430] [C loss: 0.696727]\n",
      "2543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004575] [C loss: 0.696727]\n",
      "2544 [D loss: 0.696728, acc.: 0.00%] [G loss: 0.006137] [C loss: 0.696726]\n",
      "2545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006067] [C loss: 0.696726]\n",
      "2546 [D loss: 0.696728, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.696726]\n",
      "2547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006621] [C loss: 0.696726]\n",
      "2548 [D loss: 0.696727, acc.: 0.00%] [G loss: 0.004194] [C loss: 0.696725]\n",
      "2549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.696725]\n",
      "2550 [D loss: 0.696726, acc.: 0.00%] [G loss: 0.007536] [C loss: 0.696724]\n",
      "2551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006060] [C loss: 0.696724]\n",
      "2552 [D loss: 0.696724, acc.: 0.00%] [G loss: 0.007232] [C loss: 0.696723]\n",
      "2553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006609] [C loss: 0.696723]\n",
      "2554 [D loss: 0.696724, acc.: 0.00%] [G loss: 0.007090] [C loss: 0.696718]\n",
      "2555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006658] [C loss: 0.696718]\n",
      "2556 [D loss: 0.696720, acc.: 0.00%] [G loss: 0.006649] [C loss: 0.696719]\n",
      "2557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006288] [C loss: 0.696719]\n",
      "2558 [D loss: 0.696720, acc.: 0.00%] [G loss: 0.005739] [C loss: 0.696717]\n",
      "2559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023025] [C loss: 0.696717]\n",
      "2560 [D loss: 0.696719, acc.: 0.00%] [G loss: 0.005595] [C loss: 0.696716]\n",
      "2561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006729] [C loss: 0.696716]\n",
      "2562 [D loss: 0.696718, acc.: 0.00%] [G loss: 0.007197] [C loss: 0.696714]\n",
      "2563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004638] [C loss: 0.696714]\n",
      "2564 [D loss: 0.696716, acc.: 0.00%] [G loss: 0.004228] [C loss: 0.696712]\n",
      "2565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005239] [C loss: 0.696712]\n",
      "2566 [D loss: 0.696715, acc.: 0.00%] [G loss: 0.003926] [C loss: 0.696711]\n",
      "2567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007871] [C loss: 0.696711]\n",
      "2568 [D loss: 0.696713, acc.: 0.00%] [G loss: 0.005648] [C loss: 0.696711]\n",
      "2569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005999] [C loss: 0.696711]\n",
      "2570 [D loss: 0.696713, acc.: 0.00%] [G loss: 0.007172] [C loss: 0.696711]\n",
      "2571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004900] [C loss: 0.696711]\n",
      "2572 [D loss: 0.696712, acc.: 0.00%] [G loss: 0.010266] [C loss: 0.696710]\n",
      "2573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.696710]\n",
      "2574 [D loss: 0.696711, acc.: 0.00%] [G loss: 0.005152] [C loss: 0.696709]\n",
      "2575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006813] [C loss: 0.696709]\n",
      "2576 [D loss: 0.696710, acc.: 0.00%] [G loss: 0.007874] [C loss: 0.696707]\n",
      "2577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006015] [C loss: 0.696707]\n",
      "2578 [D loss: 0.696708, acc.: 0.00%] [G loss: 0.006239] [C loss: 0.696707]\n",
      "2579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007295] [C loss: 0.696707]\n",
      "2580 [D loss: 0.696708, acc.: 0.00%] [G loss: 0.012538] [C loss: 0.696706]\n",
      "2581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007168] [C loss: 0.696706]\n",
      "2582 [D loss: 0.696707, acc.: 0.00%] [G loss: 0.007227] [C loss: 0.696705]\n",
      "2583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008028] [C loss: 0.696705]\n",
      "2584 [D loss: 0.696705, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.696702]\n",
      "2585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005403] [C loss: 0.696702]\n",
      "2586 [D loss: 0.696703, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.696700]\n",
      "2587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004669] [C loss: 0.696700]\n",
      "2588 [D loss: 0.696702, acc.: 0.00%] [G loss: 0.005288] [C loss: 0.696701]\n",
      "2589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005796] [C loss: 0.696701]\n",
      "2590 [D loss: 0.696702, acc.: 0.00%] [G loss: 0.006282] [C loss: 0.696699]\n",
      "2591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010307] [C loss: 0.696699]\n",
      "2592 [D loss: 0.696701, acc.: 0.00%] [G loss: 0.005662] [C loss: 0.696696]\n",
      "2593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004014] [C loss: 0.696696]\n",
      "2594 [D loss: 0.696697, acc.: 0.00%] [G loss: 0.004399] [C loss: 0.696697]\n",
      "2595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006123] [C loss: 0.696697]\n",
      "2596 [D loss: 0.696697, acc.: 0.00%] [G loss: 0.005620] [C loss: 0.696696]\n",
      "2597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.696696]\n",
      "2598 [D loss: 0.696696, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.696692]\n",
      "2599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005074] [C loss: 0.696692]\n",
      "2600 [D loss: 0.696695, acc.: 0.00%] [G loss: 0.011289] [C loss: 0.696693]\n",
      "2601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005334] [C loss: 0.696693]\n",
      "2602 [D loss: 0.696695, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.696691]\n",
      "2603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005692] [C loss: 0.696691]\n",
      "2604 [D loss: 0.696693, acc.: 0.00%] [G loss: 0.004817] [C loss: 0.696687]\n",
      "2605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005192] [C loss: 0.696687]\n",
      "2606 [D loss: 0.696690, acc.: 0.00%] [G loss: 0.009813] [C loss: 0.696686]\n",
      "2607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008021] [C loss: 0.696686]\n",
      "2608 [D loss: 0.696689, acc.: 0.00%] [G loss: 0.006109] [C loss: 0.696686]\n",
      "2609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004922] [C loss: 0.696686]\n",
      "2610 [D loss: 0.696689, acc.: 0.00%] [G loss: 0.006804] [C loss: 0.696686]\n",
      "2611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006824] [C loss: 0.696686]\n",
      "2612 [D loss: 0.696688, acc.: 0.00%] [G loss: 0.004910] [C loss: 0.696683]\n",
      "2613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010039] [C loss: 0.696683]\n",
      "2614 [D loss: 0.696685, acc.: 0.00%] [G loss: 0.004824] [C loss: 0.696684]\n",
      "2615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008166] [C loss: 0.696684]\n",
      "2616 [D loss: 0.696685, acc.: 0.00%] [G loss: 0.004750] [C loss: 0.696682]\n",
      "2617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.696682]\n",
      "2618 [D loss: 0.696684, acc.: 0.00%] [G loss: 0.007266] [C loss: 0.696680]\n",
      "2619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007658] [C loss: 0.696680]\n",
      "2620 [D loss: 0.696682, acc.: 0.00%] [G loss: 0.009751] [C loss: 0.696680]\n",
      "2621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.696680]\n",
      "2622 [D loss: 0.696682, acc.: 0.00%] [G loss: 0.005259] [C loss: 0.696680]\n",
      "2623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007094] [C loss: 0.696680]\n",
      "2624 [D loss: 0.696680, acc.: 0.00%] [G loss: 0.006282] [C loss: 0.696679]\n",
      "2625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005992] [C loss: 0.696679]\n",
      "2626 [D loss: 0.696679, acc.: 0.00%] [G loss: 0.005582] [C loss: 0.696678]\n",
      "2627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004828] [C loss: 0.696678]\n",
      "2628 [D loss: 0.696677, acc.: 0.00%] [G loss: 0.005076] [C loss: 0.696674]\n",
      "2629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004315] [C loss: 0.696674]\n",
      "2630 [D loss: 0.696675, acc.: 0.00%] [G loss: 0.006036] [C loss: 0.696672]\n",
      "2631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006511] [C loss: 0.696672]\n",
      "2632 [D loss: 0.696674, acc.: 0.00%] [G loss: 0.005888] [C loss: 0.696671]\n",
      "2633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007495] [C loss: 0.696671]\n",
      "2634 [D loss: 0.696672, acc.: 0.00%] [G loss: 0.016420] [C loss: 0.696673]\n",
      "2635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006411] [C loss: 0.696673]\n",
      "2636 [D loss: 0.696673, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.696671]\n",
      "2637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006773] [C loss: 0.696671]\n",
      "2638 [D loss: 0.696670, acc.: 0.00%] [G loss: 0.007559] [C loss: 0.696668]\n",
      "2639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009817] [C loss: 0.696668]\n",
      "2640 [D loss: 0.696670, acc.: 0.00%] [G loss: 0.014455] [C loss: 0.696667]\n",
      "2641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005220] [C loss: 0.696667]\n",
      "2642 [D loss: 0.696669, acc.: 0.00%] [G loss: 0.013583] [C loss: 0.696666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006725] [C loss: 0.696666]\n",
      "2644 [D loss: 0.696669, acc.: 0.00%] [G loss: 0.006440] [C loss: 0.696664]\n",
      "2645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008758] [C loss: 0.696664]\n",
      "2646 [D loss: 0.696666, acc.: 0.00%] [G loss: 0.007097] [C loss: 0.696665]\n",
      "2647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005689] [C loss: 0.696665]\n",
      "2648 [D loss: 0.696666, acc.: 0.00%] [G loss: 0.006249] [C loss: 0.696663]\n",
      "2649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005151] [C loss: 0.696663]\n",
      "2650 [D loss: 0.696665, acc.: 0.00%] [G loss: 0.019957] [C loss: 0.696662]\n",
      "2651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008116] [C loss: 0.696662]\n",
      "2652 [D loss: 0.696664, acc.: 0.00%] [G loss: 0.005556] [C loss: 0.696660]\n",
      "2653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006516] [C loss: 0.696660]\n",
      "2654 [D loss: 0.696662, acc.: 0.00%] [G loss: 0.008928] [C loss: 0.696658]\n",
      "2655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006281] [C loss: 0.696658]\n",
      "2656 [D loss: 0.696660, acc.: 0.00%] [G loss: 0.006761] [C loss: 0.696660]\n",
      "2657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004293] [C loss: 0.696660]\n",
      "2658 [D loss: 0.696660, acc.: 0.00%] [G loss: 0.009450] [C loss: 0.696656]\n",
      "2659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007048] [C loss: 0.696656]\n",
      "2660 [D loss: 0.696658, acc.: 0.00%] [G loss: 0.007821] [C loss: 0.696656]\n",
      "2661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005736] [C loss: 0.696656]\n",
      "2662 [D loss: 0.696658, acc.: 0.00%] [G loss: 0.004640] [C loss: 0.696656]\n",
      "2663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010818] [C loss: 0.696656]\n",
      "2664 [D loss: 0.696657, acc.: 0.00%] [G loss: 0.005545] [C loss: 0.696652]\n",
      "2665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005864] [C loss: 0.696652]\n",
      "2666 [D loss: 0.696653, acc.: 0.00%] [G loss: 0.010559] [C loss: 0.696652]\n",
      "2667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006417] [C loss: 0.696652]\n",
      "2668 [D loss: 0.696652, acc.: 0.00%] [G loss: 0.009598] [C loss: 0.696650]\n",
      "2669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007289] [C loss: 0.696650]\n",
      "2670 [D loss: 0.696652, acc.: 0.00%] [G loss: 0.007311] [C loss: 0.696650]\n",
      "2671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021479] [C loss: 0.696650]\n",
      "2672 [D loss: 0.696651, acc.: 0.00%] [G loss: 0.006285] [C loss: 0.696649]\n",
      "2673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006363] [C loss: 0.696649]\n",
      "2674 [D loss: 0.696650, acc.: 0.00%] [G loss: 0.007583] [C loss: 0.696648]\n",
      "2675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007350] [C loss: 0.696648]\n",
      "2676 [D loss: 0.696649, acc.: 0.00%] [G loss: 0.008402] [C loss: 0.696646]\n",
      "2677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005976] [C loss: 0.696646]\n",
      "2678 [D loss: 0.696648, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.696645]\n",
      "2679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007657] [C loss: 0.696645]\n",
      "2680 [D loss: 0.696648, acc.: 0.00%] [G loss: 0.004882] [C loss: 0.696642]\n",
      "2681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007472] [C loss: 0.696642]\n",
      "2682 [D loss: 0.696645, acc.: 0.00%] [G loss: 0.007570] [C loss: 0.696642]\n",
      "2683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006186] [C loss: 0.696642]\n",
      "2684 [D loss: 0.696643, acc.: 0.00%] [G loss: 0.006261] [C loss: 0.696641]\n",
      "2685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005928] [C loss: 0.696641]\n",
      "2686 [D loss: 0.696642, acc.: 0.00%] [G loss: 0.006143] [C loss: 0.696639]\n",
      "2687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004608] [C loss: 0.696639]\n",
      "2688 [D loss: 0.696640, acc.: 0.00%] [G loss: 0.006422] [C loss: 0.696638]\n",
      "2689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007045] [C loss: 0.696638]\n",
      "2690 [D loss: 0.696640, acc.: 0.00%] [G loss: 0.005941] [C loss: 0.696637]\n",
      "2691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007215] [C loss: 0.696637]\n",
      "2692 [D loss: 0.696638, acc.: 0.00%] [G loss: 0.007068] [C loss: 0.696636]\n",
      "2693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005597] [C loss: 0.696636]\n",
      "2694 [D loss: 0.696639, acc.: 0.00%] [G loss: 0.004498] [C loss: 0.696637]\n",
      "2695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005397] [C loss: 0.696637]\n",
      "2696 [D loss: 0.696638, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.696635]\n",
      "2697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004586] [C loss: 0.696635]\n",
      "2698 [D loss: 0.696637, acc.: 0.00%] [G loss: 0.007815] [C loss: 0.696633]\n",
      "2699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007322] [C loss: 0.696633]\n",
      "2700 [D loss: 0.696635, acc.: 0.00%] [G loss: 0.005728] [C loss: 0.696632]\n",
      "2701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005904] [C loss: 0.696632]\n",
      "2702 [D loss: 0.696633, acc.: 0.00%] [G loss: 0.004364] [C loss: 0.696631]\n",
      "2703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005968] [C loss: 0.696631]\n",
      "2704 [D loss: 0.696632, acc.: 0.00%] [G loss: 0.004463] [C loss: 0.696629]\n",
      "2705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007677] [C loss: 0.696629]\n",
      "2706 [D loss: 0.696630, acc.: 0.00%] [G loss: 0.005675] [C loss: 0.696628]\n",
      "2707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006514] [C loss: 0.696628]\n",
      "2708 [D loss: 0.696630, acc.: 0.00%] [G loss: 0.006313] [C loss: 0.696629]\n",
      "2709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006122] [C loss: 0.696629]\n",
      "2710 [D loss: 0.696630, acc.: 0.00%] [G loss: 0.005978] [C loss: 0.696624]\n",
      "2711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006948] [C loss: 0.696624]\n",
      "2712 [D loss: 0.696626, acc.: 0.00%] [G loss: 0.005016] [C loss: 0.696624]\n",
      "2713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004501] [C loss: 0.696624]\n",
      "2714 [D loss: 0.696625, acc.: 0.00%] [G loss: 0.006320] [C loss: 0.696625]\n",
      "2715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006047] [C loss: 0.696625]\n",
      "2716 [D loss: 0.696625, acc.: 0.00%] [G loss: 0.005703] [C loss: 0.696622]\n",
      "2717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004318] [C loss: 0.696622]\n",
      "2718 [D loss: 0.696623, acc.: 0.00%] [G loss: 0.008565] [C loss: 0.696621]\n",
      "2719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004590] [C loss: 0.696621]\n",
      "2720 [D loss: 0.696621, acc.: 0.00%] [G loss: 0.005077] [C loss: 0.696620]\n",
      "2721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009047] [C loss: 0.696620]\n",
      "2722 [D loss: 0.696622, acc.: 0.00%] [G loss: 0.006396] [C loss: 0.696617]\n",
      "2723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007035] [C loss: 0.696617]\n",
      "2724 [D loss: 0.696618, acc.: 0.00%] [G loss: 0.005394] [C loss: 0.696615]\n",
      "2725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006534] [C loss: 0.696615]\n",
      "2726 [D loss: 0.696617, acc.: 0.00%] [G loss: 0.005336] [C loss: 0.696615]\n",
      "2727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006742] [C loss: 0.696615]\n",
      "2728 [D loss: 0.696617, acc.: 0.00%] [G loss: 0.005861] [C loss: 0.696615]\n",
      "2729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005832] [C loss: 0.696615]\n",
      "2730 [D loss: 0.696616, acc.: 0.00%] [G loss: 0.011869] [C loss: 0.696613]\n",
      "2731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006137] [C loss: 0.696613]\n",
      "2732 [D loss: 0.696615, acc.: 0.00%] [G loss: 0.009415] [C loss: 0.696612]\n",
      "2733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008866] [C loss: 0.696612]\n",
      "2734 [D loss: 0.696614, acc.: 0.00%] [G loss: 0.016707] [C loss: 0.696612]\n",
      "2735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009343] [C loss: 0.696612]\n",
      "2736 [D loss: 0.696612, acc.: 0.00%] [G loss: 0.007702] [C loss: 0.696610]\n",
      "2737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007064] [C loss: 0.696610]\n",
      "2738 [D loss: 0.696612, acc.: 0.00%] [G loss: 0.007774] [C loss: 0.696609]\n",
      "2739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006669] [C loss: 0.696609]\n",
      "2740 [D loss: 0.696610, acc.: 0.00%] [G loss: 0.005912] [C loss: 0.696606]\n",
      "2741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005140] [C loss: 0.696606]\n",
      "2742 [D loss: 0.696608, acc.: 0.00%] [G loss: 0.005095] [C loss: 0.696606]\n",
      "2743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006407] [C loss: 0.696606]\n",
      "2744 [D loss: 0.696608, acc.: 0.00%] [G loss: 0.006060] [C loss: 0.696605]\n",
      "2745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.696605]\n",
      "2746 [D loss: 0.696606, acc.: 0.00%] [G loss: 0.006059] [C loss: 0.696604]\n",
      "2747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003878] [C loss: 0.696604]\n",
      "2748 [D loss: 0.696604, acc.: 0.00%] [G loss: 0.004356] [C loss: 0.696599]\n",
      "2749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008059] [C loss: 0.696599]\n",
      "2750 [D loss: 0.696602, acc.: 0.00%] [G loss: 0.008938] [C loss: 0.696601]\n",
      "2751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006313] [C loss: 0.696601]\n",
      "2752 [D loss: 0.696603, acc.: 0.00%] [G loss: 0.008773] [C loss: 0.696599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011636] [C loss: 0.696599]\n",
      "2754 [D loss: 0.696600, acc.: 0.00%] [G loss: 0.009081] [C loss: 0.696598]\n",
      "2755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007006] [C loss: 0.696598]\n",
      "2756 [D loss: 0.696599, acc.: 0.00%] [G loss: 0.005907] [C loss: 0.696596]\n",
      "2757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006909] [C loss: 0.696596]\n",
      "2758 [D loss: 0.696597, acc.: 0.00%] [G loss: 0.007644] [C loss: 0.696595]\n",
      "2759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006024] [C loss: 0.696595]\n",
      "2760 [D loss: 0.696597, acc.: 0.00%] [G loss: 0.007206] [C loss: 0.696593]\n",
      "2761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007129] [C loss: 0.696593]\n",
      "2762 [D loss: 0.696596, acc.: 0.00%] [G loss: 0.009293] [C loss: 0.696594]\n",
      "2763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005839] [C loss: 0.696594]\n",
      "2764 [D loss: 0.696595, acc.: 0.00%] [G loss: 0.005024] [C loss: 0.696590]\n",
      "2765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006508] [C loss: 0.696590]\n",
      "2766 [D loss: 0.696593, acc.: 0.00%] [G loss: 0.007689] [C loss: 0.696591]\n",
      "2767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020078] [C loss: 0.696591]\n",
      "2768 [D loss: 0.696593, acc.: 0.00%] [G loss: 0.005567] [C loss: 0.696591]\n",
      "2769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009797] [C loss: 0.696591]\n",
      "2770 [D loss: 0.696592, acc.: 0.00%] [G loss: 0.006052] [C loss: 0.696587]\n",
      "2771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006760] [C loss: 0.696587]\n",
      "2772 [D loss: 0.696589, acc.: 0.00%] [G loss: 0.007434] [C loss: 0.696587]\n",
      "2773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004205] [C loss: 0.696587]\n",
      "2774 [D loss: 0.696589, acc.: 0.00%] [G loss: 0.008256] [C loss: 0.696587]\n",
      "2775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008616] [C loss: 0.696587]\n",
      "2776 [D loss: 0.696589, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.696585]\n",
      "2777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005507] [C loss: 0.696585]\n",
      "2778 [D loss: 0.696587, acc.: 0.00%] [G loss: 0.006765] [C loss: 0.696584]\n",
      "2779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.696584]\n",
      "2780 [D loss: 0.696586, acc.: 0.00%] [G loss: 0.007030] [C loss: 0.696581]\n",
      "2781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004933] [C loss: 0.696581]\n",
      "2782 [D loss: 0.696584, acc.: 0.00%] [G loss: 0.006438] [C loss: 0.696580]\n",
      "2783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004947] [C loss: 0.696580]\n",
      "2784 [D loss: 0.696582, acc.: 0.00%] [G loss: 0.005534] [C loss: 0.696579]\n",
      "2785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004436] [C loss: 0.696579]\n",
      "2786 [D loss: 0.696581, acc.: 0.00%] [G loss: 0.005989] [C loss: 0.696580]\n",
      "2787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.696580]\n",
      "2788 [D loss: 0.696581, acc.: 0.00%] [G loss: 0.010612] [C loss: 0.696580]\n",
      "2789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006229] [C loss: 0.696580]\n",
      "2790 [D loss: 0.696580, acc.: 0.00%] [G loss: 0.009349] [C loss: 0.696577]\n",
      "2791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005140] [C loss: 0.696577]\n",
      "2792 [D loss: 0.696578, acc.: 0.00%] [G loss: 0.006777] [C loss: 0.696577]\n",
      "2793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005052] [C loss: 0.696577]\n",
      "2794 [D loss: 0.696578, acc.: 0.00%] [G loss: 0.005295] [C loss: 0.696575]\n",
      "2795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004973] [C loss: 0.696575]\n",
      "2796 [D loss: 0.696576, acc.: 0.00%] [G loss: 0.005496] [C loss: 0.696573]\n",
      "2797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006413] [C loss: 0.696573]\n",
      "2798 [D loss: 0.696575, acc.: 0.00%] [G loss: 0.005835] [C loss: 0.696571]\n",
      "2799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006967] [C loss: 0.696571]\n",
      "2800 [D loss: 0.696573, acc.: 0.00%] [G loss: 0.003645] [C loss: 0.696571]\n",
      "2801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005769] [C loss: 0.696571]\n",
      "2802 [D loss: 0.696572, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.696568]\n",
      "2803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006351] [C loss: 0.696568]\n",
      "2804 [D loss: 0.696571, acc.: 0.00%] [G loss: 0.005952] [C loss: 0.696568]\n",
      "2805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004275] [C loss: 0.696568]\n",
      "2806 [D loss: 0.696570, acc.: 0.00%] [G loss: 0.024454] [C loss: 0.696567]\n",
      "2807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005182] [C loss: 0.696567]\n",
      "2808 [D loss: 0.696568, acc.: 0.00%] [G loss: 0.007482] [C loss: 0.696566]\n",
      "2809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006797] [C loss: 0.696566]\n",
      "2810 [D loss: 0.696567, acc.: 0.00%] [G loss: 0.011889] [C loss: 0.696564]\n",
      "2811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006402] [C loss: 0.696564]\n",
      "2812 [D loss: 0.696565, acc.: 0.00%] [G loss: 0.008349] [C loss: 0.696564]\n",
      "2813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004106] [C loss: 0.696564]\n",
      "2814 [D loss: 0.696565, acc.: 0.00%] [G loss: 0.006788] [C loss: 0.696561]\n",
      "2815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006699] [C loss: 0.696561]\n",
      "2816 [D loss: 0.696563, acc.: 0.00%] [G loss: 0.006149] [C loss: 0.696562]\n",
      "2817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005776] [C loss: 0.696562]\n",
      "2818 [D loss: 0.696563, acc.: 0.00%] [G loss: 0.004976] [C loss: 0.696558]\n",
      "2819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004669] [C loss: 0.696558]\n",
      "2820 [D loss: 0.696560, acc.: 0.00%] [G loss: 0.009333] [C loss: 0.696558]\n",
      "2821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004726] [C loss: 0.696558]\n",
      "2822 [D loss: 0.696560, acc.: 0.00%] [G loss: 0.004790] [C loss: 0.696560]\n",
      "2823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.696560]\n",
      "2824 [D loss: 0.696560, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.696556]\n",
      "2825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006520] [C loss: 0.696556]\n",
      "2826 [D loss: 0.696557, acc.: 0.00%] [G loss: 0.007572] [C loss: 0.696555]\n",
      "2827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005183] [C loss: 0.696555]\n",
      "2828 [D loss: 0.696556, acc.: 0.00%] [G loss: 0.007400] [C loss: 0.696555]\n",
      "2829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008548] [C loss: 0.696555]\n",
      "2830 [D loss: 0.696555, acc.: 0.00%] [G loss: 0.005932] [C loss: 0.696552]\n",
      "2831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018721] [C loss: 0.696552]\n",
      "2832 [D loss: 0.696554, acc.: 0.00%] [G loss: 0.004736] [C loss: 0.696552]\n",
      "2833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005682] [C loss: 0.696552]\n",
      "2834 [D loss: 0.696553, acc.: 0.00%] [G loss: 0.006383] [C loss: 0.696551]\n",
      "2835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006360] [C loss: 0.696551]\n",
      "2836 [D loss: 0.696553, acc.: 0.00%] [G loss: 0.006216] [C loss: 0.696550]\n",
      "2837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006232] [C loss: 0.696550]\n",
      "2838 [D loss: 0.696551, acc.: 0.00%] [G loss: 0.004768] [C loss: 0.696548]\n",
      "2839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005224] [C loss: 0.696548]\n",
      "2840 [D loss: 0.696549, acc.: 0.00%] [G loss: 0.004231] [C loss: 0.696549]\n",
      "2841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005326] [C loss: 0.696549]\n",
      "2842 [D loss: 0.696549, acc.: 0.00%] [G loss: 0.006588] [C loss: 0.696544]\n",
      "2843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009466] [C loss: 0.696544]\n",
      "2844 [D loss: 0.696547, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.696544]\n",
      "2845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011675] [C loss: 0.696544]\n",
      "2846 [D loss: 0.696546, acc.: 0.00%] [G loss: 0.011549] [C loss: 0.696543]\n",
      "2847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005193] [C loss: 0.696543]\n",
      "2848 [D loss: 0.696544, acc.: 0.00%] [G loss: 0.006899] [C loss: 0.696543]\n",
      "2849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016224] [C loss: 0.696543]\n",
      "2850 [D loss: 0.696544, acc.: 0.00%] [G loss: 0.012869] [C loss: 0.696540]\n",
      "2851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007290] [C loss: 0.696540]\n",
      "2852 [D loss: 0.696541, acc.: 0.00%] [G loss: 0.005812] [C loss: 0.696538]\n",
      "2853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006124] [C loss: 0.696538]\n",
      "2854 [D loss: 0.696541, acc.: 0.00%] [G loss: 0.006410] [C loss: 0.696537]\n",
      "2855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007282] [C loss: 0.696537]\n",
      "2856 [D loss: 0.696539, acc.: 0.00%] [G loss: 0.005701] [C loss: 0.696535]\n",
      "2857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013507] [C loss: 0.696535]\n",
      "2858 [D loss: 0.696538, acc.: 0.00%] [G loss: 0.004895] [C loss: 0.696536]\n",
      "2859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013167] [C loss: 0.696536]\n",
      "2860 [D loss: 0.696538, acc.: 0.00%] [G loss: 0.006220] [C loss: 0.696534]\n",
      "2861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005713] [C loss: 0.696534]\n",
      "2862 [D loss: 0.696536, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.696532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004516] [C loss: 0.696532]\n",
      "2864 [D loss: 0.696535, acc.: 0.00%] [G loss: 0.005346] [C loss: 0.696530]\n",
      "2865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006298] [C loss: 0.696530]\n",
      "2866 [D loss: 0.696531, acc.: 0.00%] [G loss: 0.005519] [C loss: 0.696532]\n",
      "2867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.027647] [C loss: 0.696532]\n",
      "2868 [D loss: 0.696533, acc.: 0.00%] [G loss: 0.005047] [C loss: 0.696528]\n",
      "2869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005971] [C loss: 0.696528]\n",
      "2870 [D loss: 0.696530, acc.: 0.00%] [G loss: 0.004214] [C loss: 0.696530]\n",
      "2871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011757] [C loss: 0.696530]\n",
      "2872 [D loss: 0.696530, acc.: 0.00%] [G loss: 0.006018] [C loss: 0.696527]\n",
      "2873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009965] [C loss: 0.696527]\n",
      "2874 [D loss: 0.696528, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.696526]\n",
      "2875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004856] [C loss: 0.696526]\n",
      "2876 [D loss: 0.696527, acc.: 0.00%] [G loss: 0.005488] [C loss: 0.696526]\n",
      "2877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.696526]\n",
      "2878 [D loss: 0.696527, acc.: 0.00%] [G loss: 0.006531] [C loss: 0.696523]\n",
      "2879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008211] [C loss: 0.696523]\n",
      "2880 [D loss: 0.696525, acc.: 0.00%] [G loss: 0.004798] [C loss: 0.696523]\n",
      "2881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005230] [C loss: 0.696523]\n",
      "2882 [D loss: 0.696525, acc.: 0.00%] [G loss: 0.006830] [C loss: 0.696520]\n",
      "2883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015057] [C loss: 0.696520]\n",
      "2884 [D loss: 0.696522, acc.: 0.00%] [G loss: 0.003949] [C loss: 0.696521]\n",
      "2885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006150] [C loss: 0.696521]\n",
      "2886 [D loss: 0.696522, acc.: 0.00%] [G loss: 0.008181] [C loss: 0.696519]\n",
      "2887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008181] [C loss: 0.696519]\n",
      "2888 [D loss: 0.696521, acc.: 0.00%] [G loss: 0.021973] [C loss: 0.696518]\n",
      "2889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005443] [C loss: 0.696518]\n",
      "2890 [D loss: 0.696520, acc.: 0.00%] [G loss: 0.004781] [C loss: 0.696517]\n",
      "2891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006673] [C loss: 0.696517]\n",
      "2892 [D loss: 0.696518, acc.: 0.00%] [G loss: 0.005362] [C loss: 0.696516]\n",
      "2893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009676] [C loss: 0.696516]\n",
      "2894 [D loss: 0.696517, acc.: 0.00%] [G loss: 0.005665] [C loss: 0.696517]\n",
      "2895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005854] [C loss: 0.696517]\n",
      "2896 [D loss: 0.696517, acc.: 0.00%] [G loss: 0.006564] [C loss: 0.696512]\n",
      "2897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004154] [C loss: 0.696512]\n",
      "2898 [D loss: 0.696514, acc.: 0.00%] [G loss: 0.005344] [C loss: 0.696512]\n",
      "2899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004564] [C loss: 0.696512]\n",
      "2900 [D loss: 0.696513, acc.: 0.00%] [G loss: 0.009392] [C loss: 0.696510]\n",
      "2901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006023] [C loss: 0.696510]\n",
      "2902 [D loss: 0.696512, acc.: 0.00%] [G loss: 0.007160] [C loss: 0.696510]\n",
      "2903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008513] [C loss: 0.696510]\n",
      "2904 [D loss: 0.696512, acc.: 0.00%] [G loss: 0.006307] [C loss: 0.696507]\n",
      "2905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007683] [C loss: 0.696507]\n",
      "2906 [D loss: 0.696508, acc.: 0.00%] [G loss: 0.005636] [C loss: 0.696506]\n",
      "2907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007043] [C loss: 0.696506]\n",
      "2908 [D loss: 0.696508, acc.: 0.00%] [G loss: 0.006289] [C loss: 0.696504]\n",
      "2909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005927] [C loss: 0.696504]\n",
      "2910 [D loss: 0.696505, acc.: 0.00%] [G loss: 0.004602] [C loss: 0.696503]\n",
      "2911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.696503]\n",
      "2912 [D loss: 0.696506, acc.: 0.00%] [G loss: 0.006910] [C loss: 0.696502]\n",
      "2913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004949] [C loss: 0.696502]\n",
      "2914 [D loss: 0.696504, acc.: 0.00%] [G loss: 0.005443] [C loss: 0.696501]\n",
      "2915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006013] [C loss: 0.696501]\n",
      "2916 [D loss: 0.696502, acc.: 0.00%] [G loss: 0.007018] [C loss: 0.696498]\n",
      "2917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006876] [C loss: 0.696498]\n",
      "2918 [D loss: 0.696501, acc.: 0.00%] [G loss: 0.005237] [C loss: 0.696500]\n",
      "2919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.696500]\n",
      "2920 [D loss: 0.696500, acc.: 0.00%] [G loss: 0.005321] [C loss: 0.696498]\n",
      "2921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007713] [C loss: 0.696498]\n",
      "2922 [D loss: 0.696500, acc.: 0.00%] [G loss: 0.004451] [C loss: 0.696496]\n",
      "2923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005774] [C loss: 0.696496]\n",
      "2924 [D loss: 0.696499, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.696496]\n",
      "2925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007020] [C loss: 0.696496]\n",
      "2926 [D loss: 0.696498, acc.: 0.00%] [G loss: 0.004229] [C loss: 0.696495]\n",
      "2927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007526] [C loss: 0.696495]\n",
      "2928 [D loss: 0.696497, acc.: 0.00%] [G loss: 0.005651] [C loss: 0.696495]\n",
      "2929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007322] [C loss: 0.696495]\n",
      "2930 [D loss: 0.696496, acc.: 0.00%] [G loss: 0.006244] [C loss: 0.696494]\n",
      "2931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008421] [C loss: 0.696494]\n",
      "2932 [D loss: 0.696494, acc.: 0.00%] [G loss: 0.004688] [C loss: 0.696492]\n",
      "2933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.696492]\n",
      "2934 [D loss: 0.696493, acc.: 0.00%] [G loss: 0.004357] [C loss: 0.696487]\n",
      "2935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004304] [C loss: 0.696487]\n",
      "2936 [D loss: 0.696490, acc.: 0.00%] [G loss: 0.005672] [C loss: 0.696487]\n",
      "2937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006035] [C loss: 0.696487]\n",
      "2938 [D loss: 0.696489, acc.: 0.00%] [G loss: 0.006220] [C loss: 0.696487]\n",
      "2939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007363] [C loss: 0.696487]\n",
      "2940 [D loss: 0.696488, acc.: 0.00%] [G loss: 0.007041] [C loss: 0.696487]\n",
      "2941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005447] [C loss: 0.696487]\n",
      "2942 [D loss: 0.696489, acc.: 0.00%] [G loss: 0.011206] [C loss: 0.696485]\n",
      "2943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005200] [C loss: 0.696485]\n",
      "2944 [D loss: 0.696487, acc.: 0.00%] [G loss: 0.006017] [C loss: 0.696485]\n",
      "2945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009076] [C loss: 0.696485]\n",
      "2946 [D loss: 0.696486, acc.: 0.00%] [G loss: 0.007153] [C loss: 0.696481]\n",
      "2947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004824] [C loss: 0.696481]\n",
      "2948 [D loss: 0.696483, acc.: 0.00%] [G loss: 0.005904] [C loss: 0.696480]\n",
      "2949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005321] [C loss: 0.696480]\n",
      "2950 [D loss: 0.696481, acc.: 0.00%] [G loss: 0.004825] [C loss: 0.696480]\n",
      "2951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005000] [C loss: 0.696480]\n",
      "2952 [D loss: 0.696482, acc.: 0.00%] [G loss: 0.004156] [C loss: 0.696479]\n",
      "2953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007264] [C loss: 0.696479]\n",
      "2954 [D loss: 0.696481, acc.: 0.00%] [G loss: 0.005870] [C loss: 0.696478]\n",
      "2955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006838] [C loss: 0.696478]\n",
      "2956 [D loss: 0.696479, acc.: 0.00%] [G loss: 0.007168] [C loss: 0.696477]\n",
      "2957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004714] [C loss: 0.696477]\n",
      "2958 [D loss: 0.696478, acc.: 0.00%] [G loss: 0.004504] [C loss: 0.696476]\n",
      "2959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007195] [C loss: 0.696476]\n",
      "2960 [D loss: 0.696477, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.696471]\n",
      "2961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005294] [C loss: 0.696471]\n",
      "2962 [D loss: 0.696474, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.696472]\n",
      "2963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005611] [C loss: 0.696472]\n",
      "2964 [D loss: 0.696475, acc.: 0.00%] [G loss: 0.006442] [C loss: 0.696470]\n",
      "2965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007960] [C loss: 0.696470]\n",
      "2966 [D loss: 0.696473, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.696470]\n",
      "2967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005562] [C loss: 0.696470]\n",
      "2968 [D loss: 0.696471, acc.: 0.00%] [G loss: 0.006298] [C loss: 0.696470]\n",
      "2969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007365] [C loss: 0.696470]\n",
      "2970 [D loss: 0.696471, acc.: 0.00%] [G loss: 0.005663] [C loss: 0.696468]\n",
      "2971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010070] [C loss: 0.696468]\n",
      "2972 [D loss: 0.696469, acc.: 0.00%] [G loss: 0.005963] [C loss: 0.696465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005711] [C loss: 0.696465]\n",
      "2974 [D loss: 0.696467, acc.: 0.00%] [G loss: 0.004160] [C loss: 0.696466]\n",
      "2975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008708] [C loss: 0.696466]\n",
      "2976 [D loss: 0.696468, acc.: 0.00%] [G loss: 0.006600] [C loss: 0.696464]\n",
      "2977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005054] [C loss: 0.696464]\n",
      "2978 [D loss: 0.696466, acc.: 0.00%] [G loss: 0.006114] [C loss: 0.696463]\n",
      "2979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004576] [C loss: 0.696463]\n",
      "2980 [D loss: 0.696465, acc.: 0.00%] [G loss: 0.005436] [C loss: 0.696463]\n",
      "2981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008418] [C loss: 0.696463]\n",
      "2982 [D loss: 0.696463, acc.: 0.00%] [G loss: 0.007170] [C loss: 0.696460]\n",
      "2983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007842] [C loss: 0.696460]\n",
      "2984 [D loss: 0.696463, acc.: 0.00%] [G loss: 0.007436] [C loss: 0.696460]\n",
      "2985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004326] [C loss: 0.696460]\n",
      "2986 [D loss: 0.696460, acc.: 0.00%] [G loss: 0.005243] [C loss: 0.696458]\n",
      "2987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004633] [C loss: 0.696458]\n",
      "2988 [D loss: 0.696459, acc.: 0.00%] [G loss: 0.006339] [C loss: 0.696456]\n",
      "2989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008008] [C loss: 0.696456]\n",
      "2990 [D loss: 0.696458, acc.: 0.00%] [G loss: 0.004850] [C loss: 0.696456]\n",
      "2991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005276] [C loss: 0.696456]\n",
      "2992 [D loss: 0.696457, acc.: 0.00%] [G loss: 0.006684] [C loss: 0.696455]\n",
      "2993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005219] [C loss: 0.696455]\n",
      "2994 [D loss: 0.696456, acc.: 0.00%] [G loss: 0.005682] [C loss: 0.696451]\n",
      "2995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007317] [C loss: 0.696451]\n",
      "2996 [D loss: 0.696454, acc.: 0.00%] [G loss: 0.004524] [C loss: 0.696450]\n",
      "2997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006306] [C loss: 0.696450]\n",
      "2998 [D loss: 0.696453, acc.: 0.00%] [G loss: 0.004463] [C loss: 0.696450]\n",
      "2999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005380] [C loss: 0.696450]\n",
      "3000 [D loss: 0.696453, acc.: 0.00%] [G loss: 0.003840] [C loss: 0.696451]\n",
      "3001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004279] [C loss: 0.696451]\n",
      "3002 [D loss: 0.696453, acc.: 0.00%] [G loss: 0.007021] [C loss: 0.696450]\n",
      "3003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005895] [C loss: 0.696450]\n",
      "3004 [D loss: 0.696450, acc.: 0.00%] [G loss: 0.008596] [C loss: 0.696447]\n",
      "3005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005297] [C loss: 0.696447]\n",
      "3006 [D loss: 0.696448, acc.: 0.00%] [G loss: 0.006906] [C loss: 0.696447]\n",
      "3007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004462] [C loss: 0.696447]\n",
      "3008 [D loss: 0.696449, acc.: 0.00%] [G loss: 0.010777] [C loss: 0.696443]\n",
      "3009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004933] [C loss: 0.696443]\n",
      "3010 [D loss: 0.696445, acc.: 0.00%] [G loss: 0.009509] [C loss: 0.696444]\n",
      "3011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004908] [C loss: 0.696444]\n",
      "3012 [D loss: 0.696445, acc.: 0.00%] [G loss: 0.004715] [C loss: 0.696443]\n",
      "3013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.696443]\n",
      "3014 [D loss: 0.696443, acc.: 0.00%] [G loss: 0.006111] [C loss: 0.696443]\n",
      "3015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014065] [C loss: 0.696443]\n",
      "3016 [D loss: 0.696444, acc.: 0.00%] [G loss: 0.007502] [C loss: 0.696439]\n",
      "3017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019080] [C loss: 0.696439]\n",
      "3018 [D loss: 0.696441, acc.: 0.00%] [G loss: 0.004581] [C loss: 0.696437]\n",
      "3019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005647] [C loss: 0.696437]\n",
      "3020 [D loss: 0.696439, acc.: 0.00%] [G loss: 0.017527] [C loss: 0.696436]\n",
      "3021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005667] [C loss: 0.696436]\n",
      "3022 [D loss: 0.696439, acc.: 0.00%] [G loss: 0.020509] [C loss: 0.696435]\n",
      "3023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008361] [C loss: 0.696435]\n",
      "3024 [D loss: 0.696438, acc.: 0.00%] [G loss: 0.007238] [C loss: 0.696435]\n",
      "3025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007705] [C loss: 0.696435]\n",
      "3026 [D loss: 0.696436, acc.: 0.00%] [G loss: 0.004521] [C loss: 0.696434]\n",
      "3027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004880] [C loss: 0.696434]\n",
      "3028 [D loss: 0.696434, acc.: 0.00%] [G loss: 0.005428] [C loss: 0.696433]\n",
      "3029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006956] [C loss: 0.696433]\n",
      "3030 [D loss: 0.696435, acc.: 0.00%] [G loss: 0.004693] [C loss: 0.696431]\n",
      "3031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008334] [C loss: 0.696431]\n",
      "3032 [D loss: 0.696433, acc.: 0.00%] [G loss: 0.004986] [C loss: 0.696431]\n",
      "3033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004177] [C loss: 0.696431]\n",
      "3034 [D loss: 0.696432, acc.: 0.00%] [G loss: 0.009267] [C loss: 0.696431]\n",
      "3035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006523] [C loss: 0.696431]\n",
      "3036 [D loss: 0.696431, acc.: 0.00%] [G loss: 0.005541] [C loss: 0.696425]\n",
      "3037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006883] [C loss: 0.696425]\n",
      "3038 [D loss: 0.696429, acc.: 0.00%] [G loss: 0.005720] [C loss: 0.696428]\n",
      "3039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004887] [C loss: 0.696428]\n",
      "3040 [D loss: 0.696429, acc.: 0.00%] [G loss: 0.007255] [C loss: 0.696426]\n",
      "3041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004844] [C loss: 0.696426]\n",
      "3042 [D loss: 0.696427, acc.: 0.00%] [G loss: 0.007529] [C loss: 0.696424]\n",
      "3043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008000] [C loss: 0.696424]\n",
      "3044 [D loss: 0.696426, acc.: 0.00%] [G loss: 0.006516] [C loss: 0.696423]\n",
      "3045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008920] [C loss: 0.696423]\n",
      "3046 [D loss: 0.696424, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.696421]\n",
      "3047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003917] [C loss: 0.696421]\n",
      "3048 [D loss: 0.696423, acc.: 0.00%] [G loss: 0.005684] [C loss: 0.696422]\n",
      "3049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007251] [C loss: 0.696422]\n",
      "3050 [D loss: 0.696422, acc.: 0.00%] [G loss: 0.004004] [C loss: 0.696417]\n",
      "3051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006194] [C loss: 0.696417]\n",
      "3052 [D loss: 0.696419, acc.: 0.00%] [G loss: 0.005573] [C loss: 0.696416]\n",
      "3053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005713] [C loss: 0.696416]\n",
      "3054 [D loss: 0.696419, acc.: 0.00%] [G loss: 0.008504] [C loss: 0.696417]\n",
      "3055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006365] [C loss: 0.696417]\n",
      "3056 [D loss: 0.696419, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.696414]\n",
      "3057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006896] [C loss: 0.696414]\n",
      "3058 [D loss: 0.696417, acc.: 0.00%] [G loss: 0.006621] [C loss: 0.696414]\n",
      "3059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007397] [C loss: 0.696414]\n",
      "3060 [D loss: 0.696417, acc.: 0.00%] [G loss: 0.006434] [C loss: 0.696412]\n",
      "3061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010481] [C loss: 0.696412]\n",
      "3062 [D loss: 0.696415, acc.: 0.00%] [G loss: 0.005533] [C loss: 0.696411]\n",
      "3063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006375] [C loss: 0.696411]\n",
      "3064 [D loss: 0.696414, acc.: 0.00%] [G loss: 0.006428] [C loss: 0.696410]\n",
      "3065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005353] [C loss: 0.696410]\n",
      "3066 [D loss: 0.696412, acc.: 0.00%] [G loss: 0.005629] [C loss: 0.696409]\n",
      "3067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007673] [C loss: 0.696409]\n",
      "3068 [D loss: 0.696412, acc.: 0.00%] [G loss: 0.005538] [C loss: 0.696408]\n",
      "3069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009177] [C loss: 0.696408]\n",
      "3070 [D loss: 0.696410, acc.: 0.00%] [G loss: 0.003311] [C loss: 0.696408]\n",
      "3071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007389] [C loss: 0.696408]\n",
      "3072 [D loss: 0.696409, acc.: 0.00%] [G loss: 0.006791] [C loss: 0.696405]\n",
      "3073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004622] [C loss: 0.696405]\n",
      "3074 [D loss: 0.696407, acc.: 0.00%] [G loss: 0.005565] [C loss: 0.696406]\n",
      "3075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006808] [C loss: 0.696406]\n",
      "3076 [D loss: 0.696406, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.696404]\n",
      "3077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006650] [C loss: 0.696404]\n",
      "3078 [D loss: 0.696405, acc.: 0.00%] [G loss: 0.006411] [C loss: 0.696402]\n",
      "3079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006105] [C loss: 0.696402]\n",
      "3080 [D loss: 0.696405, acc.: 0.00%] [G loss: 0.017610] [C loss: 0.696398]\n",
      "3081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005473] [C loss: 0.696398]\n",
      "3082 [D loss: 0.696400, acc.: 0.00%] [G loss: 0.006426] [C loss: 0.696398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008381] [C loss: 0.696398]\n",
      "3084 [D loss: 0.696401, acc.: 0.00%] [G loss: 0.012038] [C loss: 0.696397]\n",
      "3085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.696397]\n",
      "3086 [D loss: 0.696400, acc.: 0.00%] [G loss: 0.005457] [C loss: 0.696397]\n",
      "3087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004945] [C loss: 0.696397]\n",
      "3088 [D loss: 0.696400, acc.: 0.00%] [G loss: 0.005349] [C loss: 0.696394]\n",
      "3089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007298] [C loss: 0.696394]\n",
      "3090 [D loss: 0.696397, acc.: 0.00%] [G loss: 0.005499] [C loss: 0.696395]\n",
      "3091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004395] [C loss: 0.696395]\n",
      "3092 [D loss: 0.696397, acc.: 0.00%] [G loss: 0.004409] [C loss: 0.696394]\n",
      "3093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005267] [C loss: 0.696394]\n",
      "3094 [D loss: 0.696396, acc.: 0.00%] [G loss: 0.006740] [C loss: 0.696391]\n",
      "3095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005375] [C loss: 0.696391]\n",
      "3096 [D loss: 0.696394, acc.: 0.00%] [G loss: 0.005660] [C loss: 0.696391]\n",
      "3097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009774] [C loss: 0.696391]\n",
      "3098 [D loss: 0.696393, acc.: 0.00%] [G loss: 0.005469] [C loss: 0.696389]\n",
      "3099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005578] [C loss: 0.696389]\n",
      "3100 [D loss: 0.696391, acc.: 0.00%] [G loss: 0.008680] [C loss: 0.696390]\n",
      "3101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006501] [C loss: 0.696390]\n",
      "3102 [D loss: 0.696391, acc.: 0.00%] [G loss: 0.005879] [C loss: 0.696387]\n",
      "3103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004459] [C loss: 0.696387]\n",
      "3104 [D loss: 0.696390, acc.: 0.00%] [G loss: 0.005095] [C loss: 0.696386]\n",
      "3105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006385] [C loss: 0.696386]\n",
      "3106 [D loss: 0.696388, acc.: 0.00%] [G loss: 0.006146] [C loss: 0.696384]\n",
      "3107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005664] [C loss: 0.696384]\n",
      "3108 [D loss: 0.696386, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.696386]\n",
      "3109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005944] [C loss: 0.696386]\n",
      "3110 [D loss: 0.696386, acc.: 0.00%] [G loss: 0.005876] [C loss: 0.696382]\n",
      "3111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005479] [C loss: 0.696382]\n",
      "3112 [D loss: 0.696384, acc.: 0.00%] [G loss: 0.007040] [C loss: 0.696383]\n",
      "3113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004443] [C loss: 0.696383]\n",
      "3114 [D loss: 0.696384, acc.: 0.00%] [G loss: 0.008227] [C loss: 0.696380]\n",
      "3115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008885] [C loss: 0.696380]\n",
      "3116 [D loss: 0.696381, acc.: 0.00%] [G loss: 0.005075] [C loss: 0.696380]\n",
      "3117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006186] [C loss: 0.696380]\n",
      "3118 [D loss: 0.696382, acc.: 0.00%] [G loss: 0.007075] [C loss: 0.696379]\n",
      "3119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007504] [C loss: 0.696379]\n",
      "3120 [D loss: 0.696379, acc.: 0.00%] [G loss: 0.005184] [C loss: 0.696377]\n",
      "3121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007687] [C loss: 0.696377]\n",
      "3122 [D loss: 0.696379, acc.: 0.00%] [G loss: 0.004786] [C loss: 0.696373]\n",
      "3123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005855] [C loss: 0.696373]\n",
      "3124 [D loss: 0.696377, acc.: 0.00%] [G loss: 0.004928] [C loss: 0.696373]\n",
      "3125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004070] [C loss: 0.696373]\n",
      "3126 [D loss: 0.696375, acc.: 0.00%] [G loss: 0.005005] [C loss: 0.696373]\n",
      "3127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005822] [C loss: 0.696373]\n",
      "3128 [D loss: 0.696376, acc.: 0.00%] [G loss: 0.005380] [C loss: 0.696370]\n",
      "3129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003739] [C loss: 0.696370]\n",
      "3130 [D loss: 0.696372, acc.: 0.00%] [G loss: 0.007458] [C loss: 0.696372]\n",
      "3131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006288] [C loss: 0.696372]\n",
      "3132 [D loss: 0.696374, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.696368]\n",
      "3133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006042] [C loss: 0.696368]\n",
      "3134 [D loss: 0.696370, acc.: 0.00%] [G loss: 0.005686] [C loss: 0.696368]\n",
      "3135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004065] [C loss: 0.696368]\n",
      "3136 [D loss: 0.696370, acc.: 0.00%] [G loss: 0.004816] [C loss: 0.696368]\n",
      "3137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006032] [C loss: 0.696368]\n",
      "3138 [D loss: 0.696368, acc.: 0.00%] [G loss: 0.005669] [C loss: 0.696365]\n",
      "3139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004658] [C loss: 0.696365]\n",
      "3140 [D loss: 0.696367, acc.: 0.00%] [G loss: 0.006482] [C loss: 0.696365]\n",
      "3141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005583] [C loss: 0.696365]\n",
      "3142 [D loss: 0.696366, acc.: 0.00%] [G loss: 0.004413] [C loss: 0.696363]\n",
      "3143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005353] [C loss: 0.696363]\n",
      "3144 [D loss: 0.696365, acc.: 0.00%] [G loss: 0.023761] [C loss: 0.696362]\n",
      "3145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004438] [C loss: 0.696362]\n",
      "3146 [D loss: 0.696364, acc.: 0.00%] [G loss: 0.006598] [C loss: 0.696361]\n",
      "3147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016228] [C loss: 0.696361]\n",
      "3148 [D loss: 0.696363, acc.: 0.00%] [G loss: 0.007408] [C loss: 0.696359]\n",
      "3149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015072] [C loss: 0.696359]\n",
      "3150 [D loss: 0.696360, acc.: 0.00%] [G loss: 0.007797] [C loss: 0.696358]\n",
      "3151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015426] [C loss: 0.696358]\n",
      "3152 [D loss: 0.696360, acc.: 0.00%] [G loss: 0.006891] [C loss: 0.696356]\n",
      "3153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007607] [C loss: 0.696356]\n",
      "3154 [D loss: 0.696358, acc.: 0.00%] [G loss: 0.006934] [C loss: 0.696354]\n",
      "3155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008748] [C loss: 0.696354]\n",
      "3156 [D loss: 0.696357, acc.: 0.00%] [G loss: 0.008791] [C loss: 0.696355]\n",
      "3157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006926] [C loss: 0.696355]\n",
      "3158 [D loss: 0.696357, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.696351]\n",
      "3159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007425] [C loss: 0.696351]\n",
      "3160 [D loss: 0.696354, acc.: 0.00%] [G loss: 0.007453] [C loss: 0.696353]\n",
      "3161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012545] [C loss: 0.696353]\n",
      "3162 [D loss: 0.696354, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.696349]\n",
      "3163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005573] [C loss: 0.696349]\n",
      "3164 [D loss: 0.696351, acc.: 0.00%] [G loss: 0.005960] [C loss: 0.696351]\n",
      "3165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006726] [C loss: 0.696351]\n",
      "3166 [D loss: 0.696352, acc.: 0.00%] [G loss: 0.004975] [C loss: 0.696349]\n",
      "3167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004718] [C loss: 0.696349]\n",
      "3168 [D loss: 0.696350, acc.: 0.00%] [G loss: 0.005298] [C loss: 0.696345]\n",
      "3169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006771] [C loss: 0.696345]\n",
      "3170 [D loss: 0.696348, acc.: 0.00%] [G loss: 0.008456] [C loss: 0.696345]\n",
      "3171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007453] [C loss: 0.696345]\n",
      "3172 [D loss: 0.696347, acc.: 0.00%] [G loss: 0.006349] [C loss: 0.696344]\n",
      "3173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006699] [C loss: 0.696344]\n",
      "3174 [D loss: 0.696347, acc.: 0.00%] [G loss: 0.008228] [C loss: 0.696343]\n",
      "3175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005157] [C loss: 0.696343]\n",
      "3176 [D loss: 0.696345, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.696342]\n",
      "3177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004604] [C loss: 0.696342]\n",
      "3178 [D loss: 0.696344, acc.: 0.00%] [G loss: 0.006491] [C loss: 0.696339]\n",
      "3179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003963] [C loss: 0.696339]\n",
      "3180 [D loss: 0.696341, acc.: 0.00%] [G loss: 0.004816] [C loss: 0.696339]\n",
      "3181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007311] [C loss: 0.696339]\n",
      "3182 [D loss: 0.696342, acc.: 0.00%] [G loss: 0.006614] [C loss: 0.696338]\n",
      "3183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005031] [C loss: 0.696338]\n",
      "3184 [D loss: 0.696340, acc.: 0.00%] [G loss: 0.005467] [C loss: 0.696335]\n",
      "3185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005713] [C loss: 0.696335]\n",
      "3186 [D loss: 0.696338, acc.: 0.00%] [G loss: 0.005999] [C loss: 0.696334]\n",
      "3187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008814] [C loss: 0.696334]\n",
      "3188 [D loss: 0.696337, acc.: 0.00%] [G loss: 0.005917] [C loss: 0.696334]\n",
      "3189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.696334]\n",
      "3190 [D loss: 0.696336, acc.: 0.00%] [G loss: 0.006005] [C loss: 0.696335]\n",
      "3191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004299] [C loss: 0.696335]\n",
      "3192 [D loss: 0.696336, acc.: 0.00%] [G loss: 0.005417] [C loss: 0.696332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004369] [C loss: 0.696332]\n",
      "3194 [D loss: 0.696334, acc.: 0.00%] [G loss: 0.014254] [C loss: 0.696332]\n",
      "3195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010745] [C loss: 0.696332]\n",
      "3196 [D loss: 0.696334, acc.: 0.00%] [G loss: 0.007390] [C loss: 0.696329]\n",
      "3197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006975] [C loss: 0.696329]\n",
      "3198 [D loss: 0.696331, acc.: 0.00%] [G loss: 0.004890] [C loss: 0.696329]\n",
      "3199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005084] [C loss: 0.696329]\n",
      "3200 [D loss: 0.696331, acc.: 0.00%] [G loss: 0.005834] [C loss: 0.696328]\n",
      "3201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004951] [C loss: 0.696328]\n",
      "3202 [D loss: 0.696330, acc.: 0.00%] [G loss: 0.008776] [C loss: 0.696326]\n",
      "3203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005129] [C loss: 0.696326]\n",
      "3204 [D loss: 0.696328, acc.: 0.00%] [G loss: 0.006699] [C loss: 0.696325]\n",
      "3205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005763] [C loss: 0.696325]\n",
      "3206 [D loss: 0.696326, acc.: 0.00%] [G loss: 0.009957] [C loss: 0.696325]\n",
      "3207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004616] [C loss: 0.696325]\n",
      "3208 [D loss: 0.696326, acc.: 0.00%] [G loss: 0.007373] [C loss: 0.696323]\n",
      "3209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009883] [C loss: 0.696323]\n",
      "3210 [D loss: 0.696325, acc.: 0.00%] [G loss: 0.005077] [C loss: 0.696323]\n",
      "3211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006234] [C loss: 0.696323]\n",
      "3212 [D loss: 0.696324, acc.: 0.00%] [G loss: 0.006416] [C loss: 0.696321]\n",
      "3213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005537] [C loss: 0.696321]\n",
      "3214 [D loss: 0.696322, acc.: 0.00%] [G loss: 0.006609] [C loss: 0.696317]\n",
      "3215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005939] [C loss: 0.696317]\n",
      "3216 [D loss: 0.696319, acc.: 0.00%] [G loss: 0.004520] [C loss: 0.696318]\n",
      "3217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003632] [C loss: 0.696318]\n",
      "3218 [D loss: 0.696319, acc.: 0.00%] [G loss: 0.007308] [C loss: 0.696317]\n",
      "3219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006762] [C loss: 0.696317]\n",
      "3220 [D loss: 0.696319, acc.: 0.00%] [G loss: 0.004706] [C loss: 0.696316]\n",
      "3221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008073] [C loss: 0.696316]\n",
      "3222 [D loss: 0.696318, acc.: 0.00%] [G loss: 0.004964] [C loss: 0.696314]\n",
      "3223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004556] [C loss: 0.696314]\n",
      "3224 [D loss: 0.696316, acc.: 0.00%] [G loss: 0.005998] [C loss: 0.696312]\n",
      "3225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003841] [C loss: 0.696312]\n",
      "3226 [D loss: 0.696314, acc.: 0.00%] [G loss: 0.005866] [C loss: 0.696311]\n",
      "3227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007010] [C loss: 0.696311]\n",
      "3228 [D loss: 0.696314, acc.: 0.00%] [G loss: 0.003856] [C loss: 0.696310]\n",
      "3229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005038] [C loss: 0.696310]\n",
      "3230 [D loss: 0.696311, acc.: 0.00%] [G loss: 0.003844] [C loss: 0.696308]\n",
      "3231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005522] [C loss: 0.696308]\n",
      "3232 [D loss: 0.696311, acc.: 0.00%] [G loss: 0.003852] [C loss: 0.696305]\n",
      "3233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006452] [C loss: 0.696305]\n",
      "3234 [D loss: 0.696309, acc.: 0.00%] [G loss: 0.006347] [C loss: 0.696305]\n",
      "3235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006886] [C loss: 0.696305]\n",
      "3236 [D loss: 0.696308, acc.: 0.00%] [G loss: 0.007338] [C loss: 0.696305]\n",
      "3237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006647] [C loss: 0.696305]\n",
      "3238 [D loss: 0.696308, acc.: 0.00%] [G loss: 0.008383] [C loss: 0.696305]\n",
      "3239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021608] [C loss: 0.696305]\n",
      "3240 [D loss: 0.696307, acc.: 0.00%] [G loss: 0.007261] [C loss: 0.696304]\n",
      "3241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010092] [C loss: 0.696304]\n",
      "3242 [D loss: 0.696306, acc.: 0.00%] [G loss: 0.008979] [C loss: 0.696302]\n",
      "3243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006341] [C loss: 0.696302]\n",
      "3244 [D loss: 0.696304, acc.: 0.00%] [G loss: 0.006799] [C loss: 0.696300]\n",
      "3245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018929] [C loss: 0.696300]\n",
      "3246 [D loss: 0.696302, acc.: 0.00%] [G loss: 0.006906] [C loss: 0.696300]\n",
      "3247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006587] [C loss: 0.696300]\n",
      "3248 [D loss: 0.696300, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.696298]\n",
      "3249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005863] [C loss: 0.696298]\n",
      "3250 [D loss: 0.696300, acc.: 0.00%] [G loss: 0.005283] [C loss: 0.696298]\n",
      "3251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006126] [C loss: 0.696298]\n",
      "3252 [D loss: 0.696299, acc.: 0.00%] [G loss: 0.005918] [C loss: 0.696296]\n",
      "3253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005236] [C loss: 0.696296]\n",
      "3254 [D loss: 0.696298, acc.: 0.00%] [G loss: 0.006603] [C loss: 0.696293]\n",
      "3255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004994] [C loss: 0.696293]\n",
      "3256 [D loss: 0.696295, acc.: 0.00%] [G loss: 0.006662] [C loss: 0.696291]\n",
      "3257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005203] [C loss: 0.696291]\n",
      "3258 [D loss: 0.696294, acc.: 0.00%] [G loss: 0.005627] [C loss: 0.696291]\n",
      "3259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007059] [C loss: 0.696291]\n",
      "3260 [D loss: 0.696293, acc.: 0.00%] [G loss: 0.006232] [C loss: 0.696288]\n",
      "3261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009576] [C loss: 0.696288]\n",
      "3262 [D loss: 0.696290, acc.: 0.00%] [G loss: 0.007598] [C loss: 0.696289]\n",
      "3263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010596] [C loss: 0.696289]\n",
      "3264 [D loss: 0.696290, acc.: 0.00%] [G loss: 0.006306] [C loss: 0.696287]\n",
      "3265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004917] [C loss: 0.696287]\n",
      "3266 [D loss: 0.696290, acc.: 0.00%] [G loss: 0.008518] [C loss: 0.696285]\n",
      "3267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005361] [C loss: 0.696285]\n",
      "3268 [D loss: 0.696288, acc.: 0.00%] [G loss: 0.006224] [C loss: 0.696284]\n",
      "3269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.696284]\n",
      "3270 [D loss: 0.696287, acc.: 0.00%] [G loss: 0.004793] [C loss: 0.696282]\n",
      "3271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007664] [C loss: 0.696282]\n",
      "3272 [D loss: 0.696286, acc.: 0.00%] [G loss: 0.004867] [C loss: 0.696282]\n",
      "3273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007322] [C loss: 0.696282]\n",
      "3274 [D loss: 0.696285, acc.: 0.00%] [G loss: 0.005784] [C loss: 0.696281]\n",
      "3275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006598] [C loss: 0.696281]\n",
      "3276 [D loss: 0.696282, acc.: 0.00%] [G loss: 0.005814] [C loss: 0.696280]\n",
      "3277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004582] [C loss: 0.696280]\n",
      "3278 [D loss: 0.696283, acc.: 0.00%] [G loss: 0.006029] [C loss: 0.696279]\n",
      "3279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006325] [C loss: 0.696279]\n",
      "3280 [D loss: 0.696281, acc.: 0.00%] [G loss: 0.006318] [C loss: 0.696278]\n",
      "3281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012956] [C loss: 0.696278]\n",
      "3282 [D loss: 0.696280, acc.: 0.00%] [G loss: 0.004660] [C loss: 0.696276]\n",
      "3283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005908] [C loss: 0.696276]\n",
      "3284 [D loss: 0.696279, acc.: 0.00%] [G loss: 0.006431] [C loss: 0.696276]\n",
      "3285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004646] [C loss: 0.696276]\n",
      "3286 [D loss: 0.696277, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.696275]\n",
      "3287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007978] [C loss: 0.696275]\n",
      "3288 [D loss: 0.696277, acc.: 0.00%] [G loss: 0.006616] [C loss: 0.696271]\n",
      "3289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005243] [C loss: 0.696271]\n",
      "3290 [D loss: 0.696274, acc.: 0.00%] [G loss: 0.006112] [C loss: 0.696272]\n",
      "3291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003834] [C loss: 0.696272]\n",
      "3292 [D loss: 0.696272, acc.: 0.00%] [G loss: 0.004784] [C loss: 0.696273]\n",
      "3293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005883] [C loss: 0.696273]\n",
      "3294 [D loss: 0.696275, acc.: 0.00%] [G loss: 0.006989] [C loss: 0.696269]\n",
      "3295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005688] [C loss: 0.696269]\n",
      "3296 [D loss: 0.696271, acc.: 0.00%] [G loss: 0.006212] [C loss: 0.696268]\n",
      "3297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006765] [C loss: 0.696268]\n",
      "3298 [D loss: 0.696270, acc.: 0.00%] [G loss: 0.006998] [C loss: 0.696265]\n",
      "3299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004152] [C loss: 0.696265]\n",
      "3300 [D loss: 0.696267, acc.: 0.00%] [G loss: 0.004636] [C loss: 0.696265]\n",
      "3301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005274] [C loss: 0.696265]\n",
      "3302 [D loss: 0.696267, acc.: 0.00%] [G loss: 0.004655] [C loss: 0.696264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014825] [C loss: 0.696264]\n",
      "3304 [D loss: 0.696265, acc.: 0.00%] [G loss: 0.007011] [C loss: 0.696261]\n",
      "3305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004782] [C loss: 0.696261]\n",
      "3306 [D loss: 0.696265, acc.: 0.00%] [G loss: 0.006542] [C loss: 0.696262]\n",
      "3307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006825] [C loss: 0.696262]\n",
      "3308 [D loss: 0.696265, acc.: 0.00%] [G loss: 0.005894] [C loss: 0.696261]\n",
      "3309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007380] [C loss: 0.696261]\n",
      "3310 [D loss: 0.696262, acc.: 0.00%] [G loss: 0.005492] [C loss: 0.696260]\n",
      "3311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003901] [C loss: 0.696260]\n",
      "3312 [D loss: 0.696262, acc.: 0.00%] [G loss: 0.005011] [C loss: 0.696259]\n",
      "3313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004126] [C loss: 0.696259]\n",
      "3314 [D loss: 0.696261, acc.: 0.00%] [G loss: 0.004918] [C loss: 0.696256]\n",
      "3315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005797] [C loss: 0.696256]\n",
      "3316 [D loss: 0.696258, acc.: 0.00%] [G loss: 0.005742] [C loss: 0.696255]\n",
      "3317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005414] [C loss: 0.696255]\n",
      "3318 [D loss: 0.696258, acc.: 0.00%] [G loss: 0.005070] [C loss: 0.696253]\n",
      "3319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006632] [C loss: 0.696253]\n",
      "3320 [D loss: 0.696256, acc.: 0.00%] [G loss: 0.006372] [C loss: 0.696252]\n",
      "3321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004968] [C loss: 0.696252]\n",
      "3322 [D loss: 0.696255, acc.: 0.00%] [G loss: 0.008179] [C loss: 0.696251]\n",
      "3323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005588] [C loss: 0.696251]\n",
      "3324 [D loss: 0.696253, acc.: 0.00%] [G loss: 0.004563] [C loss: 0.696250]\n",
      "3325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004798] [C loss: 0.696250]\n",
      "3326 [D loss: 0.696252, acc.: 0.00%] [G loss: 0.005349] [C loss: 0.696250]\n",
      "3327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005120] [C loss: 0.696250]\n",
      "3328 [D loss: 0.696252, acc.: 0.00%] [G loss: 0.003711] [C loss: 0.696248]\n",
      "3329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005289] [C loss: 0.696248]\n",
      "3330 [D loss: 0.696250, acc.: 0.00%] [G loss: 0.005814] [C loss: 0.696247]\n",
      "3331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010094] [C loss: 0.696247]\n",
      "3332 [D loss: 0.696250, acc.: 0.00%] [G loss: 0.007581] [C loss: 0.696247]\n",
      "3333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006189] [C loss: 0.696247]\n",
      "3334 [D loss: 0.696248, acc.: 0.00%] [G loss: 0.006066] [C loss: 0.696244]\n",
      "3335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006663] [C loss: 0.696244]\n",
      "3336 [D loss: 0.696247, acc.: 0.00%] [G loss: 0.006724] [C loss: 0.696244]\n",
      "3337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007440] [C loss: 0.696244]\n",
      "3338 [D loss: 0.696245, acc.: 0.00%] [G loss: 0.008171] [C loss: 0.696243]\n",
      "3339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005971] [C loss: 0.696243]\n",
      "3340 [D loss: 0.696245, acc.: 0.00%] [G loss: 0.006392] [C loss: 0.696243]\n",
      "3341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005462] [C loss: 0.696243]\n",
      "3342 [D loss: 0.696244, acc.: 0.00%] [G loss: 0.006881] [C loss: 0.696241]\n",
      "3343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005593] [C loss: 0.696241]\n",
      "3344 [D loss: 0.696242, acc.: 0.00%] [G loss: 0.014142] [C loss: 0.696239]\n",
      "3345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004672] [C loss: 0.696239]\n",
      "3346 [D loss: 0.696241, acc.: 0.00%] [G loss: 0.005155] [C loss: 0.696237]\n",
      "3347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004988] [C loss: 0.696237]\n",
      "3348 [D loss: 0.696239, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.696237]\n",
      "3349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006052] [C loss: 0.696237]\n",
      "3350 [D loss: 0.696238, acc.: 0.00%] [G loss: 0.005658] [C loss: 0.696238]\n",
      "3351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006555] [C loss: 0.696238]\n",
      "3352 [D loss: 0.696238, acc.: 0.00%] [G loss: 0.005676] [C loss: 0.696235]\n",
      "3353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005592] [C loss: 0.696235]\n",
      "3354 [D loss: 0.696236, acc.: 0.00%] [G loss: 0.003605] [C loss: 0.696233]\n",
      "3355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005417] [C loss: 0.696233]\n",
      "3356 [D loss: 0.696234, acc.: 0.00%] [G loss: 0.005171] [C loss: 0.696231]\n",
      "3357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006162] [C loss: 0.696231]\n",
      "3358 [D loss: 0.696234, acc.: 0.00%] [G loss: 0.007707] [C loss: 0.696230]\n",
      "3359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004876] [C loss: 0.696230]\n",
      "3360 [D loss: 0.696232, acc.: 0.00%] [G loss: 0.005921] [C loss: 0.696229]\n",
      "3361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007670] [C loss: 0.696229]\n",
      "3362 [D loss: 0.696231, acc.: 0.00%] [G loss: 0.007588] [C loss: 0.696228]\n",
      "3363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004317] [C loss: 0.696228]\n",
      "3364 [D loss: 0.696229, acc.: 0.00%] [G loss: 0.007048] [C loss: 0.696225]\n",
      "3365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006078] [C loss: 0.696225]\n",
      "3366 [D loss: 0.696227, acc.: 0.00%] [G loss: 0.013912] [C loss: 0.696222]\n",
      "3367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007704] [C loss: 0.696222]\n",
      "3368 [D loss: 0.696226, acc.: 0.00%] [G loss: 0.005007] [C loss: 0.696222]\n",
      "3369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005891] [C loss: 0.696222]\n",
      "3370 [D loss: 0.696224, acc.: 0.00%] [G loss: 0.006638] [C loss: 0.696221]\n",
      "3371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006903] [C loss: 0.696221]\n",
      "3372 [D loss: 0.696223, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.696221]\n",
      "3373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004578] [C loss: 0.696221]\n",
      "3374 [D loss: 0.696223, acc.: 0.00%] [G loss: 0.007441] [C loss: 0.696219]\n",
      "3375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004970] [C loss: 0.696219]\n",
      "3376 [D loss: 0.696221, acc.: 0.00%] [G loss: 0.004284] [C loss: 0.696216]\n",
      "3377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012607] [C loss: 0.696216]\n",
      "3378 [D loss: 0.696219, acc.: 0.00%] [G loss: 0.005877] [C loss: 0.696218]\n",
      "3379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005720] [C loss: 0.696218]\n",
      "3380 [D loss: 0.696220, acc.: 0.00%] [G loss: 0.006122] [C loss: 0.696215]\n",
      "3381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003847] [C loss: 0.696215]\n",
      "3382 [D loss: 0.696218, acc.: 0.00%] [G loss: 0.004509] [C loss: 0.696213]\n",
      "3383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003754] [C loss: 0.696213]\n",
      "3384 [D loss: 0.696217, acc.: 0.00%] [G loss: 0.005470] [C loss: 0.696215]\n",
      "3385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004951] [C loss: 0.696215]\n",
      "3386 [D loss: 0.696216, acc.: 0.00%] [G loss: 0.006081] [C loss: 0.696211]\n",
      "3387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013562] [C loss: 0.696211]\n",
      "3388 [D loss: 0.696214, acc.: 0.00%] [G loss: 0.011412] [C loss: 0.696212]\n",
      "3389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004745] [C loss: 0.696212]\n",
      "3390 [D loss: 0.696213, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.696207]\n",
      "3391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016342] [C loss: 0.696207]\n",
      "3392 [D loss: 0.696211, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.696209]\n",
      "3393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006490] [C loss: 0.696209]\n",
      "3394 [D loss: 0.696210, acc.: 0.00%] [G loss: 0.005318] [C loss: 0.696206]\n",
      "3395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004088] [C loss: 0.696206]\n",
      "3396 [D loss: 0.696210, acc.: 0.00%] [G loss: 0.005948] [C loss: 0.696204]\n",
      "3397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005492] [C loss: 0.696204]\n",
      "3398 [D loss: 0.696207, acc.: 0.00%] [G loss: 0.003583] [C loss: 0.696204]\n",
      "3399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006019] [C loss: 0.696204]\n",
      "3400 [D loss: 0.696206, acc.: 0.00%] [G loss: 0.010239] [C loss: 0.696203]\n",
      "3401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008028] [C loss: 0.696203]\n",
      "3402 [D loss: 0.696205, acc.: 0.00%] [G loss: 0.005274] [C loss: 0.696204]\n",
      "3403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004114] [C loss: 0.696204]\n",
      "3404 [D loss: 0.696205, acc.: 0.00%] [G loss: 0.004947] [C loss: 0.696200]\n",
      "3405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005004] [C loss: 0.696200]\n",
      "3406 [D loss: 0.696202, acc.: 0.00%] [G loss: 0.005697] [C loss: 0.696201]\n",
      "3407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006544] [C loss: 0.696201]\n",
      "3408 [D loss: 0.696203, acc.: 0.00%] [G loss: 0.008648] [C loss: 0.696200]\n",
      "3409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007903] [C loss: 0.696200]\n",
      "3410 [D loss: 0.696202, acc.: 0.00%] [G loss: 0.005145] [C loss: 0.696198]\n",
      "3411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006980] [C loss: 0.696198]\n",
      "3412 [D loss: 0.696199, acc.: 0.00%] [G loss: 0.003534] [C loss: 0.696196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006206] [C loss: 0.696196]\n",
      "3414 [D loss: 0.696198, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.696197]\n",
      "3415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007077] [C loss: 0.696197]\n",
      "3416 [D loss: 0.696198, acc.: 0.00%] [G loss: 0.007830] [C loss: 0.696193]\n",
      "3417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008385] [C loss: 0.696193]\n",
      "3418 [D loss: 0.696194, acc.: 0.00%] [G loss: 0.006808] [C loss: 0.696191]\n",
      "3419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004946] [C loss: 0.696191]\n",
      "3420 [D loss: 0.696193, acc.: 0.00%] [G loss: 0.006862] [C loss: 0.696191]\n",
      "3421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005506] [C loss: 0.696191]\n",
      "3422 [D loss: 0.696193, acc.: 0.00%] [G loss: 0.004726] [C loss: 0.696189]\n",
      "3423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007115] [C loss: 0.696189]\n",
      "3424 [D loss: 0.696191, acc.: 0.00%] [G loss: 0.020824] [C loss: 0.696189]\n",
      "3425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004544] [C loss: 0.696189]\n",
      "3426 [D loss: 0.696190, acc.: 0.00%] [G loss: 0.005065] [C loss: 0.696187]\n",
      "3427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010571] [C loss: 0.696187]\n",
      "3428 [D loss: 0.696189, acc.: 0.00%] [G loss: 0.005666] [C loss: 0.696184]\n",
      "3429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007240] [C loss: 0.696184]\n",
      "3430 [D loss: 0.696188, acc.: 0.00%] [G loss: 0.006748] [C loss: 0.696184]\n",
      "3431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004678] [C loss: 0.696184]\n",
      "3432 [D loss: 0.696187, acc.: 0.00%] [G loss: 0.006506] [C loss: 0.696181]\n",
      "3433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004572] [C loss: 0.696181]\n",
      "3434 [D loss: 0.696184, acc.: 0.00%] [G loss: 0.005515] [C loss: 0.696181]\n",
      "3435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005398] [C loss: 0.696181]\n",
      "3436 [D loss: 0.696184, acc.: 0.00%] [G loss: 0.010232] [C loss: 0.696179]\n",
      "3437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005030] [C loss: 0.696179]\n",
      "3438 [D loss: 0.696181, acc.: 0.00%] [G loss: 0.005565] [C loss: 0.696178]\n",
      "3439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006824] [C loss: 0.696178]\n",
      "3440 [D loss: 0.696181, acc.: 0.00%] [G loss: 0.004519] [C loss: 0.696175]\n",
      "3441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005302] [C loss: 0.696175]\n",
      "3442 [D loss: 0.696179, acc.: 0.00%] [G loss: 0.007115] [C loss: 0.696175]\n",
      "3443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004688] [C loss: 0.696175]\n",
      "3444 [D loss: 0.696178, acc.: 0.00%] [G loss: 0.005798] [C loss: 0.696175]\n",
      "3445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006854] [C loss: 0.696175]\n",
      "3446 [D loss: 0.696177, acc.: 0.00%] [G loss: 0.005256] [C loss: 0.696172]\n",
      "3447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004995] [C loss: 0.696172]\n",
      "3448 [D loss: 0.696175, acc.: 0.00%] [G loss: 0.010125] [C loss: 0.696172]\n",
      "3449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006312] [C loss: 0.696172]\n",
      "3450 [D loss: 0.696175, acc.: 0.00%] [G loss: 0.004761] [C loss: 0.696171]\n",
      "3451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006513] [C loss: 0.696171]\n",
      "3452 [D loss: 0.696173, acc.: 0.00%] [G loss: 0.005319] [C loss: 0.696171]\n",
      "3453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007148] [C loss: 0.696171]\n",
      "3454 [D loss: 0.696174, acc.: 0.00%] [G loss: 0.008911] [C loss: 0.696169]\n",
      "3455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005945] [C loss: 0.696169]\n",
      "3456 [D loss: 0.696170, acc.: 0.00%] [G loss: 0.004667] [C loss: 0.696168]\n",
      "3457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.696168]\n",
      "3458 [D loss: 0.696169, acc.: 0.00%] [G loss: 0.005216] [C loss: 0.696166]\n",
      "3459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.696166]\n",
      "3460 [D loss: 0.696168, acc.: 0.00%] [G loss: 0.020212] [C loss: 0.696166]\n",
      "3461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006746] [C loss: 0.696166]\n",
      "3462 [D loss: 0.696167, acc.: 0.00%] [G loss: 0.005063] [C loss: 0.696165]\n",
      "3463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004607] [C loss: 0.696165]\n",
      "3464 [D loss: 0.696167, acc.: 0.00%] [G loss: 0.008428] [C loss: 0.696166]\n",
      "3465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006211] [C loss: 0.696166]\n",
      "3466 [D loss: 0.696166, acc.: 0.00%] [G loss: 0.005820] [C loss: 0.696161]\n",
      "3467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006278] [C loss: 0.696161]\n",
      "3468 [D loss: 0.696163, acc.: 0.00%] [G loss: 0.006693] [C loss: 0.696160]\n",
      "3469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008376] [C loss: 0.696160]\n",
      "3470 [D loss: 0.696161, acc.: 0.00%] [G loss: 0.006693] [C loss: 0.696159]\n",
      "3471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004649] [C loss: 0.696159]\n",
      "3472 [D loss: 0.696161, acc.: 0.00%] [G loss: 0.004550] [C loss: 0.696158]\n",
      "3473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004090] [C loss: 0.696158]\n",
      "3474 [D loss: 0.696160, acc.: 0.00%] [G loss: 0.005549] [C loss: 0.696155]\n",
      "3475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005062] [C loss: 0.696155]\n",
      "3476 [D loss: 0.696158, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.696154]\n",
      "3477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015258] [C loss: 0.696154]\n",
      "3478 [D loss: 0.696157, acc.: 0.00%] [G loss: 0.004596] [C loss: 0.696152]\n",
      "3479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011719] [C loss: 0.696152]\n",
      "3480 [D loss: 0.696155, acc.: 0.00%] [G loss: 0.005268] [C loss: 0.696151]\n",
      "3481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007736] [C loss: 0.696151]\n",
      "3482 [D loss: 0.696154, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.696152]\n",
      "3483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007354] [C loss: 0.696152]\n",
      "3484 [D loss: 0.696152, acc.: 0.00%] [G loss: 0.003988] [C loss: 0.696149]\n",
      "3485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005421] [C loss: 0.696149]\n",
      "3486 [D loss: 0.696152, acc.: 0.00%] [G loss: 0.005740] [C loss: 0.696149]\n",
      "3487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.696149]\n",
      "3488 [D loss: 0.696150, acc.: 0.00%] [G loss: 0.004320] [C loss: 0.696146]\n",
      "3489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013617] [C loss: 0.696146]\n",
      "3490 [D loss: 0.696149, acc.: 0.00%] [G loss: 0.007208] [C loss: 0.696144]\n",
      "3491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005373] [C loss: 0.696144]\n",
      "3492 [D loss: 0.696147, acc.: 0.00%] [G loss: 0.008092] [C loss: 0.696146]\n",
      "3493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005407] [C loss: 0.696146]\n",
      "3494 [D loss: 0.696147, acc.: 0.00%] [G loss: 0.004995] [C loss: 0.696145]\n",
      "3495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004067] [C loss: 0.696145]\n",
      "3496 [D loss: 0.696146, acc.: 0.00%] [G loss: 0.015871] [C loss: 0.696143]\n",
      "3497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.696143]\n",
      "3498 [D loss: 0.696145, acc.: 0.00%] [G loss: 0.005249] [C loss: 0.696141]\n",
      "3499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007888] [C loss: 0.696141]\n",
      "3500 [D loss: 0.696143, acc.: 0.00%] [G loss: 0.005414] [C loss: 0.696140]\n",
      "3501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006789] [C loss: 0.696140]\n",
      "3502 [D loss: 0.696141, acc.: 0.00%] [G loss: 0.006915] [C loss: 0.696137]\n",
      "3503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005749] [C loss: 0.696137]\n",
      "3504 [D loss: 0.696140, acc.: 0.00%] [G loss: 0.004036] [C loss: 0.696138]\n",
      "3505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004862] [C loss: 0.696138]\n",
      "3506 [D loss: 0.696140, acc.: 0.00%] [G loss: 0.011624] [C loss: 0.696137]\n",
      "3507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004694] [C loss: 0.696137]\n",
      "3508 [D loss: 0.696139, acc.: 0.00%] [G loss: 0.006344] [C loss: 0.696135]\n",
      "3509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004867] [C loss: 0.696135]\n",
      "3510 [D loss: 0.696136, acc.: 0.00%] [G loss: 0.004043] [C loss: 0.696132]\n",
      "3511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004810] [C loss: 0.696132]\n",
      "3512 [D loss: 0.696135, acc.: 0.00%] [G loss: 0.006721] [C loss: 0.696131]\n",
      "3513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005314] [C loss: 0.696131]\n",
      "3514 [D loss: 0.696134, acc.: 0.00%] [G loss: 0.005244] [C loss: 0.696129]\n",
      "3515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006272] [C loss: 0.696129]\n",
      "3516 [D loss: 0.696133, acc.: 0.00%] [G loss: 0.005362] [C loss: 0.696128]\n",
      "3517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004356] [C loss: 0.696128]\n",
      "3518 [D loss: 0.696131, acc.: 0.00%] [G loss: 0.006926] [C loss: 0.696127]\n",
      "3519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007801] [C loss: 0.696127]\n",
      "3520 [D loss: 0.696130, acc.: 0.00%] [G loss: 0.008444] [C loss: 0.696124]\n",
      "3521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005510] [C loss: 0.696124]\n",
      "3522 [D loss: 0.696127, acc.: 0.00%] [G loss: 0.008108] [C loss: 0.696124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008110] [C loss: 0.696124]\n",
      "3524 [D loss: 0.696126, acc.: 0.00%] [G loss: 0.006464] [C loss: 0.696125]\n",
      "3525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006420] [C loss: 0.696125]\n",
      "3526 [D loss: 0.696126, acc.: 0.00%] [G loss: 0.006695] [C loss: 0.696124]\n",
      "3527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006968] [C loss: 0.696124]\n",
      "3528 [D loss: 0.696125, acc.: 0.00%] [G loss: 0.006135] [C loss: 0.696120]\n",
      "3529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010393] [C loss: 0.696120]\n",
      "3530 [D loss: 0.696123, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.696121]\n",
      "3531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005131] [C loss: 0.696121]\n",
      "3532 [D loss: 0.696121, acc.: 0.00%] [G loss: 0.004975] [C loss: 0.696119]\n",
      "3533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007270] [C loss: 0.696119]\n",
      "3534 [D loss: 0.696121, acc.: 0.00%] [G loss: 0.004883] [C loss: 0.696117]\n",
      "3535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004721] [C loss: 0.696117]\n",
      "3536 [D loss: 0.696119, acc.: 0.00%] [G loss: 0.004983] [C loss: 0.696117]\n",
      "3537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006918] [C loss: 0.696117]\n",
      "3538 [D loss: 0.696119, acc.: 0.00%] [G loss: 0.007261] [C loss: 0.696114]\n",
      "3539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010768] [C loss: 0.696114]\n",
      "3540 [D loss: 0.696117, acc.: 0.00%] [G loss: 0.007791] [C loss: 0.696115]\n",
      "3541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004437] [C loss: 0.696115]\n",
      "3542 [D loss: 0.696117, acc.: 0.00%] [G loss: 0.007384] [C loss: 0.696111]\n",
      "3543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006523] [C loss: 0.696111]\n",
      "3544 [D loss: 0.696115, acc.: 0.00%] [G loss: 0.006718] [C loss: 0.696109]\n",
      "3545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007692] [C loss: 0.696109]\n",
      "3546 [D loss: 0.696112, acc.: 0.00%] [G loss: 0.006552] [C loss: 0.696111]\n",
      "3547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008689] [C loss: 0.696111]\n",
      "3548 [D loss: 0.696112, acc.: 0.00%] [G loss: 0.006098] [C loss: 0.696107]\n",
      "3549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005652] [C loss: 0.696107]\n",
      "3550 [D loss: 0.696110, acc.: 0.00%] [G loss: 0.004173] [C loss: 0.696107]\n",
      "3551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008258] [C loss: 0.696107]\n",
      "3552 [D loss: 0.696109, acc.: 0.00%] [G loss: 0.008748] [C loss: 0.696105]\n",
      "3553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006707] [C loss: 0.696105]\n",
      "3554 [D loss: 0.696108, acc.: 0.00%] [G loss: 0.005520] [C loss: 0.696105]\n",
      "3555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006151] [C loss: 0.696105]\n",
      "3556 [D loss: 0.696108, acc.: 0.00%] [G loss: 0.025717] [C loss: 0.696103]\n",
      "3557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004728] [C loss: 0.696103]\n",
      "3558 [D loss: 0.696106, acc.: 0.00%] [G loss: 0.006139] [C loss: 0.696103]\n",
      "3559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004818] [C loss: 0.696103]\n",
      "3560 [D loss: 0.696105, acc.: 0.00%] [G loss: 0.003849] [C loss: 0.696101]\n",
      "3561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.696101]\n",
      "3562 [D loss: 0.696103, acc.: 0.00%] [G loss: 0.008828] [C loss: 0.696100]\n",
      "3563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006517] [C loss: 0.696100]\n",
      "3564 [D loss: 0.696102, acc.: 0.00%] [G loss: 0.022435] [C loss: 0.696101]\n",
      "3565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006033] [C loss: 0.696101]\n",
      "3566 [D loss: 0.696102, acc.: 0.00%] [G loss: 0.006250] [C loss: 0.696097]\n",
      "3567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004798] [C loss: 0.696097]\n",
      "3568 [D loss: 0.696099, acc.: 0.00%] [G loss: 0.005334] [C loss: 0.696094]\n",
      "3569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007210] [C loss: 0.696094]\n",
      "3570 [D loss: 0.696097, acc.: 0.00%] [G loss: 0.006650] [C loss: 0.696095]\n",
      "3571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008070] [C loss: 0.696095]\n",
      "3572 [D loss: 0.696097, acc.: 0.00%] [G loss: 0.008387] [C loss: 0.696095]\n",
      "3573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006314] [C loss: 0.696095]\n",
      "3574 [D loss: 0.696096, acc.: 0.00%] [G loss: 0.006164] [C loss: 0.696092]\n",
      "3575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007026] [C loss: 0.696092]\n",
      "3576 [D loss: 0.696095, acc.: 0.00%] [G loss: 0.007454] [C loss: 0.696090]\n",
      "3577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.696090]\n",
      "3578 [D loss: 0.696094, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.696087]\n",
      "3579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006115] [C loss: 0.696087]\n",
      "3580 [D loss: 0.696091, acc.: 0.00%] [G loss: 0.006039] [C loss: 0.696087]\n",
      "3581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.696087]\n",
      "3582 [D loss: 0.696090, acc.: 0.00%] [G loss: 0.005278] [C loss: 0.696087]\n",
      "3583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.696087]\n",
      "3584 [D loss: 0.696089, acc.: 0.00%] [G loss: 0.004199] [C loss: 0.696085]\n",
      "3585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005178] [C loss: 0.696085]\n",
      "3586 [D loss: 0.696088, acc.: 0.00%] [G loss: 0.006599] [C loss: 0.696086]\n",
      "3587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004497] [C loss: 0.696086]\n",
      "3588 [D loss: 0.696088, acc.: 0.00%] [G loss: 0.006347] [C loss: 0.696082]\n",
      "3589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004812] [C loss: 0.696082]\n",
      "3590 [D loss: 0.696085, acc.: 0.00%] [G loss: 0.004497] [C loss: 0.696081]\n",
      "3591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004492] [C loss: 0.696081]\n",
      "3592 [D loss: 0.696084, acc.: 0.00%] [G loss: 0.006119] [C loss: 0.696080]\n",
      "3593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004041] [C loss: 0.696080]\n",
      "3594 [D loss: 0.696082, acc.: 0.00%] [G loss: 0.006667] [C loss: 0.696078]\n",
      "3595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006713] [C loss: 0.696078]\n",
      "3596 [D loss: 0.696081, acc.: 0.00%] [G loss: 0.005101] [C loss: 0.696078]\n",
      "3597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022367] [C loss: 0.696078]\n",
      "3598 [D loss: 0.696080, acc.: 0.00%] [G loss: 0.005699] [C loss: 0.696077]\n",
      "3599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010228] [C loss: 0.696077]\n",
      "3600 [D loss: 0.696080, acc.: 0.00%] [G loss: 0.004751] [C loss: 0.696075]\n",
      "3601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017500] [C loss: 0.696075]\n",
      "3602 [D loss: 0.696078, acc.: 0.00%] [G loss: 0.006444] [C loss: 0.696074]\n",
      "3603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.696074]\n",
      "3604 [D loss: 0.696075, acc.: 0.00%] [G loss: 0.008135] [C loss: 0.696074]\n",
      "3605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006163] [C loss: 0.696074]\n",
      "3606 [D loss: 0.696076, acc.: 0.00%] [G loss: 0.004801] [C loss: 0.696071]\n",
      "3607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006226] [C loss: 0.696071]\n",
      "3608 [D loss: 0.696073, acc.: 0.00%] [G loss: 0.004448] [C loss: 0.696070]\n",
      "3609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006147] [C loss: 0.696070]\n",
      "3610 [D loss: 0.696072, acc.: 0.00%] [G loss: 0.009690] [C loss: 0.696069]\n",
      "3611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006268] [C loss: 0.696069]\n",
      "3612 [D loss: 0.696072, acc.: 0.00%] [G loss: 0.007137] [C loss: 0.696068]\n",
      "3613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004231] [C loss: 0.696068]\n",
      "3614 [D loss: 0.696069, acc.: 0.00%] [G loss: 0.006380] [C loss: 0.696066]\n",
      "3615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005296] [C loss: 0.696066]\n",
      "3616 [D loss: 0.696068, acc.: 0.00%] [G loss: 0.005398] [C loss: 0.696065]\n",
      "3617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003539] [C loss: 0.696065]\n",
      "3618 [D loss: 0.696067, acc.: 0.00%] [G loss: 0.005356] [C loss: 0.696064]\n",
      "3619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006882] [C loss: 0.696064]\n",
      "3620 [D loss: 0.696066, acc.: 0.00%] [G loss: 0.007134] [C loss: 0.696062]\n",
      "3621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006986] [C loss: 0.696062]\n",
      "3622 [D loss: 0.696065, acc.: 0.00%] [G loss: 0.004474] [C loss: 0.696060]\n",
      "3623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012284] [C loss: 0.696060]\n",
      "3624 [D loss: 0.696063, acc.: 0.00%] [G loss: 0.004147] [C loss: 0.696059]\n",
      "3625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005892] [C loss: 0.696059]\n",
      "3626 [D loss: 0.696062, acc.: 0.00%] [G loss: 0.008087] [C loss: 0.696059]\n",
      "3627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005534] [C loss: 0.696059]\n",
      "3628 [D loss: 0.696061, acc.: 0.00%] [G loss: 0.003618] [C loss: 0.696056]\n",
      "3629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005782] [C loss: 0.696056]\n",
      "3630 [D loss: 0.696059, acc.: 0.00%] [G loss: 0.014311] [C loss: 0.696056]\n",
      "3631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007783] [C loss: 0.696056]\n",
      "3632 [D loss: 0.696059, acc.: 0.00%] [G loss: 0.007934] [C loss: 0.696055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005037] [C loss: 0.696055]\n",
      "3634 [D loss: 0.696057, acc.: 0.00%] [G loss: 0.007605] [C loss: 0.696053]\n",
      "3635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009224] [C loss: 0.696053]\n",
      "3636 [D loss: 0.696056, acc.: 0.00%] [G loss: 0.009161] [C loss: 0.696053]\n",
      "3637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004205] [C loss: 0.696053]\n",
      "3638 [D loss: 0.696055, acc.: 0.00%] [G loss: 0.004494] [C loss: 0.696052]\n",
      "3639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005410] [C loss: 0.696052]\n",
      "3640 [D loss: 0.696053, acc.: 0.00%] [G loss: 0.008223] [C loss: 0.696051]\n",
      "3641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006614] [C loss: 0.696051]\n",
      "3642 [D loss: 0.696052, acc.: 0.00%] [G loss: 0.005927] [C loss: 0.696048]\n",
      "3643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006443] [C loss: 0.696048]\n",
      "3644 [D loss: 0.696051, acc.: 0.00%] [G loss: 0.004808] [C loss: 0.696048]\n",
      "3645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005977] [C loss: 0.696048]\n",
      "3646 [D loss: 0.696049, acc.: 0.00%] [G loss: 0.005087] [C loss: 0.696045]\n",
      "3647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019845] [C loss: 0.696045]\n",
      "3648 [D loss: 0.696048, acc.: 0.00%] [G loss: 0.008574] [C loss: 0.696044]\n",
      "3649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.696044]\n",
      "3650 [D loss: 0.696047, acc.: 0.00%] [G loss: 0.005503] [C loss: 0.696043]\n",
      "3651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005374] [C loss: 0.696043]\n",
      "3652 [D loss: 0.696047, acc.: 0.00%] [G loss: 0.006257] [C loss: 0.696042]\n",
      "3653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004229] [C loss: 0.696042]\n",
      "3654 [D loss: 0.696044, acc.: 0.00%] [G loss: 0.006090] [C loss: 0.696040]\n",
      "3655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009833] [C loss: 0.696040]\n",
      "3656 [D loss: 0.696043, acc.: 0.00%] [G loss: 0.006134] [C loss: 0.696038]\n",
      "3657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.696038]\n",
      "3658 [D loss: 0.696041, acc.: 0.00%] [G loss: 0.005816] [C loss: 0.696036]\n",
      "3659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005641] [C loss: 0.696036]\n",
      "3660 [D loss: 0.696040, acc.: 0.00%] [G loss: 0.014815] [C loss: 0.696037]\n",
      "3661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005621] [C loss: 0.696037]\n",
      "3662 [D loss: 0.696038, acc.: 0.00%] [G loss: 0.005121] [C loss: 0.696036]\n",
      "3663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008859] [C loss: 0.696036]\n",
      "3664 [D loss: 0.696038, acc.: 0.00%] [G loss: 0.003923] [C loss: 0.696035]\n",
      "3665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006138] [C loss: 0.696035]\n",
      "3666 [D loss: 0.696037, acc.: 0.00%] [G loss: 0.006581] [C loss: 0.696034]\n",
      "3667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005286] [C loss: 0.696034]\n",
      "3668 [D loss: 0.696035, acc.: 0.00%] [G loss: 0.006994] [C loss: 0.696029]\n",
      "3669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007326] [C loss: 0.696029]\n",
      "3670 [D loss: 0.696033, acc.: 0.00%] [G loss: 0.008308] [C loss: 0.696031]\n",
      "3671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006820] [C loss: 0.696031]\n",
      "3672 [D loss: 0.696033, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.696030]\n",
      "3673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005620] [C loss: 0.696030]\n",
      "3674 [D loss: 0.696032, acc.: 0.00%] [G loss: 0.005157] [C loss: 0.696028]\n",
      "3675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005791] [C loss: 0.696028]\n",
      "3676 [D loss: 0.696030, acc.: 0.00%] [G loss: 0.006412] [C loss: 0.696026]\n",
      "3677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006389] [C loss: 0.696026]\n",
      "3678 [D loss: 0.696028, acc.: 0.00%] [G loss: 0.006359] [C loss: 0.696027]\n",
      "3679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009290] [C loss: 0.696027]\n",
      "3680 [D loss: 0.696028, acc.: 0.00%] [G loss: 0.005509] [C loss: 0.696025]\n",
      "3681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004280] [C loss: 0.696025]\n",
      "3682 [D loss: 0.696027, acc.: 0.00%] [G loss: 0.004812] [C loss: 0.696023]\n",
      "3683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.696023]\n",
      "3684 [D loss: 0.696025, acc.: 0.00%] [G loss: 0.005911] [C loss: 0.696020]\n",
      "3685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005313] [C loss: 0.696020]\n",
      "3686 [D loss: 0.696023, acc.: 0.00%] [G loss: 0.005101] [C loss: 0.696020]\n",
      "3687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005510] [C loss: 0.696020]\n",
      "3688 [D loss: 0.696022, acc.: 0.00%] [G loss: 0.021442] [C loss: 0.696019]\n",
      "3689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004438] [C loss: 0.696019]\n",
      "3690 [D loss: 0.696021, acc.: 0.00%] [G loss: 0.003036] [C loss: 0.696017]\n",
      "3691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007032] [C loss: 0.696017]\n",
      "3692 [D loss: 0.696021, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.696014]\n",
      "3693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006164] [C loss: 0.696014]\n",
      "3694 [D loss: 0.696018, acc.: 0.00%] [G loss: 0.010996] [C loss: 0.696015]\n",
      "3695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005992] [C loss: 0.696015]\n",
      "3696 [D loss: 0.696017, acc.: 0.00%] [G loss: 0.009288] [C loss: 0.696013]\n",
      "3697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005764] [C loss: 0.696013]\n",
      "3698 [D loss: 0.696015, acc.: 0.00%] [G loss: 0.006330] [C loss: 0.696012]\n",
      "3699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006038] [C loss: 0.696012]\n",
      "3700 [D loss: 0.696015, acc.: 0.00%] [G loss: 0.005472] [C loss: 0.696011]\n",
      "3701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005814] [C loss: 0.696011]\n",
      "3702 [D loss: 0.696014, acc.: 0.00%] [G loss: 0.006081] [C loss: 0.696009]\n",
      "3703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009884] [C loss: 0.696009]\n",
      "3704 [D loss: 0.696011, acc.: 0.00%] [G loss: 0.007947] [C loss: 0.696008]\n",
      "3705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006926] [C loss: 0.696008]\n",
      "3706 [D loss: 0.696011, acc.: 0.00%] [G loss: 0.006072] [C loss: 0.696006]\n",
      "3707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005682] [C loss: 0.696006]\n",
      "3708 [D loss: 0.696009, acc.: 0.00%] [G loss: 0.005316] [C loss: 0.696004]\n",
      "3709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005029] [C loss: 0.696004]\n",
      "3710 [D loss: 0.696007, acc.: 0.00%] [G loss: 0.006020] [C loss: 0.696005]\n",
      "3711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006835] [C loss: 0.696005]\n",
      "3712 [D loss: 0.696007, acc.: 0.00%] [G loss: 0.007192] [C loss: 0.696002]\n",
      "3713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009308] [C loss: 0.696002]\n",
      "3714 [D loss: 0.696005, acc.: 0.00%] [G loss: 0.007058] [C loss: 0.696001]\n",
      "3715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004353] [C loss: 0.696001]\n",
      "3716 [D loss: 0.696004, acc.: 0.00%] [G loss: 0.007658] [C loss: 0.696000]\n",
      "3717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004632] [C loss: 0.696000]\n",
      "3718 [D loss: 0.696003, acc.: 0.00%] [G loss: 0.004448] [C loss: 0.695997]\n",
      "3719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004368] [C loss: 0.695997]\n",
      "3720 [D loss: 0.696000, acc.: 0.00%] [G loss: 0.005290] [C loss: 0.695997]\n",
      "3721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005878] [C loss: 0.695997]\n",
      "3722 [D loss: 0.695999, acc.: 0.00%] [G loss: 0.004179] [C loss: 0.695998]\n",
      "3723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008350] [C loss: 0.695998]\n",
      "3724 [D loss: 0.696000, acc.: 0.00%] [G loss: 0.006389] [C loss: 0.695995]\n",
      "3725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.695995]\n",
      "3726 [D loss: 0.695997, acc.: 0.00%] [G loss: 0.006464] [C loss: 0.695994]\n",
      "3727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007279] [C loss: 0.695994]\n",
      "3728 [D loss: 0.695997, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.695993]\n",
      "3729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005772] [C loss: 0.695993]\n",
      "3730 [D loss: 0.695996, acc.: 0.00%] [G loss: 0.009773] [C loss: 0.695990]\n",
      "3731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013280] [C loss: 0.695990]\n",
      "3732 [D loss: 0.695993, acc.: 0.00%] [G loss: 0.007108] [C loss: 0.695989]\n",
      "3733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.695989]\n",
      "3734 [D loss: 0.695992, acc.: 0.00%] [G loss: 0.006328] [C loss: 0.695988]\n",
      "3735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005086] [C loss: 0.695988]\n",
      "3736 [D loss: 0.695991, acc.: 0.00%] [G loss: 0.006497] [C loss: 0.695986]\n",
      "3737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005598] [C loss: 0.695986]\n",
      "3738 [D loss: 0.695989, acc.: 0.00%] [G loss: 0.003761] [C loss: 0.695985]\n",
      "3739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008087] [C loss: 0.695985]\n",
      "3740 [D loss: 0.695987, acc.: 0.00%] [G loss: 0.009282] [C loss: 0.695985]\n",
      "3741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004551] [C loss: 0.695985]\n",
      "3742 [D loss: 0.695987, acc.: 0.00%] [G loss: 0.005473] [C loss: 0.695984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006812] [C loss: 0.695984]\n",
      "3744 [D loss: 0.695986, acc.: 0.00%] [G loss: 0.006331] [C loss: 0.695981]\n",
      "3745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007366] [C loss: 0.695981]\n",
      "3746 [D loss: 0.695984, acc.: 0.00%] [G loss: 0.007693] [C loss: 0.695981]\n",
      "3747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004456] [C loss: 0.695981]\n",
      "3748 [D loss: 0.695983, acc.: 0.00%] [G loss: 0.011295] [C loss: 0.695979]\n",
      "3749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005723] [C loss: 0.695979]\n",
      "3750 [D loss: 0.695982, acc.: 0.00%] [G loss: 0.005098] [C loss: 0.695978]\n",
      "3751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007135] [C loss: 0.695978]\n",
      "3752 [D loss: 0.695980, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.695978]\n",
      "3753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007210] [C loss: 0.695978]\n",
      "3754 [D loss: 0.695979, acc.: 0.00%] [G loss: 0.008552] [C loss: 0.695977]\n",
      "3755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005681] [C loss: 0.695977]\n",
      "3756 [D loss: 0.695979, acc.: 0.00%] [G loss: 0.004764] [C loss: 0.695976]\n",
      "3757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006845] [C loss: 0.695976]\n",
      "3758 [D loss: 0.695978, acc.: 0.00%] [G loss: 0.004460] [C loss: 0.695974]\n",
      "3759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004806] [C loss: 0.695974]\n",
      "3760 [D loss: 0.695976, acc.: 0.00%] [G loss: 0.004448] [C loss: 0.695971]\n",
      "3761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005307] [C loss: 0.695971]\n",
      "3762 [D loss: 0.695974, acc.: 0.00%] [G loss: 0.005142] [C loss: 0.695971]\n",
      "3763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008918] [C loss: 0.695971]\n",
      "3764 [D loss: 0.695973, acc.: 0.00%] [G loss: 0.005828] [C loss: 0.695971]\n",
      "3765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005048] [C loss: 0.695971]\n",
      "3766 [D loss: 0.695972, acc.: 0.00%] [G loss: 0.006438] [C loss: 0.695968]\n",
      "3767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010306] [C loss: 0.695968]\n",
      "3768 [D loss: 0.695970, acc.: 0.00%] [G loss: 0.006699] [C loss: 0.695964]\n",
      "3769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005627] [C loss: 0.695964]\n",
      "3770 [D loss: 0.695967, acc.: 0.00%] [G loss: 0.003908] [C loss: 0.695964]\n",
      "3771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006030] [C loss: 0.695964]\n",
      "3772 [D loss: 0.695967, acc.: 0.00%] [G loss: 0.005689] [C loss: 0.695964]\n",
      "3773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005816] [C loss: 0.695964]\n",
      "3774 [D loss: 0.695968, acc.: 0.00%] [G loss: 0.010352] [C loss: 0.695961]\n",
      "3775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011146] [C loss: 0.695961]\n",
      "3776 [D loss: 0.695965, acc.: 0.00%] [G loss: 0.005829] [C loss: 0.695959]\n",
      "3777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004098] [C loss: 0.695959]\n",
      "3778 [D loss: 0.695963, acc.: 0.00%] [G loss: 0.022687] [C loss: 0.695959]\n",
      "3779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004536] [C loss: 0.695959]\n",
      "3780 [D loss: 0.695962, acc.: 0.00%] [G loss: 0.005722] [C loss: 0.695957]\n",
      "3781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007984] [C loss: 0.695957]\n",
      "3782 [D loss: 0.695961, acc.: 0.00%] [G loss: 0.009830] [C loss: 0.695954]\n",
      "3783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006092] [C loss: 0.695954]\n",
      "3784 [D loss: 0.695958, acc.: 0.00%] [G loss: 0.008775] [C loss: 0.695956]\n",
      "3785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.695956]\n",
      "3786 [D loss: 0.695966, acc.: 6.25%] [G loss: 0.005496] [C loss: 0.695956]\n",
      "3787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008538] [C loss: 0.695956]\n",
      "3788 [D loss: 0.695957, acc.: 0.00%] [G loss: 0.006695] [C loss: 0.695953]\n",
      "3789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003895] [C loss: 0.695953]\n",
      "3790 [D loss: 0.695955, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.695950]\n",
      "3791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004752] [C loss: 0.695950]\n",
      "3792 [D loss: 0.695953, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.695952]\n",
      "3793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010639] [C loss: 0.695952]\n",
      "3794 [D loss: 0.695954, acc.: 0.00%] [G loss: 0.006039] [C loss: 0.695950]\n",
      "3795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007562] [C loss: 0.695950]\n",
      "3796 [D loss: 0.695952, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.695947]\n",
      "3797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.695947]\n",
      "3798 [D loss: 0.695950, acc.: 0.00%] [G loss: 0.004976] [C loss: 0.695946]\n",
      "3799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006884] [C loss: 0.695946]\n",
      "3800 [D loss: 0.695949, acc.: 0.00%] [G loss: 0.004945] [C loss: 0.695945]\n",
      "3801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013522] [C loss: 0.695945]\n",
      "3802 [D loss: 0.695947, acc.: 0.00%] [G loss: 0.004608] [C loss: 0.695944]\n",
      "3803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011593] [C loss: 0.695944]\n",
      "3804 [D loss: 0.695948, acc.: 0.00%] [G loss: 0.007409] [C loss: 0.695942]\n",
      "3805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.695942]\n",
      "3806 [D loss: 0.695945, acc.: 0.00%] [G loss: 0.010241] [C loss: 0.695941]\n",
      "3807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011657] [C loss: 0.695941]\n",
      "3808 [D loss: 0.695943, acc.: 0.00%] [G loss: 0.005937] [C loss: 0.695939]\n",
      "3809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.695939]\n",
      "3810 [D loss: 0.695942, acc.: 0.00%] [G loss: 0.005929] [C loss: 0.695938]\n",
      "3811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011545] [C loss: 0.695938]\n",
      "3812 [D loss: 0.695940, acc.: 0.00%] [G loss: 0.005811] [C loss: 0.695939]\n",
      "3813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005375] [C loss: 0.695939]\n",
      "3814 [D loss: 0.695940, acc.: 0.00%] [G loss: 0.004058] [C loss: 0.695936]\n",
      "3815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006026] [C loss: 0.695936]\n",
      "3816 [D loss: 0.695938, acc.: 0.00%] [G loss: 0.005425] [C loss: 0.695933]\n",
      "3817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.695933]\n",
      "3818 [D loss: 0.695937, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.695936]\n",
      "3819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013331] [C loss: 0.695936]\n",
      "3820 [D loss: 0.695936, acc.: 0.00%] [G loss: 0.005083] [C loss: 0.695932]\n",
      "3821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007144] [C loss: 0.695932]\n",
      "3822 [D loss: 0.695935, acc.: 0.00%] [G loss: 0.005239] [C loss: 0.695930]\n",
      "3823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009045] [C loss: 0.695930]\n",
      "3824 [D loss: 0.695933, acc.: 0.00%] [G loss: 0.004888] [C loss: 0.695929]\n",
      "3825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005805] [C loss: 0.695929]\n",
      "3826 [D loss: 0.695931, acc.: 0.00%] [G loss: 0.005634] [C loss: 0.695927]\n",
      "3827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006698] [C loss: 0.695927]\n",
      "3828 [D loss: 0.695930, acc.: 0.00%] [G loss: 0.005993] [C loss: 0.695926]\n",
      "3829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007179] [C loss: 0.695926]\n",
      "3830 [D loss: 0.695929, acc.: 0.00%] [G loss: 0.010800] [C loss: 0.695923]\n",
      "3831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008557] [C loss: 0.695923]\n",
      "3832 [D loss: 0.695928, acc.: 0.00%] [G loss: 0.006088] [C loss: 0.695925]\n",
      "3833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003884] [C loss: 0.695925]\n",
      "3834 [D loss: 0.695927, acc.: 0.00%] [G loss: 0.006381] [C loss: 0.695921]\n",
      "3835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006757] [C loss: 0.695921]\n",
      "3836 [D loss: 0.695925, acc.: 0.00%] [G loss: 0.005656] [C loss: 0.695921]\n",
      "3837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006037] [C loss: 0.695921]\n",
      "3838 [D loss: 0.695923, acc.: 0.00%] [G loss: 0.005379] [C loss: 0.695920]\n",
      "3839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007061] [C loss: 0.695920]\n",
      "3840 [D loss: 0.695923, acc.: 0.00%] [G loss: 0.005386] [C loss: 0.695918]\n",
      "3841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005651] [C loss: 0.695918]\n",
      "3842 [D loss: 0.695921, acc.: 0.00%] [G loss: 0.004687] [C loss: 0.695915]\n",
      "3843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005548] [C loss: 0.695915]\n",
      "3844 [D loss: 0.695919, acc.: 0.00%] [G loss: 0.006406] [C loss: 0.695916]\n",
      "3845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006518] [C loss: 0.695916]\n",
      "3846 [D loss: 0.695918, acc.: 0.00%] [G loss: 0.005937] [C loss: 0.695913]\n",
      "3847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005902] [C loss: 0.695913]\n",
      "3848 [D loss: 0.695916, acc.: 0.00%] [G loss: 0.004477] [C loss: 0.695912]\n",
      "3849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006343] [C loss: 0.695912]\n",
      "3850 [D loss: 0.695915, acc.: 0.00%] [G loss: 0.004995] [C loss: 0.695911]\n",
      "3851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005223] [C loss: 0.695911]\n",
      "3852 [D loss: 0.695914, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.695911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006665] [C loss: 0.695911]\n",
      "3854 [D loss: 0.695912, acc.: 0.00%] [G loss: 0.005276] [C loss: 0.695909]\n",
      "3855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006213] [C loss: 0.695909]\n",
      "3856 [D loss: 0.695910, acc.: 0.00%] [G loss: 0.007615] [C loss: 0.695909]\n",
      "3857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005273] [C loss: 0.695909]\n",
      "3858 [D loss: 0.695911, acc.: 0.00%] [G loss: 0.006459] [C loss: 0.695907]\n",
      "3859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004824] [C loss: 0.695907]\n",
      "3860 [D loss: 0.695909, acc.: 0.00%] [G loss: 0.005253] [C loss: 0.695906]\n",
      "3861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006528] [C loss: 0.695906]\n",
      "3862 [D loss: 0.695909, acc.: 0.00%] [G loss: 0.005007] [C loss: 0.695904]\n",
      "3863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004055] [C loss: 0.695904]\n",
      "3864 [D loss: 0.695907, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.695902]\n",
      "3865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005160] [C loss: 0.695902]\n",
      "3866 [D loss: 0.695905, acc.: 0.00%] [G loss: 0.004896] [C loss: 0.695901]\n",
      "3867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004982] [C loss: 0.695901]\n",
      "3868 [D loss: 0.695903, acc.: 0.00%] [G loss: 0.007448] [C loss: 0.695899]\n",
      "3869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004491] [C loss: 0.695899]\n",
      "3870 [D loss: 0.695903, acc.: 0.00%] [G loss: 0.005930] [C loss: 0.695898]\n",
      "3871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006272] [C loss: 0.695898]\n",
      "3872 [D loss: 0.695901, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.695898]\n",
      "3873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005018] [C loss: 0.695898]\n",
      "3874 [D loss: 0.695900, acc.: 0.00%] [G loss: 0.005429] [C loss: 0.695894]\n",
      "3875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006137] [C loss: 0.695894]\n",
      "3876 [D loss: 0.695897, acc.: 0.00%] [G loss: 0.014188] [C loss: 0.695895]\n",
      "3877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006867] [C loss: 0.695895]\n",
      "3878 [D loss: 0.695897, acc.: 0.00%] [G loss: 0.004870] [C loss: 0.695892]\n",
      "3879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007943] [C loss: 0.695892]\n",
      "3880 [D loss: 0.695896, acc.: 0.00%] [G loss: 0.009500] [C loss: 0.695891]\n",
      "3881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004536] [C loss: 0.695891]\n",
      "3882 [D loss: 0.695894, acc.: 0.00%] [G loss: 0.007559] [C loss: 0.695890]\n",
      "3883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008462] [C loss: 0.695890]\n",
      "3884 [D loss: 0.695893, acc.: 0.00%] [G loss: 0.004214] [C loss: 0.695889]\n",
      "3885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007439] [C loss: 0.695889]\n",
      "3886 [D loss: 0.695891, acc.: 0.00%] [G loss: 0.003678] [C loss: 0.695888]\n",
      "3887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.695888]\n",
      "3888 [D loss: 0.695889, acc.: 0.00%] [G loss: 0.004772] [C loss: 0.695887]\n",
      "3889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010152] [C loss: 0.695887]\n",
      "3890 [D loss: 0.695889, acc.: 0.00%] [G loss: 0.007472] [C loss: 0.695883]\n",
      "3891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006777] [C loss: 0.695883]\n",
      "3892 [D loss: 0.695886, acc.: 0.00%] [G loss: 0.004854] [C loss: 0.695882]\n",
      "3893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005094] [C loss: 0.695882]\n",
      "3894 [D loss: 0.695886, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.695884]\n",
      "3895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006086] [C loss: 0.695884]\n",
      "3896 [D loss: 0.695885, acc.: 0.00%] [G loss: 0.007504] [C loss: 0.695880]\n",
      "3897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012600] [C loss: 0.695880]\n",
      "3898 [D loss: 0.695882, acc.: 0.00%] [G loss: 0.008233] [C loss: 0.695880]\n",
      "3899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005254] [C loss: 0.695880]\n",
      "3900 [D loss: 0.695882, acc.: 0.00%] [G loss: 0.006455] [C loss: 0.695877]\n",
      "3901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011812] [C loss: 0.695877]\n",
      "3902 [D loss: 0.695880, acc.: 0.00%] [G loss: 0.010231] [C loss: 0.695877]\n",
      "3903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005872] [C loss: 0.695877]\n",
      "3904 [D loss: 0.695879, acc.: 0.00%] [G loss: 0.005411] [C loss: 0.695875]\n",
      "3905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008144] [C loss: 0.695875]\n",
      "3906 [D loss: 0.695878, acc.: 0.00%] [G loss: 0.006396] [C loss: 0.695872]\n",
      "3907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006867] [C loss: 0.695872]\n",
      "3908 [D loss: 0.695876, acc.: 0.00%] [G loss: 0.013010] [C loss: 0.695871]\n",
      "3909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003839] [C loss: 0.695871]\n",
      "3910 [D loss: 0.695874, acc.: 0.00%] [G loss: 0.005245] [C loss: 0.695872]\n",
      "3911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007586] [C loss: 0.695872]\n",
      "3912 [D loss: 0.695874, acc.: 0.00%] [G loss: 0.004529] [C loss: 0.695871]\n",
      "3913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005344] [C loss: 0.695871]\n",
      "3914 [D loss: 0.695873, acc.: 0.00%] [G loss: 0.010234] [C loss: 0.695868]\n",
      "3915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004146] [C loss: 0.695868]\n",
      "3916 [D loss: 0.695871, acc.: 0.00%] [G loss: 0.007758] [C loss: 0.695868]\n",
      "3917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005754] [C loss: 0.695868]\n",
      "3918 [D loss: 0.695870, acc.: 0.00%] [G loss: 0.005761] [C loss: 0.695865]\n",
      "3919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004969] [C loss: 0.695865]\n",
      "3920 [D loss: 0.695868, acc.: 0.00%] [G loss: 0.005295] [C loss: 0.695863]\n",
      "3921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003897] [C loss: 0.695863]\n",
      "3922 [D loss: 0.695867, acc.: 0.00%] [G loss: 0.006994] [C loss: 0.695862]\n",
      "3923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004459] [C loss: 0.695862]\n",
      "3924 [D loss: 0.695866, acc.: 0.00%] [G loss: 0.004532] [C loss: 0.695861]\n",
      "3925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005471] [C loss: 0.695861]\n",
      "3926 [D loss: 0.695864, acc.: 0.00%] [G loss: 0.003618] [C loss: 0.695859]\n",
      "3927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005249] [C loss: 0.695859]\n",
      "3928 [D loss: 0.695863, acc.: 0.00%] [G loss: 0.004433] [C loss: 0.695858]\n",
      "3929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005974] [C loss: 0.695858]\n",
      "3930 [D loss: 0.695861, acc.: 0.00%] [G loss: 0.008583] [C loss: 0.695857]\n",
      "3931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005563] [C loss: 0.695857]\n",
      "3932 [D loss: 0.695860, acc.: 0.00%] [G loss: 0.003754] [C loss: 0.695857]\n",
      "3933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008627] [C loss: 0.695857]\n",
      "3934 [D loss: 0.695859, acc.: 0.00%] [G loss: 0.004367] [C loss: 0.695855]\n",
      "3935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005164] [C loss: 0.695855]\n",
      "3936 [D loss: 0.695858, acc.: 0.00%] [G loss: 0.005764] [C loss: 0.695854]\n",
      "3937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007889] [C loss: 0.695854]\n",
      "3938 [D loss: 0.695856, acc.: 0.00%] [G loss: 0.003794] [C loss: 0.695852]\n",
      "3939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005602] [C loss: 0.695852]\n",
      "3940 [D loss: 0.695854, acc.: 0.00%] [G loss: 0.006164] [C loss: 0.695851]\n",
      "3941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005227] [C loss: 0.695851]\n",
      "3942 [D loss: 0.695854, acc.: 0.00%] [G loss: 0.012362] [C loss: 0.695849]\n",
      "3943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003291] [C loss: 0.695849]\n",
      "3944 [D loss: 0.695851, acc.: 0.00%] [G loss: 0.005351] [C loss: 0.695848]\n",
      "3945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006123] [C loss: 0.695848]\n",
      "3946 [D loss: 0.695851, acc.: 0.00%] [G loss: 0.006478] [C loss: 0.695847]\n",
      "3947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004078] [C loss: 0.695847]\n",
      "3948 [D loss: 0.695850, acc.: 0.00%] [G loss: 0.008468] [C loss: 0.695845]\n",
      "3949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005053] [C loss: 0.695845]\n",
      "3950 [D loss: 0.695848, acc.: 0.00%] [G loss: 0.010616] [C loss: 0.695845]\n",
      "3951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005121] [C loss: 0.695845]\n",
      "3952 [D loss: 0.695847, acc.: 0.00%] [G loss: 0.008220] [C loss: 0.695844]\n",
      "3953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006135] [C loss: 0.695844]\n",
      "3954 [D loss: 0.695845, acc.: 0.00%] [G loss: 0.005457] [C loss: 0.695841]\n",
      "3955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009740] [C loss: 0.695841]\n",
      "3956 [D loss: 0.695844, acc.: 0.00%] [G loss: 0.007378] [C loss: 0.695840]\n",
      "3957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008025] [C loss: 0.695840]\n",
      "3958 [D loss: 0.695842, acc.: 0.00%] [G loss: 0.010465] [C loss: 0.695838]\n",
      "3959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009100] [C loss: 0.695838]\n",
      "3960 [D loss: 0.695840, acc.: 0.00%] [G loss: 0.004349] [C loss: 0.695837]\n",
      "3961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005755] [C loss: 0.695837]\n",
      "3962 [D loss: 0.695839, acc.: 0.00%] [G loss: 0.004439] [C loss: 0.695836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.695836]\n",
      "3964 [D loss: 0.695839, acc.: 0.00%] [G loss: 0.004704] [C loss: 0.695834]\n",
      "3965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004453] [C loss: 0.695834]\n",
      "3966 [D loss: 0.695837, acc.: 0.00%] [G loss: 0.004521] [C loss: 0.695834]\n",
      "3967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005424] [C loss: 0.695834]\n",
      "3968 [D loss: 0.695836, acc.: 0.00%] [G loss: 0.006795] [C loss: 0.695830]\n",
      "3969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005447] [C loss: 0.695830]\n",
      "3970 [D loss: 0.695834, acc.: 0.00%] [G loss: 0.010329] [C loss: 0.695831]\n",
      "3971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007590] [C loss: 0.695831]\n",
      "3972 [D loss: 0.695834, acc.: 0.00%] [G loss: 0.006524] [C loss: 0.695828]\n",
      "3973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005739] [C loss: 0.695828]\n",
      "3974 [D loss: 0.695832, acc.: 0.00%] [G loss: 0.005813] [C loss: 0.695827]\n",
      "3975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006966] [C loss: 0.695827]\n",
      "3976 [D loss: 0.695832, acc.: 0.00%] [G loss: 0.008388] [C loss: 0.695827]\n",
      "3977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006458] [C loss: 0.695827]\n",
      "3978 [D loss: 0.695830, acc.: 0.00%] [G loss: 0.006630] [C loss: 0.695826]\n",
      "3979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008825] [C loss: 0.695826]\n",
      "3980 [D loss: 0.695828, acc.: 0.00%] [G loss: 0.006007] [C loss: 0.695823]\n",
      "3981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006476] [C loss: 0.695823]\n",
      "3982 [D loss: 0.695826, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.695823]\n",
      "3983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004294] [C loss: 0.695823]\n",
      "3984 [D loss: 0.695825, acc.: 0.00%] [G loss: 0.005509] [C loss: 0.695820]\n",
      "3985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004411] [C loss: 0.695820]\n",
      "3986 [D loss: 0.695823, acc.: 0.00%] [G loss: 0.006119] [C loss: 0.695820]\n",
      "3987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006002] [C loss: 0.695820]\n",
      "3988 [D loss: 0.695822, acc.: 0.00%] [G loss: 0.008649] [C loss: 0.695817]\n",
      "3989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005719] [C loss: 0.695817]\n",
      "3990 [D loss: 0.695820, acc.: 0.00%] [G loss: 0.004355] [C loss: 0.695817]\n",
      "3991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008142] [C loss: 0.695817]\n",
      "3992 [D loss: 0.695820, acc.: 0.00%] [G loss: 0.007330] [C loss: 0.695814]\n",
      "3993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004581] [C loss: 0.695814]\n",
      "3994 [D loss: 0.695817, acc.: 0.00%] [G loss: 0.009279] [C loss: 0.695815]\n",
      "3995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007452] [C loss: 0.695815]\n",
      "3996 [D loss: 0.695817, acc.: 0.00%] [G loss: 0.005347] [C loss: 0.695814]\n",
      "3997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021008] [C loss: 0.695814]\n",
      "3998 [D loss: 0.695815, acc.: 0.00%] [G loss: 0.006643] [C loss: 0.695810]\n",
      "3999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009573] [C loss: 0.695810]\n",
      "4000 [D loss: 0.695813, acc.: 0.00%] [G loss: 0.007534] [C loss: 0.695809]\n",
      "4001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005902] [C loss: 0.695809]\n",
      "4002 [D loss: 0.695812, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.695809]\n",
      "4003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009251] [C loss: 0.695809]\n",
      "4004 [D loss: 0.695811, acc.: 0.00%] [G loss: 0.004300] [C loss: 0.695808]\n",
      "4005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005314] [C loss: 0.695808]\n",
      "4006 [D loss: 0.695810, acc.: 0.00%] [G loss: 0.006384] [C loss: 0.695805]\n",
      "4007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004739] [C loss: 0.695805]\n",
      "4008 [D loss: 0.695807, acc.: 0.00%] [G loss: 0.006862] [C loss: 0.695803]\n",
      "4009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008397] [C loss: 0.695803]\n",
      "4010 [D loss: 0.695807, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.695803]\n",
      "4011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005584] [C loss: 0.695803]\n",
      "4012 [D loss: 0.695806, acc.: 0.00%] [G loss: 0.007577] [C loss: 0.695801]\n",
      "4013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006393] [C loss: 0.695801]\n",
      "4014 [D loss: 0.695805, acc.: 0.00%] [G loss: 0.006737] [C loss: 0.695800]\n",
      "4015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.024062] [C loss: 0.695800]\n",
      "4016 [D loss: 0.695802, acc.: 0.00%] [G loss: 0.005746] [C loss: 0.695799]\n",
      "4017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005126] [C loss: 0.695799]\n",
      "4018 [D loss: 0.695802, acc.: 0.00%] [G loss: 0.004927] [C loss: 0.695797]\n",
      "4019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007369] [C loss: 0.695797]\n",
      "4020 [D loss: 0.695804, acc.: 6.25%] [G loss: 0.005037] [C loss: 0.695795]\n",
      "4021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006256] [C loss: 0.695795]\n",
      "4022 [D loss: 0.695798, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.695795]\n",
      "4023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006844] [C loss: 0.695795]\n",
      "4024 [D loss: 0.695797, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.695793]\n",
      "4025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006676] [C loss: 0.695793]\n",
      "4026 [D loss: 0.695796, acc.: 0.00%] [G loss: 0.005093] [C loss: 0.695792]\n",
      "4027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005891] [C loss: 0.695792]\n",
      "4028 [D loss: 0.695794, acc.: 0.00%] [G loss: 0.005161] [C loss: 0.695791]\n",
      "4029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009466] [C loss: 0.695791]\n",
      "4030 [D loss: 0.695793, acc.: 0.00%] [G loss: 0.006706] [C loss: 0.695789]\n",
      "4031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005575] [C loss: 0.695789]\n",
      "4032 [D loss: 0.695791, acc.: 0.00%] [G loss: 0.005518] [C loss: 0.695787]\n",
      "4033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.695787]\n",
      "4034 [D loss: 0.695790, acc.: 0.00%] [G loss: 0.005412] [C loss: 0.695787]\n",
      "4035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006038] [C loss: 0.695787]\n",
      "4036 [D loss: 0.695790, acc.: 0.00%] [G loss: 0.006112] [C loss: 0.695784]\n",
      "4037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012353] [C loss: 0.695784]\n",
      "4038 [D loss: 0.695787, acc.: 0.00%] [G loss: 0.007400] [C loss: 0.695783]\n",
      "4039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005833] [C loss: 0.695783]\n",
      "4040 [D loss: 0.695786, acc.: 0.00%] [G loss: 0.006791] [C loss: 0.695781]\n",
      "4041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006930] [C loss: 0.695781]\n",
      "4042 [D loss: 0.695785, acc.: 0.00%] [G loss: 0.005973] [C loss: 0.695781]\n",
      "4043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008119] [C loss: 0.695781]\n",
      "4044 [D loss: 0.695784, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.695778]\n",
      "4045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004821] [C loss: 0.695778]\n",
      "4046 [D loss: 0.695782, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.695778]\n",
      "4047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004894] [C loss: 0.695778]\n",
      "4048 [D loss: 0.695781, acc.: 0.00%] [G loss: 0.005008] [C loss: 0.695777]\n",
      "4049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007266] [C loss: 0.695777]\n",
      "4050 [D loss: 0.695779, acc.: 0.00%] [G loss: 0.005918] [C loss: 0.695774]\n",
      "4051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007489] [C loss: 0.695774]\n",
      "4052 [D loss: 0.695777, acc.: 0.00%] [G loss: 0.004865] [C loss: 0.695773]\n",
      "4053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004842] [C loss: 0.695773]\n",
      "4054 [D loss: 0.695777, acc.: 0.00%] [G loss: 0.005958] [C loss: 0.695773]\n",
      "4055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007491] [C loss: 0.695773]\n",
      "4056 [D loss: 0.695776, acc.: 0.00%] [G loss: 0.004749] [C loss: 0.695772]\n",
      "4057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006018] [C loss: 0.695772]\n",
      "4058 [D loss: 0.695774, acc.: 0.00%] [G loss: 0.003794] [C loss: 0.695769]\n",
      "4059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007439] [C loss: 0.695769]\n",
      "4060 [D loss: 0.695772, acc.: 0.00%] [G loss: 0.004170] [C loss: 0.695770]\n",
      "4061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006650] [C loss: 0.695770]\n",
      "4062 [D loss: 0.695773, acc.: 0.00%] [G loss: 0.009684] [C loss: 0.695767]\n",
      "4063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005340] [C loss: 0.695767]\n",
      "4064 [D loss: 0.695770, acc.: 0.00%] [G loss: 0.006965] [C loss: 0.695767]\n",
      "4065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006085] [C loss: 0.695767]\n",
      "4066 [D loss: 0.695769, acc.: 0.00%] [G loss: 0.008762] [C loss: 0.695764]\n",
      "4067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004885] [C loss: 0.695764]\n",
      "4068 [D loss: 0.695767, acc.: 0.00%] [G loss: 0.013690] [C loss: 0.695764]\n",
      "4069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005560] [C loss: 0.695764]\n",
      "4070 [D loss: 0.695765, acc.: 0.00%] [G loss: 0.004836] [C loss: 0.695762]\n",
      "4071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006576] [C loss: 0.695762]\n",
      "4072 [D loss: 0.695765, acc.: 0.00%] [G loss: 0.005402] [C loss: 0.695762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005549] [C loss: 0.695762]\n",
      "4074 [D loss: 0.695764, acc.: 0.00%] [G loss: 0.005250] [C loss: 0.695758]\n",
      "4075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004002] [C loss: 0.695758]\n",
      "4076 [D loss: 0.695761, acc.: 0.00%] [G loss: 0.009581] [C loss: 0.695758]\n",
      "4077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004903] [C loss: 0.695758]\n",
      "4078 [D loss: 0.695760, acc.: 0.00%] [G loss: 0.005010] [C loss: 0.695754]\n",
      "4079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006222] [C loss: 0.695754]\n",
      "4080 [D loss: 0.695758, acc.: 0.00%] [G loss: 0.005110] [C loss: 0.695754]\n",
      "4081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006645] [C loss: 0.695754]\n",
      "4082 [D loss: 0.695758, acc.: 0.00%] [G loss: 0.005237] [C loss: 0.695753]\n",
      "4083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004953] [C loss: 0.695753]\n",
      "4084 [D loss: 0.695756, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.695752]\n",
      "4085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005894] [C loss: 0.695752]\n",
      "4086 [D loss: 0.695755, acc.: 0.00%] [G loss: 0.005096] [C loss: 0.695749]\n",
      "4087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005586] [C loss: 0.695749]\n",
      "4088 [D loss: 0.695753, acc.: 0.00%] [G loss: 0.005775] [C loss: 0.695749]\n",
      "4089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003729] [C loss: 0.695749]\n",
      "4090 [D loss: 0.695752, acc.: 0.00%] [G loss: 0.007366] [C loss: 0.695748]\n",
      "4091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004425] [C loss: 0.695748]\n",
      "4092 [D loss: 0.695751, acc.: 0.00%] [G loss: 0.004478] [C loss: 0.695745]\n",
      "4093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006028] [C loss: 0.695745]\n",
      "4094 [D loss: 0.695748, acc.: 0.00%] [G loss: 0.010909] [C loss: 0.695744]\n",
      "4095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012039] [C loss: 0.695744]\n",
      "4096 [D loss: 0.695746, acc.: 0.00%] [G loss: 0.005573] [C loss: 0.695745]\n",
      "4097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007573] [C loss: 0.695745]\n",
      "4098 [D loss: 0.695746, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.695740]\n",
      "4099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004686] [C loss: 0.695740]\n",
      "4100 [D loss: 0.695744, acc.: 0.00%] [G loss: 0.006703] [C loss: 0.695741]\n",
      "4101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008212] [C loss: 0.695741]\n",
      "4102 [D loss: 0.695744, acc.: 0.00%] [G loss: 0.006570] [C loss: 0.695739]\n",
      "4103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004799] [C loss: 0.695739]\n",
      "4104 [D loss: 0.695742, acc.: 0.00%] [G loss: 0.006064] [C loss: 0.695740]\n",
      "4105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006055] [C loss: 0.695740]\n",
      "4106 [D loss: 0.695742, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.695735]\n",
      "4107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006130] [C loss: 0.695735]\n",
      "4108 [D loss: 0.695738, acc.: 0.00%] [G loss: 0.011145] [C loss: 0.695734]\n",
      "4109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010131] [C loss: 0.695734]\n",
      "4110 [D loss: 0.695737, acc.: 0.00%] [G loss: 0.006856] [C loss: 0.695735]\n",
      "4111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.695735]\n",
      "4112 [D loss: 0.695737, acc.: 0.00%] [G loss: 0.006840] [C loss: 0.695733]\n",
      "4113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006147] [C loss: 0.695733]\n",
      "4114 [D loss: 0.695735, acc.: 0.00%] [G loss: 0.006944] [C loss: 0.695731]\n",
      "4115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006779] [C loss: 0.695731]\n",
      "4116 [D loss: 0.695735, acc.: 0.00%] [G loss: 0.006021] [C loss: 0.695731]\n",
      "4117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005407] [C loss: 0.695731]\n",
      "4118 [D loss: 0.695733, acc.: 0.00%] [G loss: 0.007911] [C loss: 0.695729]\n",
      "4119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006079] [C loss: 0.695729]\n",
      "4120 [D loss: 0.695731, acc.: 0.00%] [G loss: 0.005623] [C loss: 0.695728]\n",
      "4121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.695728]\n",
      "4122 [D loss: 0.695730, acc.: 0.00%] [G loss: 0.007682] [C loss: 0.695726]\n",
      "4123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004671] [C loss: 0.695726]\n",
      "4124 [D loss: 0.695729, acc.: 0.00%] [G loss: 0.005731] [C loss: 0.695724]\n",
      "4125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007730] [C loss: 0.695724]\n",
      "4126 [D loss: 0.695727, acc.: 0.00%] [G loss: 0.004198] [C loss: 0.695722]\n",
      "4127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003936] [C loss: 0.695722]\n",
      "4128 [D loss: 0.695725, acc.: 0.00%] [G loss: 0.006840] [C loss: 0.695722]\n",
      "4129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005808] [C loss: 0.695722]\n",
      "4130 [D loss: 0.695725, acc.: 0.00%] [G loss: 0.006849] [C loss: 0.695720]\n",
      "4131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007758] [C loss: 0.695720]\n",
      "4132 [D loss: 0.695723, acc.: 0.00%] [G loss: 0.007509] [C loss: 0.695719]\n",
      "4133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006150] [C loss: 0.695719]\n",
      "4134 [D loss: 0.695722, acc.: 0.00%] [G loss: 0.006302] [C loss: 0.695717]\n",
      "4135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016572] [C loss: 0.695717]\n",
      "4136 [D loss: 0.695721, acc.: 0.00%] [G loss: 0.005107] [C loss: 0.695714]\n",
      "4137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006559] [C loss: 0.695714]\n",
      "4138 [D loss: 0.695718, acc.: 0.00%] [G loss: 0.004153] [C loss: 0.695716]\n",
      "4139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007203] [C loss: 0.695716]\n",
      "4140 [D loss: 0.695719, acc.: 0.00%] [G loss: 0.004824] [C loss: 0.695715]\n",
      "4141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008089] [C loss: 0.695715]\n",
      "4142 [D loss: 0.695717, acc.: 0.00%] [G loss: 0.005132] [C loss: 0.695711]\n",
      "4143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.695711]\n",
      "4144 [D loss: 0.695714, acc.: 0.00%] [G loss: 0.005956] [C loss: 0.695709]\n",
      "4145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007573] [C loss: 0.695709]\n",
      "4146 [D loss: 0.695713, acc.: 0.00%] [G loss: 0.005549] [C loss: 0.695708]\n",
      "4147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004911] [C loss: 0.695708]\n",
      "4148 [D loss: 0.695711, acc.: 0.00%] [G loss: 0.005259] [C loss: 0.695706]\n",
      "4149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005616] [C loss: 0.695706]\n",
      "4150 [D loss: 0.695709, acc.: 0.00%] [G loss: 0.006070] [C loss: 0.695706]\n",
      "4151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012102] [C loss: 0.695706]\n",
      "4152 [D loss: 0.695709, acc.: 0.00%] [G loss: 0.006906] [C loss: 0.695705]\n",
      "4153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005859] [C loss: 0.695705]\n",
      "4154 [D loss: 0.695707, acc.: 0.00%] [G loss: 0.004488] [C loss: 0.695702]\n",
      "4155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005162] [C loss: 0.695702]\n",
      "4156 [D loss: 0.695706, acc.: 0.00%] [G loss: 0.004699] [C loss: 0.695702]\n",
      "4157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008832] [C loss: 0.695702]\n",
      "4158 [D loss: 0.695706, acc.: 0.00%] [G loss: 0.006901] [C loss: 0.695699]\n",
      "4159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004672] [C loss: 0.695699]\n",
      "4160 [D loss: 0.695702, acc.: 0.00%] [G loss: 0.005932] [C loss: 0.695700]\n",
      "4161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007141] [C loss: 0.695700]\n",
      "4162 [D loss: 0.695702, acc.: 0.00%] [G loss: 0.009121] [C loss: 0.695698]\n",
      "4163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004921] [C loss: 0.695698]\n",
      "4164 [D loss: 0.695700, acc.: 0.00%] [G loss: 0.004483] [C loss: 0.695695]\n",
      "4165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004460] [C loss: 0.695695]\n",
      "4166 [D loss: 0.695699, acc.: 0.00%] [G loss: 0.007512] [C loss: 0.695696]\n",
      "4167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005131] [C loss: 0.695696]\n",
      "4168 [D loss: 0.695697, acc.: 0.00%] [G loss: 0.005924] [C loss: 0.695693]\n",
      "4169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006189] [C loss: 0.695693]\n",
      "4170 [D loss: 0.695696, acc.: 0.00%] [G loss: 0.004898] [C loss: 0.695692]\n",
      "4171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006912] [C loss: 0.695692]\n",
      "4172 [D loss: 0.695695, acc.: 0.00%] [G loss: 0.005121] [C loss: 0.695690]\n",
      "4173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006919] [C loss: 0.695690]\n",
      "4174 [D loss: 0.695693, acc.: 0.00%] [G loss: 0.006295] [C loss: 0.695689]\n",
      "4175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004540] [C loss: 0.695689]\n",
      "4176 [D loss: 0.695692, acc.: 0.00%] [G loss: 0.004430] [C loss: 0.695689]\n",
      "4177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004974] [C loss: 0.695689]\n",
      "4178 [D loss: 0.695692, acc.: 0.00%] [G loss: 0.004505] [C loss: 0.695686]\n",
      "4179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005351] [C loss: 0.695686]\n",
      "4180 [D loss: 0.695690, acc.: 0.00%] [G loss: 0.003750] [C loss: 0.695687]\n",
      "4181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007223] [C loss: 0.695687]\n",
      "4182 [D loss: 0.695688, acc.: 0.00%] [G loss: 0.003985] [C loss: 0.695683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006798] [C loss: 0.695683]\n",
      "4184 [D loss: 0.695686, acc.: 0.00%] [G loss: 0.005987] [C loss: 0.695682]\n",
      "4185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005622] [C loss: 0.695682]\n",
      "4186 [D loss: 0.695686, acc.: 0.00%] [G loss: 0.006159] [C loss: 0.695683]\n",
      "4187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008367] [C loss: 0.695683]\n",
      "4188 [D loss: 0.695685, acc.: 0.00%] [G loss: 0.006058] [C loss: 0.695680]\n",
      "4189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008060] [C loss: 0.695680]\n",
      "4190 [D loss: 0.695683, acc.: 0.00%] [G loss: 0.005760] [C loss: 0.695679]\n",
      "4191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008770] [C loss: 0.695679]\n",
      "4192 [D loss: 0.695681, acc.: 0.00%] [G loss: 0.005112] [C loss: 0.695677]\n",
      "4193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007848] [C loss: 0.695677]\n",
      "4194 [D loss: 0.695680, acc.: 0.00%] [G loss: 0.006387] [C loss: 0.695676]\n",
      "4195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007170] [C loss: 0.695676]\n",
      "4196 [D loss: 0.695678, acc.: 0.00%] [G loss: 0.005477] [C loss: 0.695674]\n",
      "4197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006160] [C loss: 0.695674]\n",
      "4198 [D loss: 0.695677, acc.: 0.00%] [G loss: 0.006284] [C loss: 0.695671]\n",
      "4199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005787] [C loss: 0.695671]\n",
      "4200 [D loss: 0.695675, acc.: 0.00%] [G loss: 0.004307] [C loss: 0.695671]\n",
      "4201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005323] [C loss: 0.695671]\n",
      "4202 [D loss: 0.695674, acc.: 0.00%] [G loss: 0.006271] [C loss: 0.695671]\n",
      "4203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005059] [C loss: 0.695671]\n",
      "4204 [D loss: 0.695673, acc.: 0.00%] [G loss: 0.005557] [C loss: 0.695670]\n",
      "4205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004999] [C loss: 0.695670]\n",
      "4206 [D loss: 0.695673, acc.: 0.00%] [G loss: 0.005039] [C loss: 0.695668]\n",
      "4207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005819] [C loss: 0.695668]\n",
      "4208 [D loss: 0.695671, acc.: 0.00%] [G loss: 0.006802] [C loss: 0.695666]\n",
      "4209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.695666]\n",
      "4210 [D loss: 0.695669, acc.: 0.00%] [G loss: 0.009851] [C loss: 0.695664]\n",
      "4211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005177] [C loss: 0.695664]\n",
      "4212 [D loss: 0.695667, acc.: 0.00%] [G loss: 0.005142] [C loss: 0.695661]\n",
      "4213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007581] [C loss: 0.695661]\n",
      "4214 [D loss: 0.695666, acc.: 0.00%] [G loss: 0.005381] [C loss: 0.695660]\n",
      "4215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005467] [C loss: 0.695660]\n",
      "4216 [D loss: 0.695664, acc.: 0.00%] [G loss: 0.006903] [C loss: 0.695659]\n",
      "4217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005523] [C loss: 0.695659]\n",
      "4218 [D loss: 0.695662, acc.: 0.00%] [G loss: 0.005622] [C loss: 0.695657]\n",
      "4219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010412] [C loss: 0.695657]\n",
      "4220 [D loss: 0.695660, acc.: 0.00%] [G loss: 0.003768] [C loss: 0.695656]\n",
      "4221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005285] [C loss: 0.695656]\n",
      "4222 [D loss: 0.695660, acc.: 0.00%] [G loss: 0.008302] [C loss: 0.695655]\n",
      "4223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005030] [C loss: 0.695655]\n",
      "4224 [D loss: 0.695658, acc.: 0.00%] [G loss: 0.007263] [C loss: 0.695655]\n",
      "4225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007927] [C loss: 0.695655]\n",
      "4226 [D loss: 0.695657, acc.: 0.00%] [G loss: 0.005262] [C loss: 0.695653]\n",
      "4227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.695653]\n",
      "4228 [D loss: 0.695656, acc.: 0.00%] [G loss: 0.018555] [C loss: 0.695652]\n",
      "4229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005354] [C loss: 0.695652]\n",
      "4230 [D loss: 0.695655, acc.: 0.00%] [G loss: 0.006206] [C loss: 0.695650]\n",
      "4231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007854] [C loss: 0.695650]\n",
      "4232 [D loss: 0.695653, acc.: 0.00%] [G loss: 0.004702] [C loss: 0.695648]\n",
      "4233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004675] [C loss: 0.695648]\n",
      "4234 [D loss: 0.695651, acc.: 0.00%] [G loss: 0.007301] [C loss: 0.695646]\n",
      "4235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007525] [C loss: 0.695646]\n",
      "4236 [D loss: 0.695650, acc.: 0.00%] [G loss: 0.006953] [C loss: 0.695646]\n",
      "4237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007184] [C loss: 0.695646]\n",
      "4238 [D loss: 0.695649, acc.: 0.00%] [G loss: 0.005974] [C loss: 0.695644]\n",
      "4239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004917] [C loss: 0.695644]\n",
      "4240 [D loss: 0.695647, acc.: 0.00%] [G loss: 0.004815] [C loss: 0.695643]\n",
      "4241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005407] [C loss: 0.695643]\n",
      "4242 [D loss: 0.695646, acc.: 0.00%] [G loss: 0.006152] [C loss: 0.695641]\n",
      "4243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006470] [C loss: 0.695641]\n",
      "4244 [D loss: 0.695644, acc.: 0.00%] [G loss: 0.006675] [C loss: 0.695639]\n",
      "4245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007915] [C loss: 0.695639]\n",
      "4246 [D loss: 0.695643, acc.: 0.00%] [G loss: 0.004849] [C loss: 0.695639]\n",
      "4247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005908] [C loss: 0.695639]\n",
      "4248 [D loss: 0.695641, acc.: 0.00%] [G loss: 0.006758] [C loss: 0.695637]\n",
      "4249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008566] [C loss: 0.695637]\n",
      "4250 [D loss: 0.695640, acc.: 0.00%] [G loss: 0.006651] [C loss: 0.695636]\n",
      "4251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022793] [C loss: 0.695636]\n",
      "4252 [D loss: 0.695639, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.695635]\n",
      "4253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005521] [C loss: 0.695635]\n",
      "4254 [D loss: 0.695637, acc.: 0.00%] [G loss: 0.006092] [C loss: 0.695632]\n",
      "4255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004131] [C loss: 0.695632]\n",
      "4256 [D loss: 0.695635, acc.: 0.00%] [G loss: 0.007908] [C loss: 0.695632]\n",
      "4257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005725] [C loss: 0.695632]\n",
      "4258 [D loss: 0.695635, acc.: 0.00%] [G loss: 0.006831] [C loss: 0.695630]\n",
      "4259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006465] [C loss: 0.695630]\n",
      "4260 [D loss: 0.695633, acc.: 0.00%] [G loss: 0.007859] [C loss: 0.695629]\n",
      "4261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004886] [C loss: 0.695629]\n",
      "4262 [D loss: 0.695631, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.695628]\n",
      "4263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007438] [C loss: 0.695628]\n",
      "4264 [D loss: 0.695631, acc.: 0.00%] [G loss: 0.005235] [C loss: 0.695626]\n",
      "4265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006367] [C loss: 0.695626]\n",
      "4266 [D loss: 0.695629, acc.: 0.00%] [G loss: 0.006060] [C loss: 0.695625]\n",
      "4267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.695625]\n",
      "4268 [D loss: 0.695627, acc.: 0.00%] [G loss: 0.005804] [C loss: 0.695625]\n",
      "4269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006163] [C loss: 0.695625]\n",
      "4270 [D loss: 0.695627, acc.: 0.00%] [G loss: 0.004633] [C loss: 0.695620]\n",
      "4271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005821] [C loss: 0.695620]\n",
      "4272 [D loss: 0.695623, acc.: 0.00%] [G loss: 0.005967] [C loss: 0.695620]\n",
      "4273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009222] [C loss: 0.695620]\n",
      "4274 [D loss: 0.695623, acc.: 0.00%] [G loss: 0.006240] [C loss: 0.695620]\n",
      "4275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006939] [C loss: 0.695620]\n",
      "4276 [D loss: 0.695622, acc.: 0.00%] [G loss: 0.003660] [C loss: 0.695615]\n",
      "4277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011334] [C loss: 0.695615]\n",
      "4278 [D loss: 0.695619, acc.: 0.00%] [G loss: 0.008864] [C loss: 0.695618]\n",
      "4279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007299] [C loss: 0.695618]\n",
      "4280 [D loss: 0.695619, acc.: 0.00%] [G loss: 0.004901] [C loss: 0.695614]\n",
      "4281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005372] [C loss: 0.695614]\n",
      "4282 [D loss: 0.695617, acc.: 0.00%] [G loss: 0.005514] [C loss: 0.695612]\n",
      "4283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006478] [C loss: 0.695612]\n",
      "4284 [D loss: 0.695616, acc.: 0.00%] [G loss: 0.005888] [C loss: 0.695612]\n",
      "4285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.695612]\n",
      "4286 [D loss: 0.695615, acc.: 0.00%] [G loss: 0.005601] [C loss: 0.695609]\n",
      "4287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005103] [C loss: 0.695609]\n",
      "4288 [D loss: 0.695612, acc.: 0.00%] [G loss: 0.003859] [C loss: 0.695608]\n",
      "4289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005600] [C loss: 0.695608]\n",
      "4290 [D loss: 0.695612, acc.: 0.00%] [G loss: 0.004787] [C loss: 0.695605]\n",
      "4291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004984] [C loss: 0.695605]\n",
      "4292 [D loss: 0.695609, acc.: 0.00%] [G loss: 0.005157] [C loss: 0.695606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004503] [C loss: 0.695606]\n",
      "4294 [D loss: 0.695608, acc.: 0.00%] [G loss: 0.004952] [C loss: 0.695603]\n",
      "4295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004553] [C loss: 0.695603]\n",
      "4296 [D loss: 0.695606, acc.: 0.00%] [G loss: 0.005253] [C loss: 0.695601]\n",
      "4297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.695601]\n",
      "4298 [D loss: 0.695605, acc.: 0.00%] [G loss: 0.006455] [C loss: 0.695602]\n",
      "4299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004198] [C loss: 0.695602]\n",
      "4300 [D loss: 0.695605, acc.: 0.00%] [G loss: 0.004060] [C loss: 0.695600]\n",
      "4301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006412] [C loss: 0.695600]\n",
      "4302 [D loss: 0.695603, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.695599]\n",
      "4303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.695599]\n",
      "4304 [D loss: 0.695602, acc.: 0.00%] [G loss: 0.007043] [C loss: 0.695597]\n",
      "4305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004193] [C loss: 0.695597]\n",
      "4306 [D loss: 0.695600, acc.: 0.00%] [G loss: 0.006574] [C loss: 0.695595]\n",
      "4307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006328] [C loss: 0.695595]\n",
      "4308 [D loss: 0.695598, acc.: 0.00%] [G loss: 0.004598] [C loss: 0.695593]\n",
      "4309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005060] [C loss: 0.695593]\n",
      "4310 [D loss: 0.695597, acc.: 0.00%] [G loss: 0.005857] [C loss: 0.695592]\n",
      "4311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005742] [C loss: 0.695592]\n",
      "4312 [D loss: 0.695595, acc.: 0.00%] [G loss: 0.005192] [C loss: 0.695591]\n",
      "4313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004047] [C loss: 0.695591]\n",
      "4314 [D loss: 0.695595, acc.: 0.00%] [G loss: 0.003423] [C loss: 0.695589]\n",
      "4315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003656] [C loss: 0.695589]\n",
      "4316 [D loss: 0.695593, acc.: 0.00%] [G loss: 0.004992] [C loss: 0.695588]\n",
      "4317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005312] [C loss: 0.695588]\n",
      "4318 [D loss: 0.695591, acc.: 0.00%] [G loss: 0.006679] [C loss: 0.695587]\n",
      "4319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006783] [C loss: 0.695587]\n",
      "4320 [D loss: 0.695590, acc.: 0.00%] [G loss: 0.008138] [C loss: 0.695587]\n",
      "4321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005519] [C loss: 0.695587]\n",
      "4322 [D loss: 0.695589, acc.: 0.00%] [G loss: 0.004388] [C loss: 0.695583]\n",
      "4323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020386] [C loss: 0.695583]\n",
      "4324 [D loss: 0.695586, acc.: 0.00%] [G loss: 0.018288] [C loss: 0.695585]\n",
      "4325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005320] [C loss: 0.695585]\n",
      "4326 [D loss: 0.695586, acc.: 0.00%] [G loss: 0.005864] [C loss: 0.695581]\n",
      "4327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004256] [C loss: 0.695581]\n",
      "4328 [D loss: 0.695584, acc.: 0.00%] [G loss: 0.005712] [C loss: 0.695580]\n",
      "4329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004963] [C loss: 0.695580]\n",
      "4330 [D loss: 0.695582, acc.: 0.00%] [G loss: 0.004569] [C loss: 0.695579]\n",
      "4331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004952] [C loss: 0.695579]\n",
      "4332 [D loss: 0.695582, acc.: 0.00%] [G loss: 0.007406] [C loss: 0.695577]\n",
      "4333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005957] [C loss: 0.695577]\n",
      "4334 [D loss: 0.695581, acc.: 0.00%] [G loss: 0.006049] [C loss: 0.695576]\n",
      "4335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007158] [C loss: 0.695576]\n",
      "4336 [D loss: 0.695578, acc.: 0.00%] [G loss: 0.007386] [C loss: 0.695574]\n",
      "4337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004772] [C loss: 0.695574]\n",
      "4338 [D loss: 0.695577, acc.: 0.00%] [G loss: 0.004905] [C loss: 0.695573]\n",
      "4339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005728] [C loss: 0.695573]\n",
      "4340 [D loss: 0.695576, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.695573]\n",
      "4341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006565] [C loss: 0.695573]\n",
      "4342 [D loss: 0.695575, acc.: 0.00%] [G loss: 0.004557] [C loss: 0.695570]\n",
      "4343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005774] [C loss: 0.695570]\n",
      "4344 [D loss: 0.695573, acc.: 0.00%] [G loss: 0.007969] [C loss: 0.695569]\n",
      "4345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.695569]\n",
      "4346 [D loss: 0.695572, acc.: 0.00%] [G loss: 0.004901] [C loss: 0.695567]\n",
      "4347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005340] [C loss: 0.695567]\n",
      "4348 [D loss: 0.695570, acc.: 0.00%] [G loss: 0.004613] [C loss: 0.695568]\n",
      "4349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009233] [C loss: 0.695568]\n",
      "4350 [D loss: 0.695570, acc.: 0.00%] [G loss: 0.006308] [C loss: 0.695565]\n",
      "4351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008324] [C loss: 0.695565]\n",
      "4352 [D loss: 0.695567, acc.: 0.00%] [G loss: 0.006089] [C loss: 0.695562]\n",
      "4353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008396] [C loss: 0.695562]\n",
      "4354 [D loss: 0.695566, acc.: 0.00%] [G loss: 0.006501] [C loss: 0.695562]\n",
      "4355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004677] [C loss: 0.695562]\n",
      "4356 [D loss: 0.695565, acc.: 0.00%] [G loss: 0.007610] [C loss: 0.695560]\n",
      "4357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005296] [C loss: 0.695560]\n",
      "4358 [D loss: 0.695563, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.695558]\n",
      "4359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006957] [C loss: 0.695558]\n",
      "4360 [D loss: 0.695562, acc.: 0.00%] [G loss: 0.007657] [C loss: 0.695556]\n",
      "4361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005770] [C loss: 0.695556]\n",
      "4362 [D loss: 0.695560, acc.: 0.00%] [G loss: 0.005024] [C loss: 0.695557]\n",
      "4363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005076] [C loss: 0.695557]\n",
      "4364 [D loss: 0.695560, acc.: 0.00%] [G loss: 0.004434] [C loss: 0.695555]\n",
      "4365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006571] [C loss: 0.695555]\n",
      "4366 [D loss: 0.695559, acc.: 0.00%] [G loss: 0.006948] [C loss: 0.695553]\n",
      "4367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006043] [C loss: 0.695553]\n",
      "4368 [D loss: 0.695556, acc.: 0.00%] [G loss: 0.004675] [C loss: 0.695551]\n",
      "4369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006859] [C loss: 0.695551]\n",
      "4370 [D loss: 0.695554, acc.: 0.00%] [G loss: 0.004309] [C loss: 0.695548]\n",
      "4371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006796] [C loss: 0.695548]\n",
      "4372 [D loss: 0.695551, acc.: 0.00%] [G loss: 0.005924] [C loss: 0.695547]\n",
      "4373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005304] [C loss: 0.695547]\n",
      "4374 [D loss: 0.695551, acc.: 0.00%] [G loss: 0.005694] [C loss: 0.695546]\n",
      "4375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005128] [C loss: 0.695546]\n",
      "4376 [D loss: 0.695551, acc.: 0.00%] [G loss: 0.004999] [C loss: 0.695546]\n",
      "4377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005102] [C loss: 0.695546]\n",
      "4378 [D loss: 0.695549, acc.: 0.00%] [G loss: 0.007434] [C loss: 0.695544]\n",
      "4379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006542] [C loss: 0.695544]\n",
      "4380 [D loss: 0.695547, acc.: 0.00%] [G loss: 0.003503] [C loss: 0.695543]\n",
      "4381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004636] [C loss: 0.695543]\n",
      "4382 [D loss: 0.695547, acc.: 0.00%] [G loss: 0.016773] [C loss: 0.695541]\n",
      "4383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006193] [C loss: 0.695541]\n",
      "4384 [D loss: 0.695543, acc.: 0.00%] [G loss: 0.004487] [C loss: 0.695539]\n",
      "4385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007283] [C loss: 0.695539]\n",
      "4386 [D loss: 0.695543, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.695537]\n",
      "4387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003745] [C loss: 0.695537]\n",
      "4388 [D loss: 0.695540, acc.: 0.00%] [G loss: 0.004791] [C loss: 0.695538]\n",
      "4389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006807] [C loss: 0.695538]\n",
      "4390 [D loss: 0.695540, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.695536]\n",
      "4391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013514] [C loss: 0.695536]\n",
      "4392 [D loss: 0.695538, acc.: 0.00%] [G loss: 0.005681] [C loss: 0.695533]\n",
      "4393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006601] [C loss: 0.695533]\n",
      "4394 [D loss: 0.695537, acc.: 0.00%] [G loss: 0.006038] [C loss: 0.695533]\n",
      "4395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004943] [C loss: 0.695533]\n",
      "4396 [D loss: 0.695535, acc.: 0.00%] [G loss: 0.004940] [C loss: 0.695531]\n",
      "4397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004519] [C loss: 0.695531]\n",
      "4398 [D loss: 0.695534, acc.: 0.00%] [G loss: 0.004311] [C loss: 0.695531]\n",
      "4399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004842] [C loss: 0.695531]\n",
      "4400 [D loss: 0.695533, acc.: 0.00%] [G loss: 0.011000] [C loss: 0.695528]\n",
      "4401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005081] [C loss: 0.695528]\n",
      "4402 [D loss: 0.695532, acc.: 0.00%] [G loss: 0.007328] [C loss: 0.695527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005199] [C loss: 0.695527]\n",
      "4404 [D loss: 0.695529, acc.: 0.00%] [G loss: 0.004909] [C loss: 0.695525]\n",
      "4405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007060] [C loss: 0.695525]\n",
      "4406 [D loss: 0.695529, acc.: 0.00%] [G loss: 0.003448] [C loss: 0.695524]\n",
      "4407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004934] [C loss: 0.695524]\n",
      "4408 [D loss: 0.695527, acc.: 0.00%] [G loss: 0.006355] [C loss: 0.695523]\n",
      "4409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009206] [C loss: 0.695523]\n",
      "4410 [D loss: 0.695525, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.695523]\n",
      "4411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006165] [C loss: 0.695523]\n",
      "4412 [D loss: 0.695524, acc.: 0.00%] [G loss: 0.006019] [C loss: 0.695520]\n",
      "4413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006179] [C loss: 0.695520]\n",
      "4414 [D loss: 0.695523, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.695520]\n",
      "4415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006749] [C loss: 0.695520]\n",
      "4416 [D loss: 0.695521, acc.: 0.00%] [G loss: 0.005464] [C loss: 0.695517]\n",
      "4417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005986] [C loss: 0.695517]\n",
      "4418 [D loss: 0.695520, acc.: 0.00%] [G loss: 0.005880] [C loss: 0.695514]\n",
      "4419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007323] [C loss: 0.695514]\n",
      "4420 [D loss: 0.695518, acc.: 0.00%] [G loss: 0.004248] [C loss: 0.695513]\n",
      "4421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005015] [C loss: 0.695513]\n",
      "4422 [D loss: 0.695516, acc.: 0.00%] [G loss: 0.003994] [C loss: 0.695512]\n",
      "4423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010343] [C loss: 0.695512]\n",
      "4424 [D loss: 0.695515, acc.: 0.00%] [G loss: 0.006747] [C loss: 0.695511]\n",
      "4425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005802] [C loss: 0.695511]\n",
      "4426 [D loss: 0.695514, acc.: 0.00%] [G loss: 0.004862] [C loss: 0.695509]\n",
      "4427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006574] [C loss: 0.695509]\n",
      "4428 [D loss: 0.695513, acc.: 0.00%] [G loss: 0.007392] [C loss: 0.695507]\n",
      "4429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004633] [C loss: 0.695507]\n",
      "4430 [D loss: 0.695511, acc.: 0.00%] [G loss: 0.005242] [C loss: 0.695507]\n",
      "4431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023085] [C loss: 0.695507]\n",
      "4432 [D loss: 0.695510, acc.: 0.00%] [G loss: 0.005479] [C loss: 0.695505]\n",
      "4433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005246] [C loss: 0.695505]\n",
      "4434 [D loss: 0.695507, acc.: 0.00%] [G loss: 0.005249] [C loss: 0.695504]\n",
      "4435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004199] [C loss: 0.695504]\n",
      "4436 [D loss: 0.695507, acc.: 0.00%] [G loss: 0.004044] [C loss: 0.695503]\n",
      "4437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004627] [C loss: 0.695503]\n",
      "4438 [D loss: 0.695505, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.695500]\n",
      "4439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003959] [C loss: 0.695500]\n",
      "4440 [D loss: 0.695504, acc.: 0.00%] [G loss: 0.005893] [C loss: 0.695498]\n",
      "4441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005029] [C loss: 0.695498]\n",
      "4442 [D loss: 0.695501, acc.: 0.00%] [G loss: 0.004923] [C loss: 0.695499]\n",
      "4443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004030] [C loss: 0.695499]\n",
      "4444 [D loss: 0.695502, acc.: 0.00%] [G loss: 0.007867] [C loss: 0.695497]\n",
      "4445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005025] [C loss: 0.695497]\n",
      "4446 [D loss: 0.695499, acc.: 0.00%] [G loss: 0.005234] [C loss: 0.695494]\n",
      "4447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007767] [C loss: 0.695494]\n",
      "4448 [D loss: 0.695498, acc.: 0.00%] [G loss: 0.004586] [C loss: 0.695493]\n",
      "4449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005281] [C loss: 0.695493]\n",
      "4450 [D loss: 0.695496, acc.: 0.00%] [G loss: 0.004428] [C loss: 0.695491]\n",
      "4451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004427] [C loss: 0.695491]\n",
      "4452 [D loss: 0.695494, acc.: 0.00%] [G loss: 0.006124] [C loss: 0.695491]\n",
      "4453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004412] [C loss: 0.695491]\n",
      "4454 [D loss: 0.695494, acc.: 0.00%] [G loss: 0.005389] [C loss: 0.695488]\n",
      "4455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005996] [C loss: 0.695488]\n",
      "4456 [D loss: 0.695491, acc.: 0.00%] [G loss: 0.004730] [C loss: 0.695487]\n",
      "4457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008336] [C loss: 0.695487]\n",
      "4458 [D loss: 0.695489, acc.: 0.00%] [G loss: 0.005443] [C loss: 0.695485]\n",
      "4459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004574] [C loss: 0.695485]\n",
      "4460 [D loss: 0.695489, acc.: 0.00%] [G loss: 0.007844] [C loss: 0.695483]\n",
      "4461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004187] [C loss: 0.695483]\n",
      "4462 [D loss: 0.695487, acc.: 0.00%] [G loss: 0.005187] [C loss: 0.695484]\n",
      "4463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004145] [C loss: 0.695484]\n",
      "4464 [D loss: 0.695487, acc.: 0.00%] [G loss: 0.003933] [C loss: 0.695482]\n",
      "4465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004851] [C loss: 0.695482]\n",
      "4466 [D loss: 0.695484, acc.: 0.00%] [G loss: 0.004730] [C loss: 0.695481]\n",
      "4467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004297] [C loss: 0.695481]\n",
      "4468 [D loss: 0.695484, acc.: 0.00%] [G loss: 0.004363] [C loss: 0.695478]\n",
      "4469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004588] [C loss: 0.695478]\n",
      "4470 [D loss: 0.695481, acc.: 0.00%] [G loss: 0.004876] [C loss: 0.695478]\n",
      "4471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.695478]\n",
      "4472 [D loss: 0.695481, acc.: 0.00%] [G loss: 0.002923] [C loss: 0.695477]\n",
      "4473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014482] [C loss: 0.695477]\n",
      "4474 [D loss: 0.695479, acc.: 0.00%] [G loss: 0.010989] [C loss: 0.695473]\n",
      "4475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005693] [C loss: 0.695473]\n",
      "4476 [D loss: 0.695477, acc.: 0.00%] [G loss: 0.003301] [C loss: 0.695473]\n",
      "4477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005927] [C loss: 0.695473]\n",
      "4478 [D loss: 0.695477, acc.: 0.00%] [G loss: 0.006545] [C loss: 0.695473]\n",
      "4479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007540] [C loss: 0.695473]\n",
      "4480 [D loss: 0.695475, acc.: 0.00%] [G loss: 0.004061] [C loss: 0.695470]\n",
      "4481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004565] [C loss: 0.695470]\n",
      "4482 [D loss: 0.695473, acc.: 0.00%] [G loss: 0.009079] [C loss: 0.695469]\n",
      "4483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008183] [C loss: 0.695469]\n",
      "4484 [D loss: 0.695472, acc.: 0.00%] [G loss: 0.004833] [C loss: 0.695469]\n",
      "4485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006963] [C loss: 0.695469]\n",
      "4486 [D loss: 0.695472, acc.: 0.00%] [G loss: 0.007634] [C loss: 0.695466]\n",
      "4487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006916] [C loss: 0.695466]\n",
      "4488 [D loss: 0.695469, acc.: 0.00%] [G loss: 0.005760] [C loss: 0.695463]\n",
      "4489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004414] [C loss: 0.695463]\n",
      "4490 [D loss: 0.695466, acc.: 0.00%] [G loss: 0.006008] [C loss: 0.695464]\n",
      "4491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005184] [C loss: 0.695464]\n",
      "4492 [D loss: 0.695466, acc.: 0.00%] [G loss: 0.005951] [C loss: 0.695463]\n",
      "4493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.695463]\n",
      "4494 [D loss: 0.695465, acc.: 0.00%] [G loss: 0.007583] [C loss: 0.695462]\n",
      "4495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004642] [C loss: 0.695462]\n",
      "4496 [D loss: 0.695464, acc.: 0.00%] [G loss: 0.005355] [C loss: 0.695459]\n",
      "4497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005501] [C loss: 0.695459]\n",
      "4498 [D loss: 0.695462, acc.: 0.00%] [G loss: 0.004264] [C loss: 0.695458]\n",
      "4499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003941] [C loss: 0.695458]\n",
      "4500 [D loss: 0.695460, acc.: 0.00%] [G loss: 0.005127] [C loss: 0.695456]\n",
      "4501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016767] [C loss: 0.695456]\n",
      "4502 [D loss: 0.695459, acc.: 0.00%] [G loss: 0.007649] [C loss: 0.695455]\n",
      "4503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005806] [C loss: 0.695455]\n",
      "4504 [D loss: 0.695459, acc.: 0.00%] [G loss: 0.005018] [C loss: 0.695452]\n",
      "4505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005602] [C loss: 0.695452]\n",
      "4506 [D loss: 0.695457, acc.: 0.00%] [G loss: 0.005810] [C loss: 0.695451]\n",
      "4507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005126] [C loss: 0.695451]\n",
      "4508 [D loss: 0.695455, acc.: 0.00%] [G loss: 0.006344] [C loss: 0.695447]\n",
      "4509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004134] [C loss: 0.695447]\n",
      "4510 [D loss: 0.695452, acc.: 0.00%] [G loss: 0.007542] [C loss: 0.695448]\n",
      "4511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007010] [C loss: 0.695448]\n",
      "4512 [D loss: 0.695452, acc.: 0.00%] [G loss: 0.005769] [C loss: 0.695445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.695445]\n",
      "4514 [D loss: 0.695449, acc.: 0.00%] [G loss: 0.003304] [C loss: 0.695445]\n",
      "4515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004087] [C loss: 0.695445]\n",
      "4516 [D loss: 0.695449, acc.: 0.00%] [G loss: 0.005509] [C loss: 0.695442]\n",
      "4517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.695442]\n",
      "4518 [D loss: 0.695445, acc.: 0.00%] [G loss: 0.007060] [C loss: 0.695443]\n",
      "4519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007047] [C loss: 0.695443]\n",
      "4520 [D loss: 0.695446, acc.: 0.00%] [G loss: 0.010073] [C loss: 0.695442]\n",
      "4521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008748] [C loss: 0.695442]\n",
      "4522 [D loss: 0.695444, acc.: 0.00%] [G loss: 0.005065] [C loss: 0.695440]\n",
      "4523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.695440]\n",
      "4524 [D loss: 0.695443, acc.: 0.00%] [G loss: 0.008874] [C loss: 0.695438]\n",
      "4525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004834] [C loss: 0.695438]\n",
      "4526 [D loss: 0.695441, acc.: 0.00%] [G loss: 0.005377] [C loss: 0.695437]\n",
      "4527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005369] [C loss: 0.695437]\n",
      "4528 [D loss: 0.695440, acc.: 0.00%] [G loss: 0.004659] [C loss: 0.695436]\n",
      "4529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006445] [C loss: 0.695436]\n",
      "4530 [D loss: 0.695439, acc.: 0.00%] [G loss: 0.006079] [C loss: 0.695434]\n",
      "4531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006650] [C loss: 0.695434]\n",
      "4532 [D loss: 0.695437, acc.: 0.00%] [G loss: 0.005040] [C loss: 0.695434]\n",
      "4533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006486] [C loss: 0.695434]\n",
      "4534 [D loss: 0.695436, acc.: 0.00%] [G loss: 0.007548] [C loss: 0.695430]\n",
      "4535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008294] [C loss: 0.695430]\n",
      "4536 [D loss: 0.695434, acc.: 0.00%] [G loss: 0.005709] [C loss: 0.695430]\n",
      "4537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008095] [C loss: 0.695430]\n",
      "4538 [D loss: 0.695433, acc.: 0.00%] [G loss: 0.004664] [C loss: 0.695427]\n",
      "4539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005814] [C loss: 0.695427]\n",
      "4540 [D loss: 0.695431, acc.: 0.00%] [G loss: 0.004215] [C loss: 0.695429]\n",
      "4541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006550] [C loss: 0.695429]\n",
      "4542 [D loss: 0.695430, acc.: 0.00%] [G loss: 0.005955] [C loss: 0.695426]\n",
      "4543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004383] [C loss: 0.695426]\n",
      "4544 [D loss: 0.695429, acc.: 0.00%] [G loss: 0.004926] [C loss: 0.695423]\n",
      "4545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004227] [C loss: 0.695423]\n",
      "4546 [D loss: 0.695426, acc.: 0.00%] [G loss: 0.006737] [C loss: 0.695422]\n",
      "4547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004189] [C loss: 0.695422]\n",
      "4548 [D loss: 0.695425, acc.: 0.00%] [G loss: 0.004209] [C loss: 0.695421]\n",
      "4549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004820] [C loss: 0.695421]\n",
      "4550 [D loss: 0.695424, acc.: 0.00%] [G loss: 0.006583] [C loss: 0.695419]\n",
      "4551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005371] [C loss: 0.695419]\n",
      "4552 [D loss: 0.695422, acc.: 0.00%] [G loss: 0.004239] [C loss: 0.695418]\n",
      "4553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003254] [C loss: 0.695418]\n",
      "4554 [D loss: 0.695421, acc.: 0.00%] [G loss: 0.004772] [C loss: 0.695416]\n",
      "4555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005844] [C loss: 0.695416]\n",
      "4556 [D loss: 0.695419, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.695417]\n",
      "4557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006848] [C loss: 0.695417]\n",
      "4558 [D loss: 0.695419, acc.: 0.00%] [G loss: 0.007742] [C loss: 0.695414]\n",
      "4559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004047] [C loss: 0.695414]\n",
      "4560 [D loss: 0.695416, acc.: 0.00%] [G loss: 0.007039] [C loss: 0.695413]\n",
      "4561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007753] [C loss: 0.695413]\n",
      "4562 [D loss: 0.695416, acc.: 0.00%] [G loss: 0.003923] [C loss: 0.695411]\n",
      "4563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005621] [C loss: 0.695411]\n",
      "4564 [D loss: 0.695414, acc.: 0.00%] [G loss: 0.011414] [C loss: 0.695409]\n",
      "4565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005953] [C loss: 0.695409]\n",
      "4566 [D loss: 0.695411, acc.: 0.00%] [G loss: 0.004303] [C loss: 0.695405]\n",
      "4567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004599] [C loss: 0.695405]\n",
      "4568 [D loss: 0.695409, acc.: 0.00%] [G loss: 0.005292] [C loss: 0.695405]\n",
      "4569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007359] [C loss: 0.695405]\n",
      "4570 [D loss: 0.695408, acc.: 0.00%] [G loss: 0.009850] [C loss: 0.695404]\n",
      "4571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006726] [C loss: 0.695404]\n",
      "4572 [D loss: 0.695407, acc.: 0.00%] [G loss: 0.005594] [C loss: 0.695403]\n",
      "4573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006878] [C loss: 0.695403]\n",
      "4574 [D loss: 0.695406, acc.: 0.00%] [G loss: 0.004978] [C loss: 0.695402]\n",
      "4575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012147] [C loss: 0.695402]\n",
      "4576 [D loss: 0.695405, acc.: 0.00%] [G loss: 0.007122] [C loss: 0.695400]\n",
      "4577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006852] [C loss: 0.695400]\n",
      "4578 [D loss: 0.695404, acc.: 0.00%] [G loss: 0.005439] [C loss: 0.695397]\n",
      "4579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009716] [C loss: 0.695397]\n",
      "4580 [D loss: 0.695401, acc.: 0.00%] [G loss: 0.006448] [C loss: 0.695396]\n",
      "4581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008252] [C loss: 0.695396]\n",
      "4582 [D loss: 0.695400, acc.: 0.00%] [G loss: 0.007376] [C loss: 0.695397]\n",
      "4583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007098] [C loss: 0.695397]\n",
      "4584 [D loss: 0.695398, acc.: 0.00%] [G loss: 0.006866] [C loss: 0.695392]\n",
      "4585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005536] [C loss: 0.695392]\n",
      "4586 [D loss: 0.695397, acc.: 0.00%] [G loss: 0.007823] [C loss: 0.695392]\n",
      "4587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004326] [C loss: 0.695392]\n",
      "4588 [D loss: 0.695396, acc.: 0.00%] [G loss: 0.009285] [C loss: 0.695391]\n",
      "4589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015827] [C loss: 0.695391]\n",
      "4590 [D loss: 0.695394, acc.: 0.00%] [G loss: 0.005848] [C loss: 0.695391]\n",
      "4591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005220] [C loss: 0.695391]\n",
      "4592 [D loss: 0.695393, acc.: 0.00%] [G loss: 0.012307] [C loss: 0.695387]\n",
      "4593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005944] [C loss: 0.695387]\n",
      "4594 [D loss: 0.695390, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.695388]\n",
      "4595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006103] [C loss: 0.695388]\n",
      "4596 [D loss: 0.695390, acc.: 0.00%] [G loss: 0.006010] [C loss: 0.695385]\n",
      "4597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005042] [C loss: 0.695385]\n",
      "4598 [D loss: 0.695388, acc.: 0.00%] [G loss: 0.006301] [C loss: 0.695384]\n",
      "4599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009024] [C loss: 0.695384]\n",
      "4600 [D loss: 0.695387, acc.: 0.00%] [G loss: 0.006140] [C loss: 0.695383]\n",
      "4601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007215] [C loss: 0.695383]\n",
      "4602 [D loss: 0.695385, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.695380]\n",
      "4603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006047] [C loss: 0.695380]\n",
      "4604 [D loss: 0.695383, acc.: 0.00%] [G loss: 0.005454] [C loss: 0.695380]\n",
      "4605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007053] [C loss: 0.695380]\n",
      "4606 [D loss: 0.695383, acc.: 0.00%] [G loss: 0.005270] [C loss: 0.695378]\n",
      "4607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005404] [C loss: 0.695378]\n",
      "4608 [D loss: 0.695381, acc.: 0.00%] [G loss: 0.006927] [C loss: 0.695376]\n",
      "4609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003955] [C loss: 0.695376]\n",
      "4610 [D loss: 0.695379, acc.: 0.00%] [G loss: 0.004711] [C loss: 0.695373]\n",
      "4611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003621] [C loss: 0.695373]\n",
      "4612 [D loss: 0.695377, acc.: 0.00%] [G loss: 0.004348] [C loss: 0.695372]\n",
      "4613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013247] [C loss: 0.695372]\n",
      "4614 [D loss: 0.695375, acc.: 0.00%] [G loss: 0.005816] [C loss: 0.695372]\n",
      "4615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004687] [C loss: 0.695372]\n",
      "4616 [D loss: 0.695375, acc.: 0.00%] [G loss: 0.004849] [C loss: 0.695369]\n",
      "4617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005191] [C loss: 0.695369]\n",
      "4618 [D loss: 0.695372, acc.: 0.00%] [G loss: 0.008301] [C loss: 0.695368]\n",
      "4619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011995] [C loss: 0.695368]\n",
      "4620 [D loss: 0.695372, acc.: 0.00%] [G loss: 0.005125] [C loss: 0.695366]\n",
      "4621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005269] [C loss: 0.695366]\n",
      "4622 [D loss: 0.695369, acc.: 0.00%] [G loss: 0.018780] [C loss: 0.695366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007826] [C loss: 0.695366]\n",
      "4624 [D loss: 0.695368, acc.: 0.00%] [G loss: 0.006665] [C loss: 0.695363]\n",
      "4625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007627] [C loss: 0.695363]\n",
      "4626 [D loss: 0.695366, acc.: 0.00%] [G loss: 0.005293] [C loss: 0.695363]\n",
      "4627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005174] [C loss: 0.695363]\n",
      "4628 [D loss: 0.695367, acc.: 0.00%] [G loss: 0.006271] [C loss: 0.695359]\n",
      "4629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005559] [C loss: 0.695359]\n",
      "4630 [D loss: 0.695363, acc.: 0.00%] [G loss: 0.006944] [C loss: 0.695359]\n",
      "4631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006751] [C loss: 0.695359]\n",
      "4632 [D loss: 0.695362, acc.: 0.00%] [G loss: 0.007139] [C loss: 0.695359]\n",
      "4633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008865] [C loss: 0.695359]\n",
      "4634 [D loss: 0.695361, acc.: 0.00%] [G loss: 0.004192] [C loss: 0.695356]\n",
      "4635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.695356]\n",
      "4636 [D loss: 0.695359, acc.: 0.00%] [G loss: 0.004804] [C loss: 0.695356]\n",
      "4637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008215] [C loss: 0.695356]\n",
      "4638 [D loss: 0.695358, acc.: 0.00%] [G loss: 0.007642] [C loss: 0.695354]\n",
      "4639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005128] [C loss: 0.695354]\n",
      "4640 [D loss: 0.695357, acc.: 0.00%] [G loss: 0.005532] [C loss: 0.695351]\n",
      "4641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006488] [C loss: 0.695351]\n",
      "4642 [D loss: 0.695354, acc.: 0.00%] [G loss: 0.004633] [C loss: 0.695351]\n",
      "4643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005446] [C loss: 0.695351]\n",
      "4644 [D loss: 0.695353, acc.: 0.00%] [G loss: 0.005983] [C loss: 0.695349]\n",
      "4645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005671] [C loss: 0.695349]\n",
      "4646 [D loss: 0.695353, acc.: 0.00%] [G loss: 0.004366] [C loss: 0.695348]\n",
      "4647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006535] [C loss: 0.695348]\n",
      "4648 [D loss: 0.695351, acc.: 0.00%] [G loss: 0.006420] [C loss: 0.695346]\n",
      "4649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010411] [C loss: 0.695346]\n",
      "4650 [D loss: 0.695350, acc.: 0.00%] [G loss: 0.004601] [C loss: 0.695344]\n",
      "4651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.695344]\n",
      "4652 [D loss: 0.695348, acc.: 0.00%] [G loss: 0.015305] [C loss: 0.695344]\n",
      "4653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003894] [C loss: 0.695344]\n",
      "4654 [D loss: 0.695347, acc.: 0.00%] [G loss: 0.006221] [C loss: 0.695343]\n",
      "4655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005694] [C loss: 0.695343]\n",
      "4656 [D loss: 0.695346, acc.: 0.00%] [G loss: 0.005430] [C loss: 0.695342]\n",
      "4657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008040] [C loss: 0.695342]\n",
      "4658 [D loss: 0.695344, acc.: 0.00%] [G loss: 0.004836] [C loss: 0.695340]\n",
      "4659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005845] [C loss: 0.695340]\n",
      "4660 [D loss: 0.695342, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.695338]\n",
      "4661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004528] [C loss: 0.695338]\n",
      "4662 [D loss: 0.695341, acc.: 0.00%] [G loss: 0.006451] [C loss: 0.695337]\n",
      "4663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005003] [C loss: 0.695337]\n",
      "4664 [D loss: 0.695339, acc.: 0.00%] [G loss: 0.007659] [C loss: 0.695334]\n",
      "4665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004603] [C loss: 0.695334]\n",
      "4666 [D loss: 0.695337, acc.: 0.00%] [G loss: 0.004307] [C loss: 0.695333]\n",
      "4667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004884] [C loss: 0.695333]\n",
      "4668 [D loss: 0.695337, acc.: 0.00%] [G loss: 0.010783] [C loss: 0.695332]\n",
      "4669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005715] [C loss: 0.695332]\n",
      "4670 [D loss: 0.695335, acc.: 0.00%] [G loss: 0.006333] [C loss: 0.695330]\n",
      "4671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005081] [C loss: 0.695330]\n",
      "4672 [D loss: 0.695333, acc.: 0.00%] [G loss: 0.005020] [C loss: 0.695328]\n",
      "4673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007501] [C loss: 0.695328]\n",
      "4674 [D loss: 0.695331, acc.: 0.00%] [G loss: 0.005436] [C loss: 0.695329]\n",
      "4675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004028] [C loss: 0.695329]\n",
      "4676 [D loss: 0.695332, acc.: 0.00%] [G loss: 0.005119] [C loss: 0.695327]\n",
      "4677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003479] [C loss: 0.695327]\n",
      "4678 [D loss: 0.695329, acc.: 0.00%] [G loss: 0.005718] [C loss: 0.695324]\n",
      "4679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006252] [C loss: 0.695324]\n",
      "4680 [D loss: 0.695327, acc.: 0.00%] [G loss: 0.005498] [C loss: 0.695324]\n",
      "4681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004433] [C loss: 0.695324]\n",
      "4682 [D loss: 0.695325, acc.: 0.00%] [G loss: 0.004255] [C loss: 0.695321]\n",
      "4683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005603] [C loss: 0.695321]\n",
      "4684 [D loss: 0.695324, acc.: 0.00%] [G loss: 0.009885] [C loss: 0.695319]\n",
      "4685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004496] [C loss: 0.695319]\n",
      "4686 [D loss: 0.695322, acc.: 0.00%] [G loss: 0.005102] [C loss: 0.695320]\n",
      "4687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006470] [C loss: 0.695320]\n",
      "4688 [D loss: 0.695322, acc.: 0.00%] [G loss: 0.015548] [C loss: 0.695318]\n",
      "4689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007735] [C loss: 0.695318]\n",
      "4690 [D loss: 0.695320, acc.: 0.00%] [G loss: 0.006523] [C loss: 0.695315]\n",
      "4691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005110] [C loss: 0.695315]\n",
      "4692 [D loss: 0.695318, acc.: 0.00%] [G loss: 0.007133] [C loss: 0.695311]\n",
      "4693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006057] [C loss: 0.695311]\n",
      "4694 [D loss: 0.695316, acc.: 0.00%] [G loss: 0.007580] [C loss: 0.695310]\n",
      "4695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003629] [C loss: 0.695310]\n",
      "4696 [D loss: 0.695314, acc.: 0.00%] [G loss: 0.006648] [C loss: 0.695310]\n",
      "4697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007593] [C loss: 0.695310]\n",
      "4698 [D loss: 0.695314, acc.: 0.00%] [G loss: 0.006082] [C loss: 0.695309]\n",
      "4699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006410] [C loss: 0.695309]\n",
      "4700 [D loss: 0.695314, acc.: 0.00%] [G loss: 0.005951] [C loss: 0.695308]\n",
      "4701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004336] [C loss: 0.695308]\n",
      "4702 [D loss: 0.695310, acc.: 0.00%] [G loss: 0.006452] [C loss: 0.695308]\n",
      "4703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005649] [C loss: 0.695308]\n",
      "4704 [D loss: 0.695310, acc.: 0.00%] [G loss: 0.005135] [C loss: 0.695305]\n",
      "4705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007423] [C loss: 0.695305]\n",
      "4706 [D loss: 0.695309, acc.: 0.00%] [G loss: 0.004735] [C loss: 0.695305]\n",
      "4707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003669] [C loss: 0.695305]\n",
      "4708 [D loss: 0.695307, acc.: 0.00%] [G loss: 0.005492] [C loss: 0.695302]\n",
      "4709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008206] [C loss: 0.695302]\n",
      "4710 [D loss: 0.695305, acc.: 0.00%] [G loss: 0.006262] [C loss: 0.695302]\n",
      "4711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006358] [C loss: 0.695302]\n",
      "4712 [D loss: 0.695304, acc.: 0.00%] [G loss: 0.006197] [C loss: 0.695300]\n",
      "4713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005465] [C loss: 0.695300]\n",
      "4714 [D loss: 0.695302, acc.: 0.00%] [G loss: 0.007046] [C loss: 0.695297]\n",
      "4715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005648] [C loss: 0.695297]\n",
      "4716 [D loss: 0.695300, acc.: 0.00%] [G loss: 0.005941] [C loss: 0.695297]\n",
      "4717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005012] [C loss: 0.695297]\n",
      "4718 [D loss: 0.695300, acc.: 0.00%] [G loss: 0.004727] [C loss: 0.695294]\n",
      "4719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005490] [C loss: 0.695294]\n",
      "4720 [D loss: 0.695297, acc.: 0.00%] [G loss: 0.004319] [C loss: 0.695292]\n",
      "4721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.695292]\n",
      "4722 [D loss: 0.695296, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.695290]\n",
      "4723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003878] [C loss: 0.695290]\n",
      "4724 [D loss: 0.695293, acc.: 0.00%] [G loss: 0.007083] [C loss: 0.695289]\n",
      "4725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008756] [C loss: 0.695289]\n",
      "4726 [D loss: 0.695293, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.695287]\n",
      "4727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005393] [C loss: 0.695287]\n",
      "4728 [D loss: 0.695291, acc.: 0.00%] [G loss: 0.005993] [C loss: 0.695286]\n",
      "4729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005359] [C loss: 0.695286]\n",
      "4730 [D loss: 0.695291, acc.: 0.00%] [G loss: 0.004924] [C loss: 0.695286]\n",
      "4731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007128] [C loss: 0.695286]\n",
      "4732 [D loss: 0.695289, acc.: 0.00%] [G loss: 0.005889] [C loss: 0.695285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004835] [C loss: 0.695285]\n",
      "4734 [D loss: 0.695287, acc.: 0.00%] [G loss: 0.005915] [C loss: 0.695283]\n",
      "4735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007714] [C loss: 0.695283]\n",
      "4736 [D loss: 0.695286, acc.: 0.00%] [G loss: 0.004179] [C loss: 0.695280]\n",
      "4737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005941] [C loss: 0.695280]\n",
      "4738 [D loss: 0.695283, acc.: 0.00%] [G loss: 0.008001] [C loss: 0.695279]\n",
      "4739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005840] [C loss: 0.695279]\n",
      "4740 [D loss: 0.695283, acc.: 0.00%] [G loss: 0.007341] [C loss: 0.695280]\n",
      "4741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005254] [C loss: 0.695280]\n",
      "4742 [D loss: 0.695281, acc.: 0.00%] [G loss: 0.006668] [C loss: 0.695276]\n",
      "4743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008200] [C loss: 0.695276]\n",
      "4744 [D loss: 0.695279, acc.: 0.00%] [G loss: 0.005657] [C loss: 0.695275]\n",
      "4745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006093] [C loss: 0.695275]\n",
      "4746 [D loss: 0.695279, acc.: 0.00%] [G loss: 0.005441] [C loss: 0.695274]\n",
      "4747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008638] [C loss: 0.695274]\n",
      "4748 [D loss: 0.695277, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.695272]\n",
      "4749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009549] [C loss: 0.695272]\n",
      "4750 [D loss: 0.695275, acc.: 0.00%] [G loss: 0.006142] [C loss: 0.695271]\n",
      "4751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004946] [C loss: 0.695271]\n",
      "4752 [D loss: 0.695273, acc.: 0.00%] [G loss: 0.005848] [C loss: 0.695269]\n",
      "4753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009342] [C loss: 0.695269]\n",
      "4754 [D loss: 0.695273, acc.: 0.00%] [G loss: 0.007022] [C loss: 0.695267]\n",
      "4755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005939] [C loss: 0.695267]\n",
      "4756 [D loss: 0.695270, acc.: 0.00%] [G loss: 0.006055] [C loss: 0.695267]\n",
      "4757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005299] [C loss: 0.695267]\n",
      "4758 [D loss: 0.695269, acc.: 0.00%] [G loss: 0.005143] [C loss: 0.695265]\n",
      "4759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007226] [C loss: 0.695265]\n",
      "4760 [D loss: 0.695268, acc.: 0.00%] [G loss: 0.010997] [C loss: 0.695263]\n",
      "4761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005338] [C loss: 0.695263]\n",
      "4762 [D loss: 0.695266, acc.: 0.00%] [G loss: 0.007993] [C loss: 0.695261]\n",
      "4763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005827] [C loss: 0.695261]\n",
      "4764 [D loss: 0.695265, acc.: 0.00%] [G loss: 0.009518] [C loss: 0.695262]\n",
      "4765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010041] [C loss: 0.695262]\n",
      "4766 [D loss: 0.695264, acc.: 0.00%] [G loss: 0.006404] [C loss: 0.695258]\n",
      "4767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007158] [C loss: 0.695258]\n",
      "4768 [D loss: 0.695262, acc.: 0.00%] [G loss: 0.004847] [C loss: 0.695257]\n",
      "4769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005584] [C loss: 0.695257]\n",
      "4770 [D loss: 0.695261, acc.: 0.00%] [G loss: 0.005025] [C loss: 0.695257]\n",
      "4771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004638] [C loss: 0.695257]\n",
      "4772 [D loss: 0.695259, acc.: 0.00%] [G loss: 0.004564] [C loss: 0.695254]\n",
      "4773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004128] [C loss: 0.695254]\n",
      "4774 [D loss: 0.695257, acc.: 0.00%] [G loss: 0.003361] [C loss: 0.695254]\n",
      "4775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004465] [C loss: 0.695254]\n",
      "4776 [D loss: 0.695256, acc.: 0.00%] [G loss: 0.004892] [C loss: 0.695251]\n",
      "4777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005268] [C loss: 0.695251]\n",
      "4778 [D loss: 0.695253, acc.: 0.00%] [G loss: 0.009965] [C loss: 0.695251]\n",
      "4779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006473] [C loss: 0.695251]\n",
      "4780 [D loss: 0.695254, acc.: 0.00%] [G loss: 0.004018] [C loss: 0.695248]\n",
      "4781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006153] [C loss: 0.695248]\n",
      "4782 [D loss: 0.695252, acc.: 0.00%] [G loss: 0.005212] [C loss: 0.695246]\n",
      "4783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006338] [C loss: 0.695246]\n",
      "4784 [D loss: 0.695250, acc.: 0.00%] [G loss: 0.004738] [C loss: 0.695244]\n",
      "4785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005243] [C loss: 0.695244]\n",
      "4786 [D loss: 0.695248, acc.: 0.00%] [G loss: 0.005358] [C loss: 0.695243]\n",
      "4787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005271] [C loss: 0.695243]\n",
      "4788 [D loss: 0.695247, acc.: 0.00%] [G loss: 0.004357] [C loss: 0.695242]\n",
      "4789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010122] [C loss: 0.695242]\n",
      "4790 [D loss: 0.695245, acc.: 0.00%] [G loss: 0.005100] [C loss: 0.695241]\n",
      "4791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006858] [C loss: 0.695241]\n",
      "4792 [D loss: 0.695243, acc.: 0.00%] [G loss: 0.003869] [C loss: 0.695239]\n",
      "4793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005347] [C loss: 0.695239]\n",
      "4794 [D loss: 0.695242, acc.: 0.00%] [G loss: 0.006355] [C loss: 0.695238]\n",
      "4795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004219] [C loss: 0.695238]\n",
      "4796 [D loss: 0.695242, acc.: 0.00%] [G loss: 0.006301] [C loss: 0.695235]\n",
      "4797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006041] [C loss: 0.695235]\n",
      "4798 [D loss: 0.695238, acc.: 0.00%] [G loss: 0.006147] [C loss: 0.695234]\n",
      "4799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004342] [C loss: 0.695234]\n",
      "4800 [D loss: 0.695238, acc.: 0.00%] [G loss: 0.005173] [C loss: 0.695233]\n",
      "4801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008125] [C loss: 0.695233]\n",
      "4802 [D loss: 0.695237, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.695233]\n",
      "4803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004724] [C loss: 0.695233]\n",
      "4804 [D loss: 0.695236, acc.: 0.00%] [G loss: 0.008288] [C loss: 0.695229]\n",
      "4805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006333] [C loss: 0.695229]\n",
      "4806 [D loss: 0.695233, acc.: 0.00%] [G loss: 0.005355] [C loss: 0.695229]\n",
      "4807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.695229]\n",
      "4808 [D loss: 0.695232, acc.: 0.00%] [G loss: 0.005469] [C loss: 0.695229]\n",
      "4809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004588] [C loss: 0.695229]\n",
      "4810 [D loss: 0.695232, acc.: 0.00%] [G loss: 0.005139] [C loss: 0.695226]\n",
      "4811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018916] [C loss: 0.695226]\n",
      "4812 [D loss: 0.695228, acc.: 0.00%] [G loss: 0.006260] [C loss: 0.695224]\n",
      "4813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004779] [C loss: 0.695224]\n",
      "4814 [D loss: 0.695227, acc.: 0.00%] [G loss: 0.005746] [C loss: 0.695224]\n",
      "4815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006757] [C loss: 0.695224]\n",
      "4816 [D loss: 0.695226, acc.: 0.00%] [G loss: 0.005614] [C loss: 0.695223]\n",
      "4817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006365] [C loss: 0.695223]\n",
      "4818 [D loss: 0.695224, acc.: 0.00%] [G loss: 0.006294] [C loss: 0.695220]\n",
      "4819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007483] [C loss: 0.695220]\n",
      "4820 [D loss: 0.695223, acc.: 0.00%] [G loss: 0.004416] [C loss: 0.695221]\n",
      "4821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004514] [C loss: 0.695221]\n",
      "4822 [D loss: 0.695223, acc.: 0.00%] [G loss: 0.007853] [C loss: 0.695218]\n",
      "4823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010893] [C loss: 0.695218]\n",
      "4824 [D loss: 0.695219, acc.: 0.00%] [G loss: 0.004689] [C loss: 0.695216]\n",
      "4825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006646] [C loss: 0.695216]\n",
      "4826 [D loss: 0.695219, acc.: 0.00%] [G loss: 0.004779] [C loss: 0.695215]\n",
      "4827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006547] [C loss: 0.695215]\n",
      "4828 [D loss: 0.695217, acc.: 0.00%] [G loss: 0.004922] [C loss: 0.695211]\n",
      "4829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008144] [C loss: 0.695211]\n",
      "4830 [D loss: 0.695215, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.695209]\n",
      "4831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003618] [C loss: 0.695209]\n",
      "4832 [D loss: 0.695213, acc.: 0.00%] [G loss: 0.004600] [C loss: 0.695210]\n",
      "4833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004227] [C loss: 0.695210]\n",
      "4834 [D loss: 0.695213, acc.: 0.00%] [G loss: 0.010442] [C loss: 0.695206]\n",
      "4835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005098] [C loss: 0.695206]\n",
      "4836 [D loss: 0.695210, acc.: 0.00%] [G loss: 0.004756] [C loss: 0.695206]\n",
      "4837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006491] [C loss: 0.695206]\n",
      "4838 [D loss: 0.695209, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.695204]\n",
      "4839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007829] [C loss: 0.695204]\n",
      "4840 [D loss: 0.695207, acc.: 0.00%] [G loss: 0.005192] [C loss: 0.695204]\n",
      "4841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004588] [C loss: 0.695204]\n",
      "4842 [D loss: 0.695206, acc.: 0.00%] [G loss: 0.005940] [C loss: 0.695202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004367] [C loss: 0.695202]\n",
      "4844 [D loss: 0.695205, acc.: 0.00%] [G loss: 0.006523] [C loss: 0.695202]\n",
      "4845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006400] [C loss: 0.695202]\n",
      "4846 [D loss: 0.695204, acc.: 0.00%] [G loss: 0.005654] [C loss: 0.695199]\n",
      "4847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004890] [C loss: 0.695199]\n",
      "4848 [D loss: 0.695202, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.695197]\n",
      "4849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006362] [C loss: 0.695197]\n",
      "4850 [D loss: 0.695200, acc.: 0.00%] [G loss: 0.015074] [C loss: 0.695196]\n",
      "4851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009231] [C loss: 0.695196]\n",
      "4852 [D loss: 0.695199, acc.: 0.00%] [G loss: 0.004419] [C loss: 0.695194]\n",
      "4853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.695194]\n",
      "4854 [D loss: 0.695197, acc.: 0.00%] [G loss: 0.007034] [C loss: 0.695193]\n",
      "4855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005668] [C loss: 0.695193]\n",
      "4856 [D loss: 0.695196, acc.: 0.00%] [G loss: 0.005952] [C loss: 0.695190]\n",
      "4857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005275] [C loss: 0.695190]\n",
      "4858 [D loss: 0.695194, acc.: 0.00%] [G loss: 0.006853] [C loss: 0.695189]\n",
      "4859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005670] [C loss: 0.695189]\n",
      "4860 [D loss: 0.695193, acc.: 0.00%] [G loss: 0.003725] [C loss: 0.695188]\n",
      "4861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005512] [C loss: 0.695188]\n",
      "4862 [D loss: 0.695192, acc.: 0.00%] [G loss: 0.009108] [C loss: 0.695185]\n",
      "4863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007675] [C loss: 0.695185]\n",
      "4864 [D loss: 0.695189, acc.: 0.00%] [G loss: 0.005233] [C loss: 0.695185]\n",
      "4865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005710] [C loss: 0.695185]\n",
      "4866 [D loss: 0.695189, acc.: 0.00%] [G loss: 0.007097] [C loss: 0.695184]\n",
      "4867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006710] [C loss: 0.695184]\n",
      "4868 [D loss: 0.695186, acc.: 0.00%] [G loss: 0.009095] [C loss: 0.695183]\n",
      "4869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005672] [C loss: 0.695183]\n",
      "4870 [D loss: 0.695186, acc.: 0.00%] [G loss: 0.005096] [C loss: 0.695179]\n",
      "4871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004080] [C loss: 0.695179]\n",
      "4872 [D loss: 0.695183, acc.: 0.00%] [G loss: 0.005472] [C loss: 0.695179]\n",
      "4873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004962] [C loss: 0.695179]\n",
      "4874 [D loss: 0.695182, acc.: 0.00%] [G loss: 0.005785] [C loss: 0.695178]\n",
      "4875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006088] [C loss: 0.695178]\n",
      "4876 [D loss: 0.695181, acc.: 0.00%] [G loss: 0.021055] [C loss: 0.695176]\n",
      "4877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007140] [C loss: 0.695176]\n",
      "4878 [D loss: 0.695180, acc.: 0.00%] [G loss: 0.004407] [C loss: 0.695175]\n",
      "4879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007666] [C loss: 0.695175]\n",
      "4880 [D loss: 0.695179, acc.: 0.00%] [G loss: 0.008501] [C loss: 0.695173]\n",
      "4881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006808] [C loss: 0.695173]\n",
      "4882 [D loss: 0.695176, acc.: 0.00%] [G loss: 0.005524] [C loss: 0.695172]\n",
      "4883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005074] [C loss: 0.695172]\n",
      "4884 [D loss: 0.695175, acc.: 0.00%] [G loss: 0.004681] [C loss: 0.695169]\n",
      "4885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006784] [C loss: 0.695169]\n",
      "4886 [D loss: 0.695172, acc.: 0.00%] [G loss: 0.007266] [C loss: 0.695169]\n",
      "4887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006963] [C loss: 0.695169]\n",
      "4888 [D loss: 0.695172, acc.: 0.00%] [G loss: 0.003856] [C loss: 0.695168]\n",
      "4889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004923] [C loss: 0.695168]\n",
      "4890 [D loss: 0.695170, acc.: 0.00%] [G loss: 0.005831] [C loss: 0.695165]\n",
      "4891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004668] [C loss: 0.695165]\n",
      "4892 [D loss: 0.695169, acc.: 0.00%] [G loss: 0.004167] [C loss: 0.695164]\n",
      "4893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008576] [C loss: 0.695164]\n",
      "4894 [D loss: 0.695166, acc.: 0.00%] [G loss: 0.004586] [C loss: 0.695163]\n",
      "4895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005160] [C loss: 0.695163]\n",
      "4896 [D loss: 0.695166, acc.: 0.00%] [G loss: 0.005226] [C loss: 0.695161]\n",
      "4897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004744] [C loss: 0.695161]\n",
      "4898 [D loss: 0.695165, acc.: 0.00%] [G loss: 0.004802] [C loss: 0.695159]\n",
      "4899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013346] [C loss: 0.695159]\n",
      "4900 [D loss: 0.695162, acc.: 0.00%] [G loss: 0.004240] [C loss: 0.695158]\n",
      "4901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007550] [C loss: 0.695158]\n",
      "4902 [D loss: 0.695160, acc.: 0.00%] [G loss: 0.005068] [C loss: 0.695157]\n",
      "4903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006189] [C loss: 0.695157]\n",
      "4904 [D loss: 0.695160, acc.: 0.00%] [G loss: 0.005540] [C loss: 0.695156]\n",
      "4905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003877] [C loss: 0.695156]\n",
      "4906 [D loss: 0.695159, acc.: 0.00%] [G loss: 0.004959] [C loss: 0.695154]\n",
      "4907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006187] [C loss: 0.695154]\n",
      "4908 [D loss: 0.695157, acc.: 0.00%] [G loss: 0.005306] [C loss: 0.695152]\n",
      "4909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006402] [C loss: 0.695152]\n",
      "4910 [D loss: 0.695155, acc.: 0.00%] [G loss: 0.004736] [C loss: 0.695150]\n",
      "4911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015356] [C loss: 0.695150]\n",
      "4912 [D loss: 0.695153, acc.: 0.00%] [G loss: 0.006638] [C loss: 0.695148]\n",
      "4913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005311] [C loss: 0.695148]\n",
      "4914 [D loss: 0.695152, acc.: 0.00%] [G loss: 0.008674] [C loss: 0.695148]\n",
      "4915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006360] [C loss: 0.695148]\n",
      "4916 [D loss: 0.695151, acc.: 0.00%] [G loss: 0.006010] [C loss: 0.695146]\n",
      "4917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005659] [C loss: 0.695146]\n",
      "4918 [D loss: 0.695149, acc.: 0.00%] [G loss: 0.008638] [C loss: 0.695145]\n",
      "4919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005247] [C loss: 0.695145]\n",
      "4920 [D loss: 0.695147, acc.: 0.00%] [G loss: 0.004649] [C loss: 0.695143]\n",
      "4921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008138] [C loss: 0.695143]\n",
      "4922 [D loss: 0.695146, acc.: 0.00%] [G loss: 0.007241] [C loss: 0.695141]\n",
      "4923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007862] [C loss: 0.695141]\n",
      "4924 [D loss: 0.695144, acc.: 0.00%] [G loss: 0.004250] [C loss: 0.695141]\n",
      "4925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004389] [C loss: 0.695141]\n",
      "4926 [D loss: 0.695143, acc.: 0.00%] [G loss: 0.005635] [C loss: 0.695137]\n",
      "4927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006873] [C loss: 0.695137]\n",
      "4928 [D loss: 0.695140, acc.: 0.00%] [G loss: 0.004979] [C loss: 0.695136]\n",
      "4929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005540] [C loss: 0.695136]\n",
      "4930 [D loss: 0.695139, acc.: 0.00%] [G loss: 0.005019] [C loss: 0.695136]\n",
      "4931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004394] [C loss: 0.695136]\n",
      "4932 [D loss: 0.695138, acc.: 0.00%] [G loss: 0.004919] [C loss: 0.695132]\n",
      "4933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005196] [C loss: 0.695132]\n",
      "4934 [D loss: 0.695136, acc.: 0.00%] [G loss: 0.003922] [C loss: 0.695132]\n",
      "4935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007403] [C loss: 0.695132]\n",
      "4936 [D loss: 0.695136, acc.: 0.00%] [G loss: 0.004336] [C loss: 0.695130]\n",
      "4937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004416] [C loss: 0.695130]\n",
      "4938 [D loss: 0.695134, acc.: 0.00%] [G loss: 0.006286] [C loss: 0.695130]\n",
      "4939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003781] [C loss: 0.695130]\n",
      "4940 [D loss: 0.695132, acc.: 0.00%] [G loss: 0.007286] [C loss: 0.695128]\n",
      "4941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005993] [C loss: 0.695128]\n",
      "4942 [D loss: 0.695130, acc.: 0.00%] [G loss: 0.008108] [C loss: 0.695128]\n",
      "4943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008041] [C loss: 0.695128]\n",
      "4944 [D loss: 0.695130, acc.: 0.00%] [G loss: 0.005481] [C loss: 0.695125]\n",
      "4945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004656] [C loss: 0.695125]\n",
      "4946 [D loss: 0.695128, acc.: 0.00%] [G loss: 0.007503] [C loss: 0.695123]\n",
      "4947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004650] [C loss: 0.695123]\n",
      "4948 [D loss: 0.695126, acc.: 0.00%] [G loss: 0.007682] [C loss: 0.695122]\n",
      "4949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.695122]\n",
      "4950 [D loss: 0.695126, acc.: 0.00%] [G loss: 0.006253] [C loss: 0.695121]\n",
      "4951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005779] [C loss: 0.695121]\n",
      "4952 [D loss: 0.695123, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.695117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005517] [C loss: 0.695117]\n",
      "4954 [D loss: 0.695120, acc.: 0.00%] [G loss: 0.005464] [C loss: 0.695117]\n",
      "4955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.695117]\n",
      "4956 [D loss: 0.695121, acc.: 0.00%] [G loss: 0.006217] [C loss: 0.695116]\n",
      "4957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006657] [C loss: 0.695116]\n",
      "4958 [D loss: 0.695120, acc.: 0.00%] [G loss: 0.005905] [C loss: 0.695113]\n",
      "4959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009261] [C loss: 0.695113]\n",
      "4960 [D loss: 0.695117, acc.: 0.00%] [G loss: 0.004742] [C loss: 0.695114]\n",
      "4961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012069] [C loss: 0.695114]\n",
      "4962 [D loss: 0.695117, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.695111]\n",
      "4963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004363] [C loss: 0.695111]\n",
      "4964 [D loss: 0.695115, acc.: 0.00%] [G loss: 0.005172] [C loss: 0.695109]\n",
      "4965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006323] [C loss: 0.695109]\n",
      "4966 [D loss: 0.695113, acc.: 0.00%] [G loss: 0.009827] [C loss: 0.695109]\n",
      "4967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004750] [C loss: 0.695109]\n",
      "4968 [D loss: 0.695112, acc.: 0.00%] [G loss: 0.004360] [C loss: 0.695108]\n",
      "4969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006154] [C loss: 0.695108]\n",
      "4970 [D loss: 0.695111, acc.: 0.00%] [G loss: 0.005202] [C loss: 0.695105]\n",
      "4971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003514] [C loss: 0.695105]\n",
      "4972 [D loss: 0.695108, acc.: 0.00%] [G loss: 0.008643] [C loss: 0.695104]\n",
      "4973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007794] [C loss: 0.695104]\n",
      "4974 [D loss: 0.695107, acc.: 0.00%] [G loss: 0.006233] [C loss: 0.695103]\n",
      "4975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006609] [C loss: 0.695103]\n",
      "4976 [D loss: 0.695105, acc.: 0.00%] [G loss: 0.008774] [C loss: 0.695101]\n",
      "4977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006846] [C loss: 0.695101]\n",
      "4978 [D loss: 0.695104, acc.: 0.00%] [G loss: 0.009119] [C loss: 0.695098]\n",
      "4979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005529] [C loss: 0.695098]\n",
      "4980 [D loss: 0.695102, acc.: 0.00%] [G loss: 0.005574] [C loss: 0.695098]\n",
      "4981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005365] [C loss: 0.695098]\n",
      "4982 [D loss: 0.695101, acc.: 0.00%] [G loss: 0.005311] [C loss: 0.695096]\n",
      "4983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005395] [C loss: 0.695096]\n",
      "4984 [D loss: 0.695100, acc.: 0.00%] [G loss: 0.004431] [C loss: 0.695094]\n",
      "4985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006149] [C loss: 0.695094]\n",
      "4986 [D loss: 0.695098, acc.: 0.00%] [G loss: 0.004729] [C loss: 0.695092]\n",
      "4987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006725] [C loss: 0.695092]\n",
      "4988 [D loss: 0.695096, acc.: 0.00%] [G loss: 0.005546] [C loss: 0.695092]\n",
      "4989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010319] [C loss: 0.695092]\n",
      "4990 [D loss: 0.695094, acc.: 0.00%] [G loss: 0.005709] [C loss: 0.695092]\n",
      "4991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008463] [C loss: 0.695092]\n",
      "4992 [D loss: 0.695094, acc.: 0.00%] [G loss: 0.005569] [C loss: 0.695090]\n",
      "4993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007834] [C loss: 0.695090]\n",
      "4994 [D loss: 0.695092, acc.: 0.00%] [G loss: 0.008692] [C loss: 0.695088]\n",
      "4995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005704] [C loss: 0.695088]\n",
      "4996 [D loss: 0.695091, acc.: 0.00%] [G loss: 0.003871] [C loss: 0.695085]\n",
      "4997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003949] [C loss: 0.695085]\n",
      "4998 [D loss: 0.695088, acc.: 0.00%] [G loss: 0.004278] [C loss: 0.695085]\n",
      "4999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005645] [C loss: 0.695085]\n",
      "5000 [D loss: 0.695087, acc.: 0.00%] [G loss: 0.004157] [C loss: 0.695082]\n",
      "5001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005188] [C loss: 0.695082]\n",
      "5002 [D loss: 0.695085, acc.: 0.00%] [G loss: 0.005192] [C loss: 0.695081]\n",
      "5003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007347] [C loss: 0.695081]\n",
      "5004 [D loss: 0.695083, acc.: 0.00%] [G loss: 0.003590] [C loss: 0.695080]\n",
      "5005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006908] [C loss: 0.695080]\n",
      "5006 [D loss: 0.695083, acc.: 0.00%] [G loss: 0.003536] [C loss: 0.695079]\n",
      "5007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006589] [C loss: 0.695079]\n",
      "5008 [D loss: 0.695081, acc.: 0.00%] [G loss: 0.005221] [C loss: 0.695076]\n",
      "5009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004182] [C loss: 0.695076]\n",
      "5010 [D loss: 0.695079, acc.: 0.00%] [G loss: 0.004100] [C loss: 0.695075]\n",
      "5011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004983] [C loss: 0.695075]\n",
      "5012 [D loss: 0.695077, acc.: 0.00%] [G loss: 0.005653] [C loss: 0.695074]\n",
      "5013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.695074]\n",
      "5014 [D loss: 0.695077, acc.: 0.00%] [G loss: 0.008771] [C loss: 0.695072]\n",
      "5015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004370] [C loss: 0.695072]\n",
      "5016 [D loss: 0.695074, acc.: 0.00%] [G loss: 0.009183] [C loss: 0.695069]\n",
      "5017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007191] [C loss: 0.695069]\n",
      "5018 [D loss: 0.695073, acc.: 0.00%] [G loss: 0.004951] [C loss: 0.695067]\n",
      "5019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005140] [C loss: 0.695067]\n",
      "5020 [D loss: 0.695071, acc.: 0.00%] [G loss: 0.004443] [C loss: 0.695068]\n",
      "5021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005361] [C loss: 0.695068]\n",
      "5022 [D loss: 0.695071, acc.: 0.00%] [G loss: 0.005082] [C loss: 0.695066]\n",
      "5023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006070] [C loss: 0.695066]\n",
      "5024 [D loss: 0.695069, acc.: 0.00%] [G loss: 0.026962] [C loss: 0.695064]\n",
      "5025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.695064]\n",
      "5026 [D loss: 0.695067, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.695064]\n",
      "5027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005730] [C loss: 0.695064]\n",
      "5028 [D loss: 0.695067, acc.: 0.00%] [G loss: 0.004549] [C loss: 0.695061]\n",
      "5029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011858] [C loss: 0.695061]\n",
      "5030 [D loss: 0.695064, acc.: 0.00%] [G loss: 0.005477] [C loss: 0.695060]\n",
      "5031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005393] [C loss: 0.695060]\n",
      "5032 [D loss: 0.695063, acc.: 0.00%] [G loss: 0.005988] [C loss: 0.695058]\n",
      "5033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004563] [C loss: 0.695058]\n",
      "5034 [D loss: 0.695061, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.695057]\n",
      "5035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004332] [C loss: 0.695057]\n",
      "5036 [D loss: 0.695059, acc.: 0.00%] [G loss: 0.004322] [C loss: 0.695057]\n",
      "5037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007854] [C loss: 0.695057]\n",
      "5038 [D loss: 0.695059, acc.: 0.00%] [G loss: 0.005355] [C loss: 0.695054]\n",
      "5039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006066] [C loss: 0.695054]\n",
      "5040 [D loss: 0.695056, acc.: 0.00%] [G loss: 0.006961] [C loss: 0.695053]\n",
      "5041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007189] [C loss: 0.695053]\n",
      "5042 [D loss: 0.695054, acc.: 0.00%] [G loss: 0.009582] [C loss: 0.695052]\n",
      "5043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005585] [C loss: 0.695052]\n",
      "5044 [D loss: 0.695054, acc.: 0.00%] [G loss: 0.004459] [C loss: 0.695050]\n",
      "5045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006558] [C loss: 0.695050]\n",
      "5046 [D loss: 0.695052, acc.: 0.00%] [G loss: 0.007118] [C loss: 0.695048]\n",
      "5047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005384] [C loss: 0.695048]\n",
      "5048 [D loss: 0.695050, acc.: 0.00%] [G loss: 0.003968] [C loss: 0.695046]\n",
      "5049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005165] [C loss: 0.695046]\n",
      "5050 [D loss: 0.695049, acc.: 0.00%] [G loss: 0.003981] [C loss: 0.695044]\n",
      "5051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008508] [C loss: 0.695044]\n",
      "5052 [D loss: 0.695048, acc.: 0.00%] [G loss: 0.005253] [C loss: 0.695045]\n",
      "5053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005716] [C loss: 0.695045]\n",
      "5054 [D loss: 0.695047, acc.: 0.00%] [G loss: 0.007032] [C loss: 0.695041]\n",
      "5055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004678] [C loss: 0.695041]\n",
      "5056 [D loss: 0.695045, acc.: 0.00%] [G loss: 0.005716] [C loss: 0.695039]\n",
      "5057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003319] [C loss: 0.695039]\n",
      "5058 [D loss: 0.695042, acc.: 0.00%] [G loss: 0.005156] [C loss: 0.695037]\n",
      "5059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006826] [C loss: 0.695037]\n",
      "5060 [D loss: 0.695042, acc.: 0.00%] [G loss: 0.005461] [C loss: 0.695034]\n",
      "5061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005415] [C loss: 0.695034]\n",
      "5062 [D loss: 0.695039, acc.: 0.00%] [G loss: 0.006245] [C loss: 0.695035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009570] [C loss: 0.695035]\n",
      "5064 [D loss: 0.695038, acc.: 0.00%] [G loss: 0.005342] [C loss: 0.695033]\n",
      "5065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003737] [C loss: 0.695033]\n",
      "5066 [D loss: 0.695036, acc.: 0.00%] [G loss: 0.013990] [C loss: 0.695033]\n",
      "5067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007723] [C loss: 0.695033]\n",
      "5068 [D loss: 0.695036, acc.: 0.00%] [G loss: 0.005994] [C loss: 0.695032]\n",
      "5069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005642] [C loss: 0.695032]\n",
      "5070 [D loss: 0.695034, acc.: 0.00%] [G loss: 0.004948] [C loss: 0.695029]\n",
      "5071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005526] [C loss: 0.695029]\n",
      "5072 [D loss: 0.695032, acc.: 0.00%] [G loss: 0.005208] [C loss: 0.695028]\n",
      "5073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004183] [C loss: 0.695028]\n",
      "5074 [D loss: 0.695031, acc.: 0.00%] [G loss: 0.005544] [C loss: 0.695027]\n",
      "5075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004895] [C loss: 0.695027]\n",
      "5076 [D loss: 0.695030, acc.: 0.00%] [G loss: 0.004969] [C loss: 0.695024]\n",
      "5077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003823] [C loss: 0.695024]\n",
      "5078 [D loss: 0.695029, acc.: 0.00%] [G loss: 0.005995] [C loss: 0.695024]\n",
      "5079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005868] [C loss: 0.695024]\n",
      "5080 [D loss: 0.695027, acc.: 0.00%] [G loss: 0.005172] [C loss: 0.695021]\n",
      "5081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007466] [C loss: 0.695021]\n",
      "5082 [D loss: 0.695024, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.695020]\n",
      "5083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005056] [C loss: 0.695020]\n",
      "5084 [D loss: 0.695023, acc.: 0.00%] [G loss: 0.007332] [C loss: 0.695020]\n",
      "5085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004966] [C loss: 0.695020]\n",
      "5086 [D loss: 0.695022, acc.: 0.00%] [G loss: 0.009221] [C loss: 0.695018]\n",
      "5087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005432] [C loss: 0.695018]\n",
      "5088 [D loss: 0.695020, acc.: 0.00%] [G loss: 0.006283] [C loss: 0.695014]\n",
      "5089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004796] [C loss: 0.695014]\n",
      "5090 [D loss: 0.695018, acc.: 0.00%] [G loss: 0.004625] [C loss: 0.695016]\n",
      "5091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005155] [C loss: 0.695016]\n",
      "5092 [D loss: 0.695018, acc.: 0.00%] [G loss: 0.004097] [C loss: 0.695014]\n",
      "5093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011009] [C loss: 0.695014]\n",
      "5094 [D loss: 0.695015, acc.: 0.00%] [G loss: 0.005440] [C loss: 0.695011]\n",
      "5095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006254] [C loss: 0.695011]\n",
      "5096 [D loss: 0.695014, acc.: 0.00%] [G loss: 0.007373] [C loss: 0.695008]\n",
      "5097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003887] [C loss: 0.695008]\n",
      "5098 [D loss: 0.695012, acc.: 0.00%] [G loss: 0.006906] [C loss: 0.695007]\n",
      "5099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015476] [C loss: 0.695007]\n",
      "5100 [D loss: 0.695011, acc.: 0.00%] [G loss: 0.005905] [C loss: 0.695006]\n",
      "5101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005481] [C loss: 0.695006]\n",
      "5102 [D loss: 0.695009, acc.: 0.00%] [G loss: 0.006813] [C loss: 0.695006]\n",
      "5103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006109] [C loss: 0.695006]\n",
      "5104 [D loss: 0.695008, acc.: 0.00%] [G loss: 0.005252] [C loss: 0.695002]\n",
      "5105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003263] [C loss: 0.695002]\n",
      "5106 [D loss: 0.695006, acc.: 0.00%] [G loss: 0.004407] [C loss: 0.695002]\n",
      "5107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005438] [C loss: 0.695002]\n",
      "5108 [D loss: 0.695005, acc.: 0.00%] [G loss: 0.006673] [C loss: 0.695001]\n",
      "5109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003931] [C loss: 0.695001]\n",
      "5110 [D loss: 0.695004, acc.: 0.00%] [G loss: 0.004127] [C loss: 0.694997]\n",
      "5111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003780] [C loss: 0.694997]\n",
      "5112 [D loss: 0.695001, acc.: 0.00%] [G loss: 0.006566] [C loss: 0.694998]\n",
      "5113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004750] [C loss: 0.694998]\n",
      "5114 [D loss: 0.695000, acc.: 0.00%] [G loss: 0.006825] [C loss: 0.694996]\n",
      "5115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005218] [C loss: 0.694996]\n",
      "5116 [D loss: 0.694999, acc.: 0.00%] [G loss: 0.004843] [C loss: 0.694996]\n",
      "5117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004324] [C loss: 0.694996]\n",
      "5118 [D loss: 0.694998, acc.: 0.00%] [G loss: 0.003992] [C loss: 0.694993]\n",
      "5119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013304] [C loss: 0.694993]\n",
      "5120 [D loss: 0.694996, acc.: 0.00%] [G loss: 0.004761] [C loss: 0.694992]\n",
      "5121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004437] [C loss: 0.694992]\n",
      "5122 [D loss: 0.694995, acc.: 0.00%] [G loss: 0.007136] [C loss: 0.694990]\n",
      "5123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004404] [C loss: 0.694990]\n",
      "5124 [D loss: 0.694992, acc.: 0.00%] [G loss: 0.005669] [C loss: 0.694988]\n",
      "5125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004656] [C loss: 0.694988]\n",
      "5126 [D loss: 0.694991, acc.: 0.00%] [G loss: 0.004405] [C loss: 0.694987]\n",
      "5127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004656] [C loss: 0.694987]\n",
      "5128 [D loss: 0.694990, acc.: 0.00%] [G loss: 0.005262] [C loss: 0.694987]\n",
      "5129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005246] [C loss: 0.694987]\n",
      "5130 [D loss: 0.694989, acc.: 0.00%] [G loss: 0.004645] [C loss: 0.694984]\n",
      "5131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006560] [C loss: 0.694984]\n",
      "5132 [D loss: 0.694987, acc.: 0.00%] [G loss: 0.003835] [C loss: 0.694982]\n",
      "5133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004372] [C loss: 0.694982]\n",
      "5134 [D loss: 0.694985, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.694980]\n",
      "5135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008530] [C loss: 0.694980]\n",
      "5136 [D loss: 0.694984, acc.: 0.00%] [G loss: 0.007096] [C loss: 0.694979]\n",
      "5137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004581] [C loss: 0.694979]\n",
      "5138 [D loss: 0.694982, acc.: 0.00%] [G loss: 0.006450] [C loss: 0.694978]\n",
      "5139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008412] [C loss: 0.694978]\n",
      "5140 [D loss: 0.694980, acc.: 0.00%] [G loss: 0.004143] [C loss: 0.694974]\n",
      "5141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.694974]\n",
      "5142 [D loss: 0.694979, acc.: 0.00%] [G loss: 0.006937] [C loss: 0.694974]\n",
      "5143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006985] [C loss: 0.694974]\n",
      "5144 [D loss: 0.694977, acc.: 0.00%] [G loss: 0.005646] [C loss: 0.694975]\n",
      "5145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005951] [C loss: 0.694975]\n",
      "5146 [D loss: 0.694977, acc.: 0.00%] [G loss: 0.004508] [C loss: 0.694972]\n",
      "5147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008277] [C loss: 0.694972]\n",
      "5148 [D loss: 0.694975, acc.: 0.00%] [G loss: 0.004944] [C loss: 0.694969]\n",
      "5149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006877] [C loss: 0.694969]\n",
      "5150 [D loss: 0.694973, acc.: 0.00%] [G loss: 0.006360] [C loss: 0.694970]\n",
      "5151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004835] [C loss: 0.694970]\n",
      "5152 [D loss: 0.694971, acc.: 0.00%] [G loss: 0.005368] [C loss: 0.694967]\n",
      "5153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.694967]\n",
      "5154 [D loss: 0.694970, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.694965]\n",
      "5155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007143] [C loss: 0.694965]\n",
      "5156 [D loss: 0.694968, acc.: 0.00%] [G loss: 0.006804] [C loss: 0.694963]\n",
      "5157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004717] [C loss: 0.694963]\n",
      "5158 [D loss: 0.694967, acc.: 0.00%] [G loss: 0.008471] [C loss: 0.694963]\n",
      "5159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005030] [C loss: 0.694963]\n",
      "5160 [D loss: 0.694965, acc.: 0.00%] [G loss: 0.009186] [C loss: 0.694960]\n",
      "5161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004840] [C loss: 0.694960]\n",
      "5162 [D loss: 0.694962, acc.: 0.00%] [G loss: 0.004754] [C loss: 0.694960]\n",
      "5163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004902] [C loss: 0.694960]\n",
      "5164 [D loss: 0.694962, acc.: 0.00%] [G loss: 0.004727] [C loss: 0.694957]\n",
      "5165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006387] [C loss: 0.694957]\n",
      "5166 [D loss: 0.694960, acc.: 0.00%] [G loss: 0.005477] [C loss: 0.694955]\n",
      "5167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006099] [C loss: 0.694955]\n",
      "5168 [D loss: 0.694959, acc.: 0.00%] [G loss: 0.007274] [C loss: 0.694955]\n",
      "5169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005496] [C loss: 0.694955]\n",
      "5170 [D loss: 0.694957, acc.: 0.00%] [G loss: 0.005239] [C loss: 0.694954]\n",
      "5171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006215] [C loss: 0.694954]\n",
      "5172 [D loss: 0.694956, acc.: 0.00%] [G loss: 0.006796] [C loss: 0.694952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007550] [C loss: 0.694952]\n",
      "5174 [D loss: 0.694955, acc.: 0.00%] [G loss: 0.005933] [C loss: 0.694949]\n",
      "5175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.002879] [C loss: 0.694949]\n",
      "5176 [D loss: 0.694951, acc.: 0.00%] [G loss: 0.004515] [C loss: 0.694949]\n",
      "5177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006747] [C loss: 0.694949]\n",
      "5178 [D loss: 0.694951, acc.: 0.00%] [G loss: 0.004731] [C loss: 0.694946]\n",
      "5179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008450] [C loss: 0.694946]\n",
      "5180 [D loss: 0.694949, acc.: 0.00%] [G loss: 0.004345] [C loss: 0.694945]\n",
      "5181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005916] [C loss: 0.694945]\n",
      "5182 [D loss: 0.694949, acc.: 0.00%] [G loss: 0.004130] [C loss: 0.694943]\n",
      "5183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.694943]\n",
      "5184 [D loss: 0.694946, acc.: 0.00%] [G loss: 0.003784] [C loss: 0.694943]\n",
      "5185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004626] [C loss: 0.694943]\n",
      "5186 [D loss: 0.694946, acc.: 0.00%] [G loss: 0.006252] [C loss: 0.694942]\n",
      "5187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006180] [C loss: 0.694942]\n",
      "5188 [D loss: 0.694945, acc.: 0.00%] [G loss: 0.005685] [C loss: 0.694939]\n",
      "5189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006154] [C loss: 0.694939]\n",
      "5190 [D loss: 0.694942, acc.: 0.00%] [G loss: 0.004996] [C loss: 0.694938]\n",
      "5191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004996] [C loss: 0.694938]\n",
      "5192 [D loss: 0.694941, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.694938]\n",
      "5193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008875] [C loss: 0.694938]\n",
      "5194 [D loss: 0.694941, acc.: 0.00%] [G loss: 0.005653] [C loss: 0.694935]\n",
      "5195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006223] [C loss: 0.694935]\n",
      "5196 [D loss: 0.694938, acc.: 0.00%] [G loss: 0.008633] [C loss: 0.694933]\n",
      "5197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006262] [C loss: 0.694933]\n",
      "5198 [D loss: 0.694936, acc.: 0.00%] [G loss: 0.004490] [C loss: 0.694933]\n",
      "5199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004134] [C loss: 0.694933]\n",
      "5200 [D loss: 0.694935, acc.: 0.00%] [G loss: 0.007525] [C loss: 0.694931]\n",
      "5201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009709] [C loss: 0.694931]\n",
      "5202 [D loss: 0.694933, acc.: 0.00%] [G loss: 0.004320] [C loss: 0.694929]\n",
      "5203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006532] [C loss: 0.694929]\n",
      "5204 [D loss: 0.694932, acc.: 0.00%] [G loss: 0.005317] [C loss: 0.694927]\n",
      "5205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005675] [C loss: 0.694927]\n",
      "5206 [D loss: 0.694930, acc.: 0.00%] [G loss: 0.006262] [C loss: 0.694926]\n",
      "5207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.694926]\n",
      "5208 [D loss: 0.694929, acc.: 0.00%] [G loss: 0.003742] [C loss: 0.694924]\n",
      "5209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006810] [C loss: 0.694924]\n",
      "5210 [D loss: 0.694928, acc.: 0.00%] [G loss: 0.004554] [C loss: 0.694922]\n",
      "5211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005325] [C loss: 0.694922]\n",
      "5212 [D loss: 0.694925, acc.: 0.00%] [G loss: 0.006491] [C loss: 0.694920]\n",
      "5213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010918] [C loss: 0.694920]\n",
      "5214 [D loss: 0.694923, acc.: 0.00%] [G loss: 0.004328] [C loss: 0.694919]\n",
      "5215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005884] [C loss: 0.694919]\n",
      "5216 [D loss: 0.694922, acc.: 0.00%] [G loss: 0.006312] [C loss: 0.694918]\n",
      "5217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.694918]\n",
      "5218 [D loss: 0.694920, acc.: 0.00%] [G loss: 0.004678] [C loss: 0.694915]\n",
      "5219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003802] [C loss: 0.694915]\n",
      "5220 [D loss: 0.694919, acc.: 0.00%] [G loss: 0.003879] [C loss: 0.694913]\n",
      "5221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.694913]\n",
      "5222 [D loss: 0.694916, acc.: 0.00%] [G loss: 0.012116] [C loss: 0.694913]\n",
      "5223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005107] [C loss: 0.694913]\n",
      "5224 [D loss: 0.694916, acc.: 0.00%] [G loss: 0.003558] [C loss: 0.694911]\n",
      "5225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006488] [C loss: 0.694911]\n",
      "5226 [D loss: 0.694915, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.694912]\n",
      "5227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005434] [C loss: 0.694912]\n",
      "5228 [D loss: 0.694914, acc.: 0.00%] [G loss: 0.005002] [C loss: 0.694909]\n",
      "5229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005027] [C loss: 0.694909]\n",
      "5230 [D loss: 0.694912, acc.: 0.00%] [G loss: 0.005465] [C loss: 0.694908]\n",
      "5231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004278] [C loss: 0.694908]\n",
      "5232 [D loss: 0.694911, acc.: 0.00%] [G loss: 0.004571] [C loss: 0.694905]\n",
      "5233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011819] [C loss: 0.694905]\n",
      "5234 [D loss: 0.694908, acc.: 0.00%] [G loss: 0.005316] [C loss: 0.694903]\n",
      "5235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.694903]\n",
      "5236 [D loss: 0.694907, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.694904]\n",
      "5237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005227] [C loss: 0.694904]\n",
      "5238 [D loss: 0.694906, acc.: 0.00%] [G loss: 0.005264] [C loss: 0.694902]\n",
      "5239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004615] [C loss: 0.694902]\n",
      "5240 [D loss: 0.694905, acc.: 0.00%] [G loss: 0.004745] [C loss: 0.694899]\n",
      "5241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005538] [C loss: 0.694899]\n",
      "5242 [D loss: 0.694903, acc.: 0.00%] [G loss: 0.004713] [C loss: 0.694898]\n",
      "5243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004232] [C loss: 0.694898]\n",
      "5244 [D loss: 0.694901, acc.: 0.00%] [G loss: 0.006566] [C loss: 0.694897]\n",
      "5245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007438] [C loss: 0.694897]\n",
      "5246 [D loss: 0.694899, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.694895]\n",
      "5247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005372] [C loss: 0.694895]\n",
      "5248 [D loss: 0.694898, acc.: 0.00%] [G loss: 0.016046] [C loss: 0.694894]\n",
      "5249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004766] [C loss: 0.694894]\n",
      "5250 [D loss: 0.694897, acc.: 0.00%] [G loss: 0.005206] [C loss: 0.694891]\n",
      "5251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011641] [C loss: 0.694891]\n",
      "5252 [D loss: 0.694895, acc.: 0.00%] [G loss: 0.003950] [C loss: 0.694890]\n",
      "5253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003727] [C loss: 0.694890]\n",
      "5254 [D loss: 0.694894, acc.: 0.00%] [G loss: 0.008428] [C loss: 0.694889]\n",
      "5255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004498] [C loss: 0.694889]\n",
      "5256 [D loss: 0.694892, acc.: 0.00%] [G loss: 0.003736] [C loss: 0.694888]\n",
      "5257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005986] [C loss: 0.694888]\n",
      "5258 [D loss: 0.694891, acc.: 0.00%] [G loss: 0.007314] [C loss: 0.694885]\n",
      "5259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005207] [C loss: 0.694885]\n",
      "5260 [D loss: 0.694889, acc.: 0.00%] [G loss: 0.007766] [C loss: 0.694884]\n",
      "5261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004601] [C loss: 0.694884]\n",
      "5262 [D loss: 0.694887, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.694882]\n",
      "5263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005263] [C loss: 0.694882]\n",
      "5264 [D loss: 0.694885, acc.: 0.00%] [G loss: 0.004227] [C loss: 0.694882]\n",
      "5265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006494] [C loss: 0.694882]\n",
      "5266 [D loss: 0.694885, acc.: 0.00%] [G loss: 0.006867] [C loss: 0.694880]\n",
      "5267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006787] [C loss: 0.694880]\n",
      "5268 [D loss: 0.694882, acc.: 0.00%] [G loss: 0.004884] [C loss: 0.694878]\n",
      "5269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004827] [C loss: 0.694878]\n",
      "5270 [D loss: 0.694881, acc.: 0.00%] [G loss: 0.005263] [C loss: 0.694876]\n",
      "5271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003789] [C loss: 0.694876]\n",
      "5272 [D loss: 0.694879, acc.: 0.00%] [G loss: 0.003946] [C loss: 0.694876]\n",
      "5273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004330] [C loss: 0.694876]\n",
      "5274 [D loss: 0.694879, acc.: 0.00%] [G loss: 0.005557] [C loss: 0.694875]\n",
      "5275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005412] [C loss: 0.694875]\n",
      "5276 [D loss: 0.694877, acc.: 0.00%] [G loss: 0.006562] [C loss: 0.694872]\n",
      "5277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005395] [C loss: 0.694872]\n",
      "5278 [D loss: 0.694875, acc.: 0.00%] [G loss: 0.004812] [C loss: 0.694872]\n",
      "5279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006190] [C loss: 0.694872]\n",
      "5280 [D loss: 0.694874, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.694868]\n",
      "5281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005986] [C loss: 0.694868]\n",
      "5282 [D loss: 0.694872, acc.: 0.00%] [G loss: 0.006005] [C loss: 0.694868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007104] [C loss: 0.694868]\n",
      "5284 [D loss: 0.694871, acc.: 0.00%] [G loss: 0.005068] [C loss: 0.694869]\n",
      "5285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006032] [C loss: 0.694869]\n",
      "5286 [D loss: 0.694870, acc.: 0.00%] [G loss: 0.004790] [C loss: 0.694865]\n",
      "5287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005731] [C loss: 0.694865]\n",
      "5288 [D loss: 0.694868, acc.: 0.00%] [G loss: 0.004385] [C loss: 0.694864]\n",
      "5289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014100] [C loss: 0.694864]\n",
      "5290 [D loss: 0.694867, acc.: 0.00%] [G loss: 0.006124] [C loss: 0.694864]\n",
      "5291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006282] [C loss: 0.694864]\n",
      "5292 [D loss: 0.694865, acc.: 0.00%] [G loss: 0.004091] [C loss: 0.694860]\n",
      "5293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004920] [C loss: 0.694860]\n",
      "5294 [D loss: 0.694863, acc.: 0.00%] [G loss: 0.006441] [C loss: 0.694860]\n",
      "5295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005419] [C loss: 0.694860]\n",
      "5296 [D loss: 0.694862, acc.: 0.00%] [G loss: 0.016021] [C loss: 0.694856]\n",
      "5297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003626] [C loss: 0.694856]\n",
      "5298 [D loss: 0.694859, acc.: 0.00%] [G loss: 0.005360] [C loss: 0.694855]\n",
      "5299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004285] [C loss: 0.694855]\n",
      "5300 [D loss: 0.694858, acc.: 0.00%] [G loss: 0.004143] [C loss: 0.694855]\n",
      "5301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006659] [C loss: 0.694855]\n",
      "5302 [D loss: 0.694858, acc.: 0.00%] [G loss: 0.005224] [C loss: 0.694852]\n",
      "5303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005299] [C loss: 0.694852]\n",
      "5304 [D loss: 0.694855, acc.: 0.00%] [G loss: 0.004037] [C loss: 0.694851]\n",
      "5305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.694851]\n",
      "5306 [D loss: 0.694854, acc.: 0.00%] [G loss: 0.004583] [C loss: 0.694850]\n",
      "5307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005270] [C loss: 0.694850]\n",
      "5308 [D loss: 0.694853, acc.: 0.00%] [G loss: 0.006594] [C loss: 0.694847]\n",
      "5309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006739] [C loss: 0.694847]\n",
      "5310 [D loss: 0.694851, acc.: 0.00%] [G loss: 0.004001] [C loss: 0.694847]\n",
      "5311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010418] [C loss: 0.694847]\n",
      "5312 [D loss: 0.694849, acc.: 0.00%] [G loss: 0.004573] [C loss: 0.694844]\n",
      "5313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009548] [C loss: 0.694844]\n",
      "5314 [D loss: 0.694847, acc.: 0.00%] [G loss: 0.006920] [C loss: 0.694843]\n",
      "5315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005911] [C loss: 0.694843]\n",
      "5316 [D loss: 0.694847, acc.: 0.00%] [G loss: 0.006864] [C loss: 0.694842]\n",
      "5317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005123] [C loss: 0.694842]\n",
      "5318 [D loss: 0.694845, acc.: 0.00%] [G loss: 0.004525] [C loss: 0.694843]\n",
      "5319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004542] [C loss: 0.694843]\n",
      "5320 [D loss: 0.694845, acc.: 0.00%] [G loss: 0.005041] [C loss: 0.694839]\n",
      "5321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005983] [C loss: 0.694839]\n",
      "5322 [D loss: 0.694842, acc.: 0.00%] [G loss: 0.004404] [C loss: 0.694837]\n",
      "5323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014460] [C loss: 0.694837]\n",
      "5324 [D loss: 0.694840, acc.: 0.00%] [G loss: 0.004881] [C loss: 0.694835]\n",
      "5325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007261] [C loss: 0.694835]\n",
      "5326 [D loss: 0.694839, acc.: 0.00%] [G loss: 0.010097] [C loss: 0.694833]\n",
      "5327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005219] [C loss: 0.694833]\n",
      "5328 [D loss: 0.694836, acc.: 0.00%] [G loss: 0.004775] [C loss: 0.694833]\n",
      "5329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006662] [C loss: 0.694833]\n",
      "5330 [D loss: 0.694836, acc.: 0.00%] [G loss: 0.007562] [C loss: 0.694831]\n",
      "5331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006783] [C loss: 0.694831]\n",
      "5332 [D loss: 0.694834, acc.: 0.00%] [G loss: 0.005122] [C loss: 0.694830]\n",
      "5333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005210] [C loss: 0.694830]\n",
      "5334 [D loss: 0.694832, acc.: 0.00%] [G loss: 0.006131] [C loss: 0.694829]\n",
      "5335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006285] [C loss: 0.694829]\n",
      "5336 [D loss: 0.694832, acc.: 0.00%] [G loss: 0.005002] [C loss: 0.694828]\n",
      "5337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005829] [C loss: 0.694828]\n",
      "5338 [D loss: 0.694831, acc.: 0.00%] [G loss: 0.007615] [C loss: 0.694825]\n",
      "5339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005393] [C loss: 0.694825]\n",
      "5340 [D loss: 0.694828, acc.: 0.00%] [G loss: 0.005302] [C loss: 0.694824]\n",
      "5341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005329] [C loss: 0.694824]\n",
      "5342 [D loss: 0.694827, acc.: 0.00%] [G loss: 0.005657] [C loss: 0.694823]\n",
      "5343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004197] [C loss: 0.694823]\n",
      "5344 [D loss: 0.694826, acc.: 0.00%] [G loss: 0.006652] [C loss: 0.694820]\n",
      "5345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006176] [C loss: 0.694820]\n",
      "5346 [D loss: 0.694824, acc.: 0.00%] [G loss: 0.005039] [C loss: 0.694819]\n",
      "5347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011633] [C loss: 0.694819]\n",
      "5348 [D loss: 0.694822, acc.: 0.00%] [G loss: 0.004911] [C loss: 0.694818]\n",
      "5349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005875] [C loss: 0.694818]\n",
      "5350 [D loss: 0.694822, acc.: 0.00%] [G loss: 0.005637] [C loss: 0.694816]\n",
      "5351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005130] [C loss: 0.694816]\n",
      "5352 [D loss: 0.694819, acc.: 0.00%] [G loss: 0.006940] [C loss: 0.694814]\n",
      "5353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005851] [C loss: 0.694814]\n",
      "5354 [D loss: 0.694817, acc.: 0.00%] [G loss: 0.003702] [C loss: 0.694815]\n",
      "5355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006849] [C loss: 0.694815]\n",
      "5356 [D loss: 0.694817, acc.: 0.00%] [G loss: 0.004686] [C loss: 0.694811]\n",
      "5357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013297] [C loss: 0.694811]\n",
      "5358 [D loss: 0.694814, acc.: 0.00%] [G loss: 0.005207] [C loss: 0.694810]\n",
      "5359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005218] [C loss: 0.694810]\n",
      "5360 [D loss: 0.694813, acc.: 0.00%] [G loss: 0.006152] [C loss: 0.694809]\n",
      "5361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008590] [C loss: 0.694809]\n",
      "5362 [D loss: 0.694811, acc.: 0.00%] [G loss: 0.004858] [C loss: 0.694806]\n",
      "5363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006682] [C loss: 0.694806]\n",
      "5364 [D loss: 0.694810, acc.: 0.00%] [G loss: 0.004273] [C loss: 0.694806]\n",
      "5365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003869] [C loss: 0.694806]\n",
      "5366 [D loss: 0.694809, acc.: 0.00%] [G loss: 0.006609] [C loss: 0.694803]\n",
      "5367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007997] [C loss: 0.694803]\n",
      "5368 [D loss: 0.694807, acc.: 0.00%] [G loss: 0.005010] [C loss: 0.694805]\n",
      "5369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005403] [C loss: 0.694805]\n",
      "5370 [D loss: 0.694806, acc.: 0.00%] [G loss: 0.007751] [C loss: 0.694803]\n",
      "5371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006962] [C loss: 0.694803]\n",
      "5372 [D loss: 0.694804, acc.: 0.00%] [G loss: 0.005945] [C loss: 0.694799]\n",
      "5373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004687] [C loss: 0.694799]\n",
      "5374 [D loss: 0.694802, acc.: 0.00%] [G loss: 0.004825] [C loss: 0.694798]\n",
      "5375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005022] [C loss: 0.694798]\n",
      "5376 [D loss: 0.694801, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.694798]\n",
      "5377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005802] [C loss: 0.694798]\n",
      "5378 [D loss: 0.694799, acc.: 0.00%] [G loss: 0.007434] [C loss: 0.694796]\n",
      "5379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007557] [C loss: 0.694796]\n",
      "5380 [D loss: 0.694798, acc.: 0.00%] [G loss: 0.003462] [C loss: 0.694795]\n",
      "5381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006590] [C loss: 0.694795]\n",
      "5382 [D loss: 0.694797, acc.: 0.00%] [G loss: 0.006708] [C loss: 0.694793]\n",
      "5383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006260] [C loss: 0.694793]\n",
      "5384 [D loss: 0.694795, acc.: 0.00%] [G loss: 0.006186] [C loss: 0.694791]\n",
      "5385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004652] [C loss: 0.694791]\n",
      "5386 [D loss: 0.694794, acc.: 0.00%] [G loss: 0.013235] [C loss: 0.694789]\n",
      "5387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004856] [C loss: 0.694789]\n",
      "5388 [D loss: 0.694791, acc.: 0.00%] [G loss: 0.005478] [C loss: 0.694787]\n",
      "5389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005888] [C loss: 0.694787]\n",
      "5390 [D loss: 0.694789, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.694786]\n",
      "5391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006292] [C loss: 0.694786]\n",
      "5392 [D loss: 0.694789, acc.: 0.00%] [G loss: 0.004977] [C loss: 0.694783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004353] [C loss: 0.694783]\n",
      "5394 [D loss: 0.694786, acc.: 0.00%] [G loss: 0.005965] [C loss: 0.694782]\n",
      "5395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004935] [C loss: 0.694782]\n",
      "5396 [D loss: 0.694786, acc.: 0.00%] [G loss: 0.004481] [C loss: 0.694779]\n",
      "5397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004794] [C loss: 0.694779]\n",
      "5398 [D loss: 0.694783, acc.: 0.00%] [G loss: 0.004311] [C loss: 0.694779]\n",
      "5399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006728] [C loss: 0.694779]\n",
      "5400 [D loss: 0.694782, acc.: 0.00%] [G loss: 0.008806] [C loss: 0.694778]\n",
      "5401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005301] [C loss: 0.694778]\n",
      "5402 [D loss: 0.694781, acc.: 0.00%] [G loss: 0.011472] [C loss: 0.694777]\n",
      "5403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004066] [C loss: 0.694777]\n",
      "5404 [D loss: 0.694778, acc.: 0.00%] [G loss: 0.013412] [C loss: 0.694775]\n",
      "5405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004409] [C loss: 0.694775]\n",
      "5406 [D loss: 0.694778, acc.: 0.00%] [G loss: 0.005842] [C loss: 0.694772]\n",
      "5407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005541] [C loss: 0.694772]\n",
      "5408 [D loss: 0.694775, acc.: 0.00%] [G loss: 0.007414] [C loss: 0.694771]\n",
      "5409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004288] [C loss: 0.694771]\n",
      "5410 [D loss: 0.694773, acc.: 0.00%] [G loss: 0.005286] [C loss: 0.694769]\n",
      "5411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004923] [C loss: 0.694769]\n",
      "5412 [D loss: 0.694773, acc.: 0.00%] [G loss: 0.004989] [C loss: 0.694769]\n",
      "5413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006427] [C loss: 0.694769]\n",
      "5414 [D loss: 0.694773, acc.: 0.00%] [G loss: 0.004158] [C loss: 0.694766]\n",
      "5415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003961] [C loss: 0.694766]\n",
      "5416 [D loss: 0.694770, acc.: 0.00%] [G loss: 0.005677] [C loss: 0.694766]\n",
      "5417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.694766]\n",
      "5418 [D loss: 0.694769, acc.: 0.00%] [G loss: 0.003635] [C loss: 0.694763]\n",
      "5419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004858] [C loss: 0.694763]\n",
      "5420 [D loss: 0.694766, acc.: 0.00%] [G loss: 0.006016] [C loss: 0.694762]\n",
      "5421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005687] [C loss: 0.694762]\n",
      "5422 [D loss: 0.694766, acc.: 0.00%] [G loss: 0.004059] [C loss: 0.694760]\n",
      "5423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005462] [C loss: 0.694760]\n",
      "5424 [D loss: 0.694763, acc.: 0.00%] [G loss: 0.025429] [C loss: 0.694760]\n",
      "5425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004164] [C loss: 0.694760]\n",
      "5426 [D loss: 0.694763, acc.: 0.00%] [G loss: 0.005227] [C loss: 0.694758]\n",
      "5427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005535] [C loss: 0.694758]\n",
      "5428 [D loss: 0.694760, acc.: 0.00%] [G loss: 0.006420] [C loss: 0.694757]\n",
      "5429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004423] [C loss: 0.694757]\n",
      "5430 [D loss: 0.694759, acc.: 0.00%] [G loss: 0.005542] [C loss: 0.694755]\n",
      "5431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020189] [C loss: 0.694755]\n",
      "5432 [D loss: 0.694758, acc.: 0.00%] [G loss: 0.006080] [C loss: 0.694754]\n",
      "5433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005711] [C loss: 0.694754]\n",
      "5434 [D loss: 0.694756, acc.: 0.00%] [G loss: 0.006934] [C loss: 0.694752]\n",
      "5435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006367] [C loss: 0.694752]\n",
      "5436 [D loss: 0.694755, acc.: 0.00%] [G loss: 0.005036] [C loss: 0.694751]\n",
      "5437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008612] [C loss: 0.694751]\n",
      "5438 [D loss: 0.694753, acc.: 0.00%] [G loss: 0.005798] [C loss: 0.694749]\n",
      "5439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003384] [C loss: 0.694749]\n",
      "5440 [D loss: 0.694751, acc.: 0.00%] [G loss: 0.004611] [C loss: 0.694747]\n",
      "5441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004877] [C loss: 0.694747]\n",
      "5442 [D loss: 0.694749, acc.: 0.00%] [G loss: 0.004723] [C loss: 0.694745]\n",
      "5443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005744] [C loss: 0.694745]\n",
      "5444 [D loss: 0.694748, acc.: 0.00%] [G loss: 0.005180] [C loss: 0.694745]\n",
      "5445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003750] [C loss: 0.694745]\n",
      "5446 [D loss: 0.694747, acc.: 0.00%] [G loss: 0.006199] [C loss: 0.694742]\n",
      "5447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003985] [C loss: 0.694742]\n",
      "5448 [D loss: 0.694746, acc.: 0.00%] [G loss: 0.008521] [C loss: 0.694742]\n",
      "5449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007077] [C loss: 0.694742]\n",
      "5450 [D loss: 0.694745, acc.: 0.00%] [G loss: 0.004930] [C loss: 0.694740]\n",
      "5451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014520] [C loss: 0.694740]\n",
      "5452 [D loss: 0.694742, acc.: 0.00%] [G loss: 0.006835] [C loss: 0.694738]\n",
      "5453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004566] [C loss: 0.694738]\n",
      "5454 [D loss: 0.694740, acc.: 0.00%] [G loss: 0.019595] [C loss: 0.694737]\n",
      "5455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006094] [C loss: 0.694737]\n",
      "5456 [D loss: 0.694739, acc.: 0.00%] [G loss: 0.005327] [C loss: 0.694736]\n",
      "5457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005362] [C loss: 0.694736]\n",
      "5458 [D loss: 0.694738, acc.: 0.00%] [G loss: 0.003886] [C loss: 0.694732]\n",
      "5459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005952] [C loss: 0.694732]\n",
      "5460 [D loss: 0.694735, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.694731]\n",
      "5461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007119] [C loss: 0.694731]\n",
      "5462 [D loss: 0.694733, acc.: 0.00%] [G loss: 0.004732] [C loss: 0.694730]\n",
      "5463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004159] [C loss: 0.694730]\n",
      "5464 [D loss: 0.694733, acc.: 0.00%] [G loss: 0.006242] [C loss: 0.694729]\n",
      "5465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006870] [C loss: 0.694729]\n",
      "5466 [D loss: 0.694732, acc.: 0.00%] [G loss: 0.006493] [C loss: 0.694727]\n",
      "5467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005957] [C loss: 0.694727]\n",
      "5468 [D loss: 0.694731, acc.: 0.00%] [G loss: 0.004853] [C loss: 0.694726]\n",
      "5469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005087] [C loss: 0.694726]\n",
      "5470 [D loss: 0.694729, acc.: 0.00%] [G loss: 0.004990] [C loss: 0.694726]\n",
      "5471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014183] [C loss: 0.694726]\n",
      "5472 [D loss: 0.694727, acc.: 0.00%] [G loss: 0.003991] [C loss: 0.694722]\n",
      "5473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004740] [C loss: 0.694722]\n",
      "5474 [D loss: 0.694726, acc.: 0.00%] [G loss: 0.005592] [C loss: 0.694720]\n",
      "5475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004648] [C loss: 0.694720]\n",
      "5476 [D loss: 0.694723, acc.: 0.00%] [G loss: 0.005258] [C loss: 0.694720]\n",
      "5477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012584] [C loss: 0.694720]\n",
      "5478 [D loss: 0.694723, acc.: 0.00%] [G loss: 0.023530] [C loss: 0.694717]\n",
      "5479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003874] [C loss: 0.694717]\n",
      "5480 [D loss: 0.694721, acc.: 0.00%] [G loss: 0.008929] [C loss: 0.694718]\n",
      "5481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005145] [C loss: 0.694718]\n",
      "5482 [D loss: 0.694721, acc.: 0.00%] [G loss: 0.006808] [C loss: 0.694715]\n",
      "5483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005819] [C loss: 0.694715]\n",
      "5484 [D loss: 0.694718, acc.: 0.00%] [G loss: 0.008997] [C loss: 0.694715]\n",
      "5485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010155] [C loss: 0.694715]\n",
      "5486 [D loss: 0.694718, acc.: 0.00%] [G loss: 0.006261] [C loss: 0.694711]\n",
      "5487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006985] [C loss: 0.694711]\n",
      "5488 [D loss: 0.694715, acc.: 0.00%] [G loss: 0.007277] [C loss: 0.694711]\n",
      "5489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004810] [C loss: 0.694711]\n",
      "5490 [D loss: 0.694714, acc.: 0.00%] [G loss: 0.014979] [C loss: 0.694708]\n",
      "5491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005450] [C loss: 0.694708]\n",
      "5492 [D loss: 0.694712, acc.: 0.00%] [G loss: 0.004796] [C loss: 0.694708]\n",
      "5493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005364] [C loss: 0.694708]\n",
      "5494 [D loss: 0.694712, acc.: 0.00%] [G loss: 0.007848] [C loss: 0.694709]\n",
      "5495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006468] [C loss: 0.694709]\n",
      "5496 [D loss: 0.694710, acc.: 0.00%] [G loss: 0.005111] [C loss: 0.694706]\n",
      "5497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005570] [C loss: 0.694706]\n",
      "5498 [D loss: 0.694708, acc.: 0.00%] [G loss: 0.003988] [C loss: 0.694704]\n",
      "5499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003738] [C loss: 0.694704]\n",
      "5500 [D loss: 0.694706, acc.: 0.00%] [G loss: 0.004437] [C loss: 0.694704]\n",
      "5501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007138] [C loss: 0.694704]\n",
      "5502 [D loss: 0.694705, acc.: 0.00%] [G loss: 0.005707] [C loss: 0.694701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005503] [C loss: 0.694701]\n",
      "5504 [D loss: 0.694703, acc.: 0.00%] [G loss: 0.004466] [C loss: 0.694700]\n",
      "5505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005402] [C loss: 0.694700]\n",
      "5506 [D loss: 0.694703, acc.: 0.00%] [G loss: 0.005064] [C loss: 0.694698]\n",
      "5507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005448] [C loss: 0.694698]\n",
      "5508 [D loss: 0.694701, acc.: 0.00%] [G loss: 0.006994] [C loss: 0.694697]\n",
      "5509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005084] [C loss: 0.694697]\n",
      "5510 [D loss: 0.694699, acc.: 0.00%] [G loss: 0.004225] [C loss: 0.694695]\n",
      "5511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005856] [C loss: 0.694695]\n",
      "5512 [D loss: 0.694697, acc.: 0.00%] [G loss: 0.004869] [C loss: 0.694694]\n",
      "5513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007416] [C loss: 0.694694]\n",
      "5514 [D loss: 0.694695, acc.: 0.00%] [G loss: 0.005078] [C loss: 0.694693]\n",
      "5515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003753] [C loss: 0.694693]\n",
      "5516 [D loss: 0.694694, acc.: 0.00%] [G loss: 0.004511] [C loss: 0.694689]\n",
      "5517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011409] [C loss: 0.694689]\n",
      "5518 [D loss: 0.694692, acc.: 0.00%] [G loss: 0.004523] [C loss: 0.694688]\n",
      "5519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005308] [C loss: 0.694688]\n",
      "5520 [D loss: 0.694690, acc.: 0.00%] [G loss: 0.004921] [C loss: 0.694687]\n",
      "5521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.694687]\n",
      "5522 [D loss: 0.694690, acc.: 0.00%] [G loss: 0.006673] [C loss: 0.694685]\n",
      "5523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005455] [C loss: 0.694685]\n",
      "5524 [D loss: 0.694688, acc.: 0.00%] [G loss: 0.005407] [C loss: 0.694683]\n",
      "5525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006309] [C loss: 0.694683]\n",
      "5526 [D loss: 0.694686, acc.: 0.00%] [G loss: 0.006256] [C loss: 0.694682]\n",
      "5527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018904] [C loss: 0.694682]\n",
      "5528 [D loss: 0.694684, acc.: 0.00%] [G loss: 0.005090] [C loss: 0.694682]\n",
      "5529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008712] [C loss: 0.694682]\n",
      "5530 [D loss: 0.694684, acc.: 0.00%] [G loss: 0.005634] [C loss: 0.694680]\n",
      "5531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005022] [C loss: 0.694680]\n",
      "5532 [D loss: 0.694682, acc.: 0.00%] [G loss: 0.006208] [C loss: 0.694678]\n",
      "5533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006289] [C loss: 0.694678]\n",
      "5534 [D loss: 0.694680, acc.: 0.00%] [G loss: 0.005471] [C loss: 0.694676]\n",
      "5535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004987] [C loss: 0.694676]\n",
      "5536 [D loss: 0.694679, acc.: 0.00%] [G loss: 0.005699] [C loss: 0.694675]\n",
      "5537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003801] [C loss: 0.694675]\n",
      "5538 [D loss: 0.694678, acc.: 0.00%] [G loss: 0.005597] [C loss: 0.694674]\n",
      "5539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005710] [C loss: 0.694674]\n",
      "5540 [D loss: 0.694676, acc.: 0.00%] [G loss: 0.004717] [C loss: 0.694671]\n",
      "5541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.694671]\n",
      "5542 [D loss: 0.694674, acc.: 0.00%] [G loss: 0.006733] [C loss: 0.694671]\n",
      "5543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004429] [C loss: 0.694671]\n",
      "5544 [D loss: 0.694673, acc.: 0.00%] [G loss: 0.004062] [C loss: 0.694669]\n",
      "5545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008177] [C loss: 0.694669]\n",
      "5546 [D loss: 0.694671, acc.: 0.00%] [G loss: 0.011305] [C loss: 0.694667]\n",
      "5547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004173] [C loss: 0.694667]\n",
      "5548 [D loss: 0.694670, acc.: 0.00%] [G loss: 0.007278] [C loss: 0.694665]\n",
      "5549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005663] [C loss: 0.694665]\n",
      "5550 [D loss: 0.694667, acc.: 0.00%] [G loss: 0.005513] [C loss: 0.694665]\n",
      "5551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004518] [C loss: 0.694665]\n",
      "5552 [D loss: 0.694667, acc.: 0.00%] [G loss: 0.005591] [C loss: 0.694663]\n",
      "5553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004701] [C loss: 0.694663]\n",
      "5554 [D loss: 0.694665, acc.: 0.00%] [G loss: 0.007407] [C loss: 0.694661]\n",
      "5555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004241] [C loss: 0.694661]\n",
      "5556 [D loss: 0.694663, acc.: 0.00%] [G loss: 0.009627] [C loss: 0.694660]\n",
      "5557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005765] [C loss: 0.694660]\n",
      "5558 [D loss: 0.694663, acc.: 0.00%] [G loss: 0.005978] [C loss: 0.694657]\n",
      "5559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005199] [C loss: 0.694657]\n",
      "5560 [D loss: 0.694661, acc.: 0.00%] [G loss: 0.004756] [C loss: 0.694656]\n",
      "5561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006381] [C loss: 0.694656]\n",
      "5562 [D loss: 0.694660, acc.: 0.00%] [G loss: 0.005254] [C loss: 0.694655]\n",
      "5563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003902] [C loss: 0.694655]\n",
      "5564 [D loss: 0.694657, acc.: 0.00%] [G loss: 0.006081] [C loss: 0.694653]\n",
      "5565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005008] [C loss: 0.694653]\n",
      "5566 [D loss: 0.694657, acc.: 0.00%] [G loss: 0.003556] [C loss: 0.694652]\n",
      "5567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006708] [C loss: 0.694652]\n",
      "5568 [D loss: 0.694655, acc.: 0.00%] [G loss: 0.005572] [C loss: 0.694651]\n",
      "5569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003967] [C loss: 0.694651]\n",
      "5570 [D loss: 0.694654, acc.: 0.00%] [G loss: 0.004801] [C loss: 0.694649]\n",
      "5571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005167] [C loss: 0.694649]\n",
      "5572 [D loss: 0.694651, acc.: 0.00%] [G loss: 0.006623] [C loss: 0.694648]\n",
      "5573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005394] [C loss: 0.694648]\n",
      "5574 [D loss: 0.694650, acc.: 0.00%] [G loss: 0.008371] [C loss: 0.694647]\n",
      "5575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007983] [C loss: 0.694647]\n",
      "5576 [D loss: 0.694650, acc.: 0.00%] [G loss: 0.005241] [C loss: 0.694645]\n",
      "5577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.694645]\n",
      "5578 [D loss: 0.694648, acc.: 0.00%] [G loss: 0.005712] [C loss: 0.694643]\n",
      "5579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006567] [C loss: 0.694643]\n",
      "5580 [D loss: 0.694646, acc.: 0.00%] [G loss: 0.003951] [C loss: 0.694643]\n",
      "5581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007107] [C loss: 0.694643]\n",
      "5582 [D loss: 0.694645, acc.: 0.00%] [G loss: 0.015082] [C loss: 0.694639]\n",
      "5583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004985] [C loss: 0.694639]\n",
      "5584 [D loss: 0.694643, acc.: 0.00%] [G loss: 0.004781] [C loss: 0.694639]\n",
      "5585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006100] [C loss: 0.694639]\n",
      "5586 [D loss: 0.694642, acc.: 0.00%] [G loss: 0.005368] [C loss: 0.694638]\n",
      "5587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004258] [C loss: 0.694638]\n",
      "5588 [D loss: 0.694640, acc.: 0.00%] [G loss: 0.004697] [C loss: 0.694636]\n",
      "5589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005842] [C loss: 0.694636]\n",
      "5590 [D loss: 0.694638, acc.: 0.00%] [G loss: 0.002935] [C loss: 0.694634]\n",
      "5591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.694634]\n",
      "5592 [D loss: 0.694637, acc.: 0.00%] [G loss: 0.008606] [C loss: 0.694633]\n",
      "5593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004723] [C loss: 0.694633]\n",
      "5594 [D loss: 0.694635, acc.: 0.00%] [G loss: 0.004074] [C loss: 0.694631]\n",
      "5595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007349] [C loss: 0.694631]\n",
      "5596 [D loss: 0.694634, acc.: 0.00%] [G loss: 0.008534] [C loss: 0.694631]\n",
      "5597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005077] [C loss: 0.694631]\n",
      "5598 [D loss: 0.694633, acc.: 0.00%] [G loss: 0.005996] [C loss: 0.694628]\n",
      "5599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004591] [C loss: 0.694628]\n",
      "5600 [D loss: 0.694631, acc.: 0.00%] [G loss: 0.009202] [C loss: 0.694627]\n",
      "5601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021337] [C loss: 0.694627]\n",
      "5602 [D loss: 0.694629, acc.: 0.00%] [G loss: 0.004993] [C loss: 0.694625]\n",
      "5603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006590] [C loss: 0.694625]\n",
      "5604 [D loss: 0.694628, acc.: 0.00%] [G loss: 0.004368] [C loss: 0.694623]\n",
      "5605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005694] [C loss: 0.694623]\n",
      "5606 [D loss: 0.694626, acc.: 0.00%] [G loss: 0.006347] [C loss: 0.694622]\n",
      "5607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011379] [C loss: 0.694622]\n",
      "5608 [D loss: 0.694624, acc.: 0.00%] [G loss: 0.004253] [C loss: 0.694620]\n",
      "5609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003084] [C loss: 0.694620]\n",
      "5610 [D loss: 0.694622, acc.: 0.00%] [G loss: 0.006031] [C loss: 0.694619]\n",
      "5611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004703] [C loss: 0.694619]\n",
      "5612 [D loss: 0.694622, acc.: 0.00%] [G loss: 0.004492] [C loss: 0.694618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004312] [C loss: 0.694618]\n",
      "5614 [D loss: 0.694620, acc.: 0.00%] [G loss: 0.007801] [C loss: 0.694616]\n",
      "5615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004643] [C loss: 0.694616]\n",
      "5616 [D loss: 0.694619, acc.: 0.00%] [G loss: 0.011786] [C loss: 0.694615]\n",
      "5617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005867] [C loss: 0.694615]\n",
      "5618 [D loss: 0.694618, acc.: 0.00%] [G loss: 0.006403] [C loss: 0.694613]\n",
      "5619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006499] [C loss: 0.694613]\n",
      "5620 [D loss: 0.694615, acc.: 0.00%] [G loss: 0.006747] [C loss: 0.694611]\n",
      "5621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008028] [C loss: 0.694611]\n",
      "5622 [D loss: 0.694615, acc.: 0.00%] [G loss: 0.005889] [C loss: 0.694609]\n",
      "5623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003900] [C loss: 0.694609]\n",
      "5624 [D loss: 0.694612, acc.: 0.00%] [G loss: 0.006979] [C loss: 0.694610]\n",
      "5625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005977] [C loss: 0.694610]\n",
      "5626 [D loss: 0.694612, acc.: 0.00%] [G loss: 0.013086] [C loss: 0.694606]\n",
      "5627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004776] [C loss: 0.694606]\n",
      "5628 [D loss: 0.694609, acc.: 0.00%] [G loss: 0.005099] [C loss: 0.694606]\n",
      "5629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004528] [C loss: 0.694606]\n",
      "5630 [D loss: 0.694608, acc.: 0.00%] [G loss: 0.006523] [C loss: 0.694605]\n",
      "5631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005748] [C loss: 0.694605]\n",
      "5632 [D loss: 0.694607, acc.: 0.00%] [G loss: 0.005643] [C loss: 0.694604]\n",
      "5633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005460] [C loss: 0.694604]\n",
      "5634 [D loss: 0.694606, acc.: 0.00%] [G loss: 0.013337] [C loss: 0.694601]\n",
      "5635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004695] [C loss: 0.694601]\n",
      "5636 [D loss: 0.694604, acc.: 0.00%] [G loss: 0.004735] [C loss: 0.694600]\n",
      "5637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008658] [C loss: 0.694600]\n",
      "5638 [D loss: 0.694602, acc.: 0.00%] [G loss: 0.003906] [C loss: 0.694599]\n",
      "5639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004713] [C loss: 0.694599]\n",
      "5640 [D loss: 0.694601, acc.: 0.00%] [G loss: 0.004128] [C loss: 0.694597]\n",
      "5641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010489] [C loss: 0.694597]\n",
      "5642 [D loss: 0.694600, acc.: 0.00%] [G loss: 0.005994] [C loss: 0.694596]\n",
      "5643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005867] [C loss: 0.694596]\n",
      "5644 [D loss: 0.694598, acc.: 0.00%] [G loss: 0.005037] [C loss: 0.694595]\n",
      "5645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010276] [C loss: 0.694595]\n",
      "5646 [D loss: 0.694597, acc.: 0.00%] [G loss: 0.005157] [C loss: 0.694592]\n",
      "5647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005546] [C loss: 0.694592]\n",
      "5648 [D loss: 0.694595, acc.: 0.00%] [G loss: 0.004754] [C loss: 0.694591]\n",
      "5649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004519] [C loss: 0.694591]\n",
      "5650 [D loss: 0.694594, acc.: 0.00%] [G loss: 0.010616] [C loss: 0.694589]\n",
      "5651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005384] [C loss: 0.694589]\n",
      "5652 [D loss: 0.694592, acc.: 0.00%] [G loss: 0.004418] [C loss: 0.694589]\n",
      "5653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007216] [C loss: 0.694589]\n",
      "5654 [D loss: 0.694591, acc.: 0.00%] [G loss: 0.005646] [C loss: 0.694585]\n",
      "5655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005703] [C loss: 0.694585]\n",
      "5656 [D loss: 0.694588, acc.: 0.00%] [G loss: 0.003719] [C loss: 0.694585]\n",
      "5657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.694585]\n",
      "5658 [D loss: 0.694587, acc.: 0.00%] [G loss: 0.006291] [C loss: 0.694584]\n",
      "5659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005446] [C loss: 0.694584]\n",
      "5660 [D loss: 0.694587, acc.: 0.00%] [G loss: 0.004477] [C loss: 0.694583]\n",
      "5661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010577] [C loss: 0.694583]\n",
      "5662 [D loss: 0.694585, acc.: 0.00%] [G loss: 0.009721] [C loss: 0.694581]\n",
      "5663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006369] [C loss: 0.694581]\n",
      "5664 [D loss: 0.694583, acc.: 0.00%] [G loss: 0.005006] [C loss: 0.694579]\n",
      "5665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008446] [C loss: 0.694579]\n",
      "5666 [D loss: 0.694582, acc.: 0.00%] [G loss: 0.006224] [C loss: 0.694578]\n",
      "5667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004214] [C loss: 0.694578]\n",
      "5668 [D loss: 0.694580, acc.: 0.00%] [G loss: 0.008917] [C loss: 0.694576]\n",
      "5669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004053] [C loss: 0.694576]\n",
      "5670 [D loss: 0.694578, acc.: 0.00%] [G loss: 0.006932] [C loss: 0.694576]\n",
      "5671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004391] [C loss: 0.694576]\n",
      "5672 [D loss: 0.694578, acc.: 0.00%] [G loss: 0.004275] [C loss: 0.694572]\n",
      "5673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007691] [C loss: 0.694572]\n",
      "5674 [D loss: 0.694575, acc.: 0.00%] [G loss: 0.005332] [C loss: 0.694573]\n",
      "5675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006857] [C loss: 0.694573]\n",
      "5676 [D loss: 0.694574, acc.: 0.00%] [G loss: 0.008889] [C loss: 0.694573]\n",
      "5677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004909] [C loss: 0.694573]\n",
      "5678 [D loss: 0.694574, acc.: 0.00%] [G loss: 0.006683] [C loss: 0.694568]\n",
      "5679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004213] [C loss: 0.694568]\n",
      "5680 [D loss: 0.694571, acc.: 0.00%] [G loss: 0.005767] [C loss: 0.694567]\n",
      "5681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003523] [C loss: 0.694567]\n",
      "5682 [D loss: 0.694570, acc.: 0.00%] [G loss: 0.003465] [C loss: 0.694566]\n",
      "5683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007986] [C loss: 0.694566]\n",
      "5684 [D loss: 0.694568, acc.: 0.00%] [G loss: 0.005633] [C loss: 0.694563]\n",
      "5685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005852] [C loss: 0.694563]\n",
      "5686 [D loss: 0.694566, acc.: 0.00%] [G loss: 0.005261] [C loss: 0.694563]\n",
      "5687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.694563]\n",
      "5688 [D loss: 0.694564, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.694560]\n",
      "5689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008079] [C loss: 0.694560]\n",
      "5690 [D loss: 0.694564, acc.: 0.00%] [G loss: 0.005567] [C loss: 0.694560]\n",
      "5691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007212] [C loss: 0.694560]\n",
      "5692 [D loss: 0.694562, acc.: 0.00%] [G loss: 0.007565] [C loss: 0.694558]\n",
      "5693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004336] [C loss: 0.694558]\n",
      "5694 [D loss: 0.694560, acc.: 0.00%] [G loss: 0.004908] [C loss: 0.694555]\n",
      "5695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005219] [C loss: 0.694555]\n",
      "5696 [D loss: 0.694558, acc.: 0.00%] [G loss: 0.005658] [C loss: 0.694554]\n",
      "5697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006490] [C loss: 0.694554]\n",
      "5698 [D loss: 0.694556, acc.: 0.00%] [G loss: 0.008340] [C loss: 0.694552]\n",
      "5699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005135] [C loss: 0.694552]\n",
      "5700 [D loss: 0.694555, acc.: 0.00%] [G loss: 0.008312] [C loss: 0.694552]\n",
      "5701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009041] [C loss: 0.694552]\n",
      "5702 [D loss: 0.694554, acc.: 0.00%] [G loss: 0.004731] [C loss: 0.694550]\n",
      "5703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007887] [C loss: 0.694550]\n",
      "5704 [D loss: 0.694553, acc.: 0.00%] [G loss: 0.009375] [C loss: 0.694549]\n",
      "5705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007367] [C loss: 0.694549]\n",
      "5706 [D loss: 0.694551, acc.: 0.00%] [G loss: 0.006360] [C loss: 0.694548]\n",
      "5707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007417] [C loss: 0.694548]\n",
      "5708 [D loss: 0.694550, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.694545]\n",
      "5709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.694545]\n",
      "5710 [D loss: 0.694547, acc.: 0.00%] [G loss: 0.005649] [C loss: 0.694544]\n",
      "5711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005790] [C loss: 0.694544]\n",
      "5712 [D loss: 0.694547, acc.: 0.00%] [G loss: 0.007984] [C loss: 0.694541]\n",
      "5713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.694541]\n",
      "5714 [D loss: 0.694545, acc.: 0.00%] [G loss: 0.004511] [C loss: 0.694540]\n",
      "5715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005410] [C loss: 0.694540]\n",
      "5716 [D loss: 0.694543, acc.: 0.00%] [G loss: 0.005855] [C loss: 0.694540]\n",
      "5717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006563] [C loss: 0.694540]\n",
      "5718 [D loss: 0.694543, acc.: 0.00%] [G loss: 0.012471] [C loss: 0.694538]\n",
      "5719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004860] [C loss: 0.694538]\n",
      "5720 [D loss: 0.694541, acc.: 0.00%] [G loss: 0.005198] [C loss: 0.694539]\n",
      "5721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005169] [C loss: 0.694539]\n",
      "5722 [D loss: 0.694541, acc.: 0.00%] [G loss: 0.006423] [C loss: 0.694533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006552] [C loss: 0.694533]\n",
      "5724 [D loss: 0.694537, acc.: 0.00%] [G loss: 0.004100] [C loss: 0.694533]\n",
      "5725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008375] [C loss: 0.694533]\n",
      "5726 [D loss: 0.694536, acc.: 0.00%] [G loss: 0.008399] [C loss: 0.694533]\n",
      "5727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005503] [C loss: 0.694533]\n",
      "5728 [D loss: 0.694536, acc.: 0.00%] [G loss: 0.004857] [C loss: 0.694532]\n",
      "5729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007282] [C loss: 0.694532]\n",
      "5730 [D loss: 0.694534, acc.: 0.00%] [G loss: 0.015173] [C loss: 0.694530]\n",
      "5731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009176] [C loss: 0.694530]\n",
      "5732 [D loss: 0.694532, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.694528]\n",
      "5733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006473] [C loss: 0.694528]\n",
      "5734 [D loss: 0.694530, acc.: 0.00%] [G loss: 0.005333] [C loss: 0.694526]\n",
      "5735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005439] [C loss: 0.694526]\n",
      "5736 [D loss: 0.694529, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.694526]\n",
      "5737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004325] [C loss: 0.694526]\n",
      "5738 [D loss: 0.694528, acc.: 0.00%] [G loss: 0.005233] [C loss: 0.694524]\n",
      "5739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005261] [C loss: 0.694524]\n",
      "5740 [D loss: 0.694527, acc.: 0.00%] [G loss: 0.011029] [C loss: 0.694521]\n",
      "5741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005765] [C loss: 0.694521]\n",
      "5742 [D loss: 0.694524, acc.: 0.00%] [G loss: 0.018020] [C loss: 0.694522]\n",
      "5743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004972] [C loss: 0.694522]\n",
      "5744 [D loss: 0.694524, acc.: 0.00%] [G loss: 0.004775] [C loss: 0.694519]\n",
      "5745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005703] [C loss: 0.694519]\n",
      "5746 [D loss: 0.694522, acc.: 0.00%] [G loss: 0.004769] [C loss: 0.694517]\n",
      "5747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004446] [C loss: 0.694517]\n",
      "5748 [D loss: 0.694520, acc.: 0.00%] [G loss: 0.007141] [C loss: 0.694517]\n",
      "5749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005600] [C loss: 0.694517]\n",
      "5750 [D loss: 0.694519, acc.: 0.00%] [G loss: 0.006540] [C loss: 0.694515]\n",
      "5751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004819] [C loss: 0.694515]\n",
      "5752 [D loss: 0.694517, acc.: 0.00%] [G loss: 0.004777] [C loss: 0.694514]\n",
      "5753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004073] [C loss: 0.694514]\n",
      "5754 [D loss: 0.694516, acc.: 0.00%] [G loss: 0.006027] [C loss: 0.694512]\n",
      "5755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004844] [C loss: 0.694512]\n",
      "5756 [D loss: 0.694515, acc.: 0.00%] [G loss: 0.017575] [C loss: 0.694513]\n",
      "5757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.694513]\n",
      "5758 [D loss: 0.694514, acc.: 0.00%] [G loss: 0.007033] [C loss: 0.694510]\n",
      "5759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004995] [C loss: 0.694510]\n",
      "5760 [D loss: 0.694511, acc.: 0.00%] [G loss: 0.005305] [C loss: 0.694509]\n",
      "5761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007038] [C loss: 0.694509]\n",
      "5762 [D loss: 0.694510, acc.: 0.00%] [G loss: 0.006003] [C loss: 0.694505]\n",
      "5763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006666] [C loss: 0.694505]\n",
      "5764 [D loss: 0.694508, acc.: 0.00%] [G loss: 0.005999] [C loss: 0.694504]\n",
      "5765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005003] [C loss: 0.694504]\n",
      "5766 [D loss: 0.694507, acc.: 0.00%] [G loss: 0.007636] [C loss: 0.694504]\n",
      "5767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006038] [C loss: 0.694504]\n",
      "5768 [D loss: 0.694506, acc.: 0.00%] [G loss: 0.006122] [C loss: 0.694502]\n",
      "5769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007091] [C loss: 0.694502]\n",
      "5770 [D loss: 0.694504, acc.: 0.00%] [G loss: 0.003623] [C loss: 0.694499]\n",
      "5771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003912] [C loss: 0.694499]\n",
      "5772 [D loss: 0.694502, acc.: 0.00%] [G loss: 0.007965] [C loss: 0.694500]\n",
      "5773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007330] [C loss: 0.694500]\n",
      "5774 [D loss: 0.694502, acc.: 0.00%] [G loss: 0.013449] [C loss: 0.694497]\n",
      "5775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.694497]\n",
      "5776 [D loss: 0.694500, acc.: 0.00%] [G loss: 0.005260] [C loss: 0.694495]\n",
      "5777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006073] [C loss: 0.694495]\n",
      "5778 [D loss: 0.694498, acc.: 0.00%] [G loss: 0.010463] [C loss: 0.694495]\n",
      "5779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004370] [C loss: 0.694495]\n",
      "5780 [D loss: 0.694497, acc.: 0.00%] [G loss: 0.006471] [C loss: 0.694492]\n",
      "5781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006444] [C loss: 0.694492]\n",
      "5782 [D loss: 0.694495, acc.: 0.00%] [G loss: 0.005464] [C loss: 0.694492]\n",
      "5783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006215] [C loss: 0.694492]\n",
      "5784 [D loss: 0.694494, acc.: 0.00%] [G loss: 0.004981] [C loss: 0.694489]\n",
      "5785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006683] [C loss: 0.694489]\n",
      "5786 [D loss: 0.694492, acc.: 0.00%] [G loss: 0.005121] [C loss: 0.694488]\n",
      "5787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.694488]\n",
      "5788 [D loss: 0.694490, acc.: 0.00%] [G loss: 0.004524] [C loss: 0.694486]\n",
      "5789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003939] [C loss: 0.694486]\n",
      "5790 [D loss: 0.694489, acc.: 0.00%] [G loss: 0.004753] [C loss: 0.694484]\n",
      "5791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015243] [C loss: 0.694484]\n",
      "5792 [D loss: 0.694487, acc.: 0.00%] [G loss: 0.004769] [C loss: 0.694484]\n",
      "5793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005517] [C loss: 0.694484]\n",
      "5794 [D loss: 0.694486, acc.: 0.00%] [G loss: 0.004925] [C loss: 0.694483]\n",
      "5795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004893] [C loss: 0.694483]\n",
      "5796 [D loss: 0.694486, acc.: 0.00%] [G loss: 0.007176] [C loss: 0.694481]\n",
      "5797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007573] [C loss: 0.694481]\n",
      "5798 [D loss: 0.694484, acc.: 0.00%] [G loss: 0.004318] [C loss: 0.694480]\n",
      "5799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004751] [C loss: 0.694480]\n",
      "5800 [D loss: 0.694483, acc.: 0.00%] [G loss: 0.005240] [C loss: 0.694477]\n",
      "5801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006553] [C loss: 0.694477]\n",
      "5802 [D loss: 0.694480, acc.: 0.00%] [G loss: 0.005021] [C loss: 0.694476]\n",
      "5803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005540] [C loss: 0.694476]\n",
      "5804 [D loss: 0.694479, acc.: 0.00%] [G loss: 0.005830] [C loss: 0.694474]\n",
      "5805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004902] [C loss: 0.694474]\n",
      "5806 [D loss: 0.694477, acc.: 0.00%] [G loss: 0.005410] [C loss: 0.694473]\n",
      "5807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004984] [C loss: 0.694473]\n",
      "5808 [D loss: 0.694477, acc.: 0.00%] [G loss: 0.006955] [C loss: 0.694473]\n",
      "5809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008919] [C loss: 0.694473]\n",
      "5810 [D loss: 0.694476, acc.: 0.00%] [G loss: 0.006065] [C loss: 0.694469]\n",
      "5811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004959] [C loss: 0.694469]\n",
      "5812 [D loss: 0.694472, acc.: 0.00%] [G loss: 0.006998] [C loss: 0.694470]\n",
      "5813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005075] [C loss: 0.694470]\n",
      "5814 [D loss: 0.694472, acc.: 0.00%] [G loss: 0.005486] [C loss: 0.694469]\n",
      "5815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.694469]\n",
      "5816 [D loss: 0.694471, acc.: 0.00%] [G loss: 0.004301] [C loss: 0.694465]\n",
      "5817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004641] [C loss: 0.694465]\n",
      "5818 [D loss: 0.694468, acc.: 0.00%] [G loss: 0.006087] [C loss: 0.694465]\n",
      "5819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004679] [C loss: 0.694465]\n",
      "5820 [D loss: 0.694468, acc.: 0.00%] [G loss: 0.005112] [C loss: 0.694465]\n",
      "5821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007208] [C loss: 0.694465]\n",
      "5822 [D loss: 0.694466, acc.: 0.00%] [G loss: 0.003627] [C loss: 0.694462]\n",
      "5823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006150] [C loss: 0.694462]\n",
      "5824 [D loss: 0.694464, acc.: 0.00%] [G loss: 0.004474] [C loss: 0.694461]\n",
      "5825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005850] [C loss: 0.694461]\n",
      "5826 [D loss: 0.694463, acc.: 0.00%] [G loss: 0.005346] [C loss: 0.694460]\n",
      "5827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007981] [C loss: 0.694460]\n",
      "5828 [D loss: 0.694462, acc.: 0.00%] [G loss: 0.004452] [C loss: 0.694457]\n",
      "5829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005067] [C loss: 0.694457]\n",
      "5830 [D loss: 0.694459, acc.: 0.00%] [G loss: 0.024936] [C loss: 0.694455]\n",
      "5831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005143] [C loss: 0.694455]\n",
      "5832 [D loss: 0.694458, acc.: 0.00%] [G loss: 0.004318] [C loss: 0.694454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010601] [C loss: 0.694454]\n",
      "5834 [D loss: 0.694455, acc.: 0.00%] [G loss: 0.005313] [C loss: 0.694452]\n",
      "5835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005319] [C loss: 0.694452]\n",
      "5836 [D loss: 0.694456, acc.: 0.00%] [G loss: 0.004536] [C loss: 0.694450]\n",
      "5837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005713] [C loss: 0.694450]\n",
      "5838 [D loss: 0.694453, acc.: 0.00%] [G loss: 0.005383] [C loss: 0.694450]\n",
      "5839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003711] [C loss: 0.694450]\n",
      "5840 [D loss: 0.694452, acc.: 0.00%] [G loss: 0.004915] [C loss: 0.694446]\n",
      "5841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006101] [C loss: 0.694446]\n",
      "5842 [D loss: 0.694450, acc.: 0.00%] [G loss: 0.005563] [C loss: 0.694447]\n",
      "5843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005492] [C loss: 0.694447]\n",
      "5844 [D loss: 0.694449, acc.: 0.00%] [G loss: 0.007219] [C loss: 0.694444]\n",
      "5845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005234] [C loss: 0.694444]\n",
      "5846 [D loss: 0.694447, acc.: 0.00%] [G loss: 0.004610] [C loss: 0.694443]\n",
      "5847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004876] [C loss: 0.694443]\n",
      "5848 [D loss: 0.694446, acc.: 0.00%] [G loss: 0.008744] [C loss: 0.694442]\n",
      "5849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005231] [C loss: 0.694442]\n",
      "5850 [D loss: 0.694444, acc.: 0.00%] [G loss: 0.006220] [C loss: 0.694442]\n",
      "5851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005670] [C loss: 0.694442]\n",
      "5852 [D loss: 0.694444, acc.: 0.00%] [G loss: 0.005952] [C loss: 0.694439]\n",
      "5853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005645] [C loss: 0.694439]\n",
      "5854 [D loss: 0.694442, acc.: 0.00%] [G loss: 0.011525] [C loss: 0.694438]\n",
      "5855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005462] [C loss: 0.694438]\n",
      "5856 [D loss: 0.694440, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.694437]\n",
      "5857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004841] [C loss: 0.694437]\n",
      "5858 [D loss: 0.694440, acc.: 0.00%] [G loss: 0.005094] [C loss: 0.694436]\n",
      "5859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005546] [C loss: 0.694436]\n",
      "5860 [D loss: 0.694438, acc.: 0.00%] [G loss: 0.004991] [C loss: 0.694434]\n",
      "5861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005553] [C loss: 0.694434]\n",
      "5862 [D loss: 0.694437, acc.: 0.00%] [G loss: 0.004700] [C loss: 0.694432]\n",
      "5863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005596] [C loss: 0.694432]\n",
      "5864 [D loss: 0.694434, acc.: 0.00%] [G loss: 0.009283] [C loss: 0.694431]\n",
      "5865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004309] [C loss: 0.694431]\n",
      "5866 [D loss: 0.694433, acc.: 0.00%] [G loss: 0.004137] [C loss: 0.694430]\n",
      "5867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004792] [C loss: 0.694430]\n",
      "5868 [D loss: 0.694432, acc.: 0.00%] [G loss: 0.006401] [C loss: 0.694428]\n",
      "5869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004788] [C loss: 0.694428]\n",
      "5870 [D loss: 0.694429, acc.: 0.00%] [G loss: 0.007231] [C loss: 0.694428]\n",
      "5871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005781] [C loss: 0.694428]\n",
      "5872 [D loss: 0.694429, acc.: 0.00%] [G loss: 0.007048] [C loss: 0.694426]\n",
      "5873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005463] [C loss: 0.694426]\n",
      "5874 [D loss: 0.694428, acc.: 0.00%] [G loss: 0.004297] [C loss: 0.694424]\n",
      "5875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005559] [C loss: 0.694424]\n",
      "5876 [D loss: 0.694426, acc.: 0.00%] [G loss: 0.005418] [C loss: 0.694422]\n",
      "5877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003942] [C loss: 0.694422]\n",
      "5878 [D loss: 0.694425, acc.: 0.00%] [G loss: 0.007042] [C loss: 0.694421]\n",
      "5879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003111] [C loss: 0.694421]\n",
      "5880 [D loss: 0.694424, acc.: 0.00%] [G loss: 0.008583] [C loss: 0.694419]\n",
      "5881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003826] [C loss: 0.694419]\n",
      "5882 [D loss: 0.694422, acc.: 0.00%] [G loss: 0.006887] [C loss: 0.694418]\n",
      "5883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006267] [C loss: 0.694418]\n",
      "5884 [D loss: 0.694420, acc.: 0.00%] [G loss: 0.006134] [C loss: 0.694418]\n",
      "5885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010236] [C loss: 0.694418]\n",
      "5886 [D loss: 0.694420, acc.: 0.00%] [G loss: 0.004611] [C loss: 0.694418]\n",
      "5887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009906] [C loss: 0.694418]\n",
      "5888 [D loss: 0.694419, acc.: 0.00%] [G loss: 0.007017] [C loss: 0.694413]\n",
      "5889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004679] [C loss: 0.694413]\n",
      "5890 [D loss: 0.694415, acc.: 0.00%] [G loss: 0.007430] [C loss: 0.694411]\n",
      "5891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005474] [C loss: 0.694411]\n",
      "5892 [D loss: 0.694414, acc.: 0.00%] [G loss: 0.004582] [C loss: 0.694408]\n",
      "5893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004226] [C loss: 0.694408]\n",
      "5894 [D loss: 0.694412, acc.: 0.00%] [G loss: 0.005950] [C loss: 0.694411]\n",
      "5895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008434] [C loss: 0.694411]\n",
      "5896 [D loss: 0.694412, acc.: 0.00%] [G loss: 0.005908] [C loss: 0.694408]\n",
      "5897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005665] [C loss: 0.694408]\n",
      "5898 [D loss: 0.694410, acc.: 0.00%] [G loss: 0.004360] [C loss: 0.694406]\n",
      "5899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005153] [C loss: 0.694406]\n",
      "5900 [D loss: 0.694408, acc.: 0.00%] [G loss: 0.005427] [C loss: 0.694404]\n",
      "5901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.694404]\n",
      "5902 [D loss: 0.694407, acc.: 0.00%] [G loss: 0.005149] [C loss: 0.694405]\n",
      "5903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005399] [C loss: 0.694405]\n",
      "5904 [D loss: 0.694406, acc.: 0.00%] [G loss: 0.003391] [C loss: 0.694402]\n",
      "5905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013215] [C loss: 0.694402]\n",
      "5906 [D loss: 0.694404, acc.: 0.00%] [G loss: 0.006984] [C loss: 0.694400]\n",
      "5907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005099] [C loss: 0.694400]\n",
      "5908 [D loss: 0.694403, acc.: 0.00%] [G loss: 0.004452] [C loss: 0.694400]\n",
      "5909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009197] [C loss: 0.694400]\n",
      "5910 [D loss: 0.694402, acc.: 0.00%] [G loss: 0.005074] [C loss: 0.694397]\n",
      "5911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007011] [C loss: 0.694397]\n",
      "5912 [D loss: 0.694400, acc.: 0.00%] [G loss: 0.006734] [C loss: 0.694395]\n",
      "5913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006227] [C loss: 0.694395]\n",
      "5914 [D loss: 0.694398, acc.: 0.00%] [G loss: 0.005221] [C loss: 0.694394]\n",
      "5915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007860] [C loss: 0.694394]\n",
      "5916 [D loss: 0.694397, acc.: 0.00%] [G loss: 0.004447] [C loss: 0.694393]\n",
      "5917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007789] [C loss: 0.694393]\n",
      "5918 [D loss: 0.694395, acc.: 0.00%] [G loss: 0.004438] [C loss: 0.694393]\n",
      "5919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009030] [C loss: 0.694393]\n",
      "5920 [D loss: 0.694395, acc.: 0.00%] [G loss: 0.004442] [C loss: 0.694392]\n",
      "5921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003772] [C loss: 0.694392]\n",
      "5922 [D loss: 0.694394, acc.: 0.00%] [G loss: 0.005663] [C loss: 0.694390]\n",
      "5923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009284] [C loss: 0.694390]\n",
      "5924 [D loss: 0.694391, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.694387]\n",
      "5925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004771] [C loss: 0.694387]\n",
      "5926 [D loss: 0.694390, acc.: 0.00%] [G loss: 0.006026] [C loss: 0.694387]\n",
      "5927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007888] [C loss: 0.694387]\n",
      "5928 [D loss: 0.694388, acc.: 0.00%] [G loss: 0.006250] [C loss: 0.694384]\n",
      "5929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004612] [C loss: 0.694384]\n",
      "5930 [D loss: 0.694387, acc.: 0.00%] [G loss: 0.007308] [C loss: 0.694383]\n",
      "5931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004180] [C loss: 0.694383]\n",
      "5932 [D loss: 0.694385, acc.: 0.00%] [G loss: 0.007501] [C loss: 0.694380]\n",
      "5933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013254] [C loss: 0.694380]\n",
      "5934 [D loss: 0.694383, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.694380]\n",
      "5935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005359] [C loss: 0.694380]\n",
      "5936 [D loss: 0.694382, acc.: 0.00%] [G loss: 0.006685] [C loss: 0.694378]\n",
      "5937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005544] [C loss: 0.694378]\n",
      "5938 [D loss: 0.694381, acc.: 0.00%] [G loss: 0.005622] [C loss: 0.694378]\n",
      "5939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005533] [C loss: 0.694378]\n",
      "5940 [D loss: 0.694380, acc.: 0.00%] [G loss: 0.005629] [C loss: 0.694376]\n",
      "5941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005168] [C loss: 0.694376]\n",
      "5942 [D loss: 0.694378, acc.: 0.00%] [G loss: 0.005807] [C loss: 0.694375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004221] [C loss: 0.694375]\n",
      "5944 [D loss: 0.694377, acc.: 0.00%] [G loss: 0.012826] [C loss: 0.694374]\n",
      "5945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004955] [C loss: 0.694374]\n",
      "5946 [D loss: 0.694375, acc.: 0.00%] [G loss: 0.004201] [C loss: 0.694371]\n",
      "5947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005781] [C loss: 0.694371]\n",
      "5948 [D loss: 0.694375, acc.: 0.00%] [G loss: 0.005628] [C loss: 0.694370]\n",
      "5949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004401] [C loss: 0.694370]\n",
      "5950 [D loss: 0.694373, acc.: 0.00%] [G loss: 0.007043] [C loss: 0.694369]\n",
      "5951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005255] [C loss: 0.694369]\n",
      "5952 [D loss: 0.694371, acc.: 0.00%] [G loss: 0.006382] [C loss: 0.694365]\n",
      "5953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006246] [C loss: 0.694365]\n",
      "5954 [D loss: 0.694369, acc.: 0.00%] [G loss: 0.022645] [C loss: 0.694364]\n",
      "5955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006066] [C loss: 0.694364]\n",
      "5956 [D loss: 0.694367, acc.: 0.00%] [G loss: 0.004814] [C loss: 0.694364]\n",
      "5957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007369] [C loss: 0.694364]\n",
      "5958 [D loss: 0.694367, acc.: 0.00%] [G loss: 0.011601] [C loss: 0.694362]\n",
      "5959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003899] [C loss: 0.694362]\n",
      "5960 [D loss: 0.694364, acc.: 0.00%] [G loss: 0.005190] [C loss: 0.694362]\n",
      "5961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020041] [C loss: 0.694362]\n",
      "5962 [D loss: 0.694364, acc.: 0.00%] [G loss: 0.004951] [C loss: 0.694361]\n",
      "5963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007776] [C loss: 0.694361]\n",
      "5964 [D loss: 0.694362, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.694358]\n",
      "5965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005356] [C loss: 0.694358]\n",
      "5966 [D loss: 0.694361, acc.: 0.00%] [G loss: 0.006834] [C loss: 0.694358]\n",
      "5967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005052] [C loss: 0.694358]\n",
      "5968 [D loss: 0.694359, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.694355]\n",
      "5969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004624] [C loss: 0.694355]\n",
      "5970 [D loss: 0.694358, acc.: 0.00%] [G loss: 0.005589] [C loss: 0.694354]\n",
      "5971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005375] [C loss: 0.694354]\n",
      "5972 [D loss: 0.694357, acc.: 0.00%] [G loss: 0.006056] [C loss: 0.694354]\n",
      "5973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004838] [C loss: 0.694354]\n",
      "5974 [D loss: 0.694356, acc.: 0.00%] [G loss: 0.006063] [C loss: 0.694352]\n",
      "5975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005352] [C loss: 0.694352]\n",
      "5976 [D loss: 0.694354, acc.: 0.00%] [G loss: 0.004606] [C loss: 0.694351]\n",
      "5977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005427] [C loss: 0.694351]\n",
      "5978 [D loss: 0.694354, acc.: 0.00%] [G loss: 0.014675] [C loss: 0.694349]\n",
      "5979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.694349]\n",
      "5980 [D loss: 0.694351, acc.: 0.00%] [G loss: 0.005366] [C loss: 0.694348]\n",
      "5981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005091] [C loss: 0.694348]\n",
      "5982 [D loss: 0.694351, acc.: 0.00%] [G loss: 0.007924] [C loss: 0.694348]\n",
      "5983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004424] [C loss: 0.694348]\n",
      "5984 [D loss: 0.694349, acc.: 0.00%] [G loss: 0.005124] [C loss: 0.694345]\n",
      "5985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004905] [C loss: 0.694345]\n",
      "5986 [D loss: 0.694347, acc.: 0.00%] [G loss: 0.004204] [C loss: 0.694344]\n",
      "5987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004735] [C loss: 0.694344]\n",
      "5988 [D loss: 0.694346, acc.: 0.00%] [G loss: 0.006016] [C loss: 0.694342]\n",
      "5989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009635] [C loss: 0.694342]\n",
      "5990 [D loss: 0.694344, acc.: 0.00%] [G loss: 0.004663] [C loss: 0.694340]\n",
      "5991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005984] [C loss: 0.694340]\n",
      "5992 [D loss: 0.694343, acc.: 0.00%] [G loss: 0.007704] [C loss: 0.694338]\n",
      "5993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003727] [C loss: 0.694338]\n",
      "5994 [D loss: 0.694341, acc.: 0.00%] [G loss: 0.005173] [C loss: 0.694337]\n",
      "5995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008154] [C loss: 0.694337]\n",
      "5996 [D loss: 0.694338, acc.: 0.00%] [G loss: 0.005515] [C loss: 0.694336]\n",
      "5997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004253] [C loss: 0.694336]\n",
      "5998 [D loss: 0.694339, acc.: 0.00%] [G loss: 0.007161] [C loss: 0.694335]\n",
      "5999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.694335]\n",
      "6000 [D loss: 0.694337, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.694334]\n",
      "6001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004059] [C loss: 0.694334]\n",
      "6002 [D loss: 0.694336, acc.: 0.00%] [G loss: 0.005420] [C loss: 0.694332]\n",
      "6003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005138] [C loss: 0.694332]\n",
      "6004 [D loss: 0.694334, acc.: 0.00%] [G loss: 0.005184] [C loss: 0.694331]\n",
      "6005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004160] [C loss: 0.694331]\n",
      "6006 [D loss: 0.694333, acc.: 0.00%] [G loss: 0.009553] [C loss: 0.694330]\n",
      "6007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004521] [C loss: 0.694330]\n",
      "6008 [D loss: 0.694331, acc.: 0.00%] [G loss: 0.004818] [C loss: 0.694327]\n",
      "6009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.694327]\n",
      "6010 [D loss: 0.694330, acc.: 0.00%] [G loss: 0.005916] [C loss: 0.694327]\n",
      "6011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003709] [C loss: 0.694327]\n",
      "6012 [D loss: 0.694329, acc.: 0.00%] [G loss: 0.009066] [C loss: 0.694326]\n",
      "6013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014667] [C loss: 0.694326]\n",
      "6014 [D loss: 0.694328, acc.: 0.00%] [G loss: 0.006513] [C loss: 0.694322]\n",
      "6015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010150] [C loss: 0.694322]\n",
      "6016 [D loss: 0.694326, acc.: 0.00%] [G loss: 0.005112] [C loss: 0.694322]\n",
      "6017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005377] [C loss: 0.694322]\n",
      "6018 [D loss: 0.694324, acc.: 0.00%] [G loss: 0.006990] [C loss: 0.694321]\n",
      "6019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004850] [C loss: 0.694321]\n",
      "6020 [D loss: 0.694323, acc.: 0.00%] [G loss: 0.006009] [C loss: 0.694319]\n",
      "6021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003895] [C loss: 0.694319]\n",
      "6022 [D loss: 0.694321, acc.: 0.00%] [G loss: 0.005410] [C loss: 0.694317]\n",
      "6023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004944] [C loss: 0.694317]\n",
      "6024 [D loss: 0.694319, acc.: 0.00%] [G loss: 0.005225] [C loss: 0.694315]\n",
      "6025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004656] [C loss: 0.694315]\n",
      "6026 [D loss: 0.694318, acc.: 0.00%] [G loss: 0.006524] [C loss: 0.694315]\n",
      "6027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006703] [C loss: 0.694315]\n",
      "6028 [D loss: 0.694317, acc.: 0.00%] [G loss: 0.011128] [C loss: 0.694313]\n",
      "6029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005199] [C loss: 0.694313]\n",
      "6030 [D loss: 0.694315, acc.: 0.00%] [G loss: 0.005806] [C loss: 0.694311]\n",
      "6031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004767] [C loss: 0.694311]\n",
      "6032 [D loss: 0.694314, acc.: 0.00%] [G loss: 0.014036] [C loss: 0.694310]\n",
      "6033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010778] [C loss: 0.694310]\n",
      "6034 [D loss: 0.694312, acc.: 0.00%] [G loss: 0.005719] [C loss: 0.694308]\n",
      "6035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006997] [C loss: 0.694308]\n",
      "6036 [D loss: 0.694311, acc.: 0.00%] [G loss: 0.004796] [C loss: 0.694306]\n",
      "6037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004838] [C loss: 0.694306]\n",
      "6038 [D loss: 0.694309, acc.: 0.00%] [G loss: 0.006141] [C loss: 0.694306]\n",
      "6039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006551] [C loss: 0.694306]\n",
      "6040 [D loss: 0.694308, acc.: 0.00%] [G loss: 0.006687] [C loss: 0.694304]\n",
      "6041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004793] [C loss: 0.694304]\n",
      "6042 [D loss: 0.694307, acc.: 0.00%] [G loss: 0.003636] [C loss: 0.694304]\n",
      "6043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.694304]\n",
      "6044 [D loss: 0.694306, acc.: 0.00%] [G loss: 0.003789] [C loss: 0.694300]\n",
      "6045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005996] [C loss: 0.694300]\n",
      "6046 [D loss: 0.694303, acc.: 0.00%] [G loss: 0.005565] [C loss: 0.694300]\n",
      "6047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.694300]\n",
      "6048 [D loss: 0.694303, acc.: 0.00%] [G loss: 0.004762] [C loss: 0.694297]\n",
      "6049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005781] [C loss: 0.694297]\n",
      "6050 [D loss: 0.694300, acc.: 0.00%] [G loss: 0.007707] [C loss: 0.694298]\n",
      "6051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004992] [C loss: 0.694298]\n",
      "6052 [D loss: 0.694300, acc.: 0.00%] [G loss: 0.006902] [C loss: 0.694295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007282] [C loss: 0.694295]\n",
      "6054 [D loss: 0.694298, acc.: 0.00%] [G loss: 0.005307] [C loss: 0.694296]\n",
      "6055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004765] [C loss: 0.694296]\n",
      "6056 [D loss: 0.694298, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.694294]\n",
      "6057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007106] [C loss: 0.694294]\n",
      "6058 [D loss: 0.694296, acc.: 0.00%] [G loss: 0.003025] [C loss: 0.694292]\n",
      "6059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005592] [C loss: 0.694292]\n",
      "6060 [D loss: 0.694294, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.694290]\n",
      "6061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004600] [C loss: 0.694290]\n",
      "6062 [D loss: 0.694292, acc.: 0.00%] [G loss: 0.003481] [C loss: 0.694288]\n",
      "6063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005298] [C loss: 0.694288]\n",
      "6064 [D loss: 0.694291, acc.: 0.00%] [G loss: 0.006035] [C loss: 0.694287]\n",
      "6065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005040] [C loss: 0.694287]\n",
      "6066 [D loss: 0.694290, acc.: 0.00%] [G loss: 0.006295] [C loss: 0.694286]\n",
      "6067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005379] [C loss: 0.694286]\n",
      "6068 [D loss: 0.694288, acc.: 0.00%] [G loss: 0.004223] [C loss: 0.694284]\n",
      "6069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007768] [C loss: 0.694284]\n",
      "6070 [D loss: 0.694287, acc.: 0.00%] [G loss: 0.004433] [C loss: 0.694284]\n",
      "6071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009240] [C loss: 0.694284]\n",
      "6072 [D loss: 0.694286, acc.: 0.00%] [G loss: 0.004274] [C loss: 0.694281]\n",
      "6073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005852] [C loss: 0.694281]\n",
      "6074 [D loss: 0.694284, acc.: 0.00%] [G loss: 0.004793] [C loss: 0.694282]\n",
      "6075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006007] [C loss: 0.694282]\n",
      "6076 [D loss: 0.694283, acc.: 0.00%] [G loss: 0.006253] [C loss: 0.694279]\n",
      "6077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004288] [C loss: 0.694279]\n",
      "6078 [D loss: 0.694282, acc.: 0.00%] [G loss: 0.006858] [C loss: 0.694277]\n",
      "6079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005490] [C loss: 0.694277]\n",
      "6080 [D loss: 0.694280, acc.: 0.00%] [G loss: 0.004701] [C loss: 0.694278]\n",
      "6081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005813] [C loss: 0.694278]\n",
      "6082 [D loss: 0.694279, acc.: 0.00%] [G loss: 0.005138] [C loss: 0.694276]\n",
      "6083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004591] [C loss: 0.694276]\n",
      "6084 [D loss: 0.694278, acc.: 0.00%] [G loss: 0.004984] [C loss: 0.694273]\n",
      "6085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004531] [C loss: 0.694273]\n",
      "6086 [D loss: 0.694275, acc.: 0.00%] [G loss: 0.008636] [C loss: 0.694273]\n",
      "6087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005843] [C loss: 0.694273]\n",
      "6088 [D loss: 0.694275, acc.: 0.00%] [G loss: 0.015393] [C loss: 0.694271]\n",
      "6089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007111] [C loss: 0.694271]\n",
      "6090 [D loss: 0.694273, acc.: 0.00%] [G loss: 0.006221] [C loss: 0.694271]\n",
      "6091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011310] [C loss: 0.694271]\n",
      "6092 [D loss: 0.694272, acc.: 0.00%] [G loss: 0.005641] [C loss: 0.694268]\n",
      "6093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004758] [C loss: 0.694268]\n",
      "6094 [D loss: 0.694270, acc.: 0.00%] [G loss: 0.006207] [C loss: 0.694267]\n",
      "6095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005998] [C loss: 0.694267]\n",
      "6096 [D loss: 0.694268, acc.: 0.00%] [G loss: 0.006506] [C loss: 0.694266]\n",
      "6097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006958] [C loss: 0.694266]\n",
      "6098 [D loss: 0.694267, acc.: 0.00%] [G loss: 0.004747] [C loss: 0.694264]\n",
      "6099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004522] [C loss: 0.694264]\n",
      "6100 [D loss: 0.694265, acc.: 0.00%] [G loss: 0.006916] [C loss: 0.694264]\n",
      "6101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004167] [C loss: 0.694264]\n",
      "6102 [D loss: 0.694265, acc.: 0.00%] [G loss: 0.006404] [C loss: 0.694261]\n",
      "6103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009097] [C loss: 0.694261]\n",
      "6104 [D loss: 0.694263, acc.: 0.00%] [G loss: 0.006610] [C loss: 0.694260]\n",
      "6105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005512] [C loss: 0.694260]\n",
      "6106 [D loss: 0.694262, acc.: 0.00%] [G loss: 0.005125] [C loss: 0.694258]\n",
      "6107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005301] [C loss: 0.694258]\n",
      "6108 [D loss: 0.694260, acc.: 0.00%] [G loss: 0.005976] [C loss: 0.694257]\n",
      "6109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004938] [C loss: 0.694257]\n",
      "6110 [D loss: 0.694259, acc.: 0.00%] [G loss: 0.004027] [C loss: 0.694255]\n",
      "6111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.694255]\n",
      "6112 [D loss: 0.694258, acc.: 0.00%] [G loss: 0.010952] [C loss: 0.694255]\n",
      "6113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019010] [C loss: 0.694255]\n",
      "6114 [D loss: 0.694256, acc.: 0.00%] [G loss: 0.004893] [C loss: 0.694253]\n",
      "6115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004008] [C loss: 0.694253]\n",
      "6116 [D loss: 0.694254, acc.: 0.00%] [G loss: 0.004901] [C loss: 0.694252]\n",
      "6117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005102] [C loss: 0.694252]\n",
      "6118 [D loss: 0.694254, acc.: 0.00%] [G loss: 0.006178] [C loss: 0.694249]\n",
      "6119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004156] [C loss: 0.694249]\n",
      "6120 [D loss: 0.694252, acc.: 0.00%] [G loss: 0.005404] [C loss: 0.694247]\n",
      "6121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005287] [C loss: 0.694247]\n",
      "6122 [D loss: 0.694251, acc.: 0.00%] [G loss: 0.006357] [C loss: 0.694245]\n",
      "6123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006843] [C loss: 0.694245]\n",
      "6124 [D loss: 0.694249, acc.: 0.00%] [G loss: 0.008365] [C loss: 0.694245]\n",
      "6125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007762] [C loss: 0.694245]\n",
      "6126 [D loss: 0.694248, acc.: 0.00%] [G loss: 0.004370] [C loss: 0.694244]\n",
      "6127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004300] [C loss: 0.694244]\n",
      "6128 [D loss: 0.694247, acc.: 0.00%] [G loss: 0.004709] [C loss: 0.694243]\n",
      "6129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004320] [C loss: 0.694243]\n",
      "6130 [D loss: 0.694245, acc.: 0.00%] [G loss: 0.003671] [C loss: 0.694241]\n",
      "6131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005781] [C loss: 0.694241]\n",
      "6132 [D loss: 0.694244, acc.: 0.00%] [G loss: 0.006438] [C loss: 0.694239]\n",
      "6133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005444] [C loss: 0.694239]\n",
      "6134 [D loss: 0.694242, acc.: 0.00%] [G loss: 0.005923] [C loss: 0.694238]\n",
      "6135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006936] [C loss: 0.694238]\n",
      "6136 [D loss: 0.694241, acc.: 0.00%] [G loss: 0.003636] [C loss: 0.694236]\n",
      "6137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.694236]\n",
      "6138 [D loss: 0.694239, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.694237]\n",
      "6139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017044] [C loss: 0.694237]\n",
      "6140 [D loss: 0.694238, acc.: 0.00%] [G loss: 0.004696] [C loss: 0.694234]\n",
      "6141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004898] [C loss: 0.694234]\n",
      "6142 [D loss: 0.694236, acc.: 0.00%] [G loss: 0.004104] [C loss: 0.694234]\n",
      "6143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006525] [C loss: 0.694234]\n",
      "6144 [D loss: 0.694236, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.694231]\n",
      "6145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003388] [C loss: 0.694231]\n",
      "6146 [D loss: 0.694233, acc.: 0.00%] [G loss: 0.003602] [C loss: 0.694230]\n",
      "6147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005074] [C loss: 0.694230]\n",
      "6148 [D loss: 0.694232, acc.: 0.00%] [G loss: 0.007339] [C loss: 0.694229]\n",
      "6149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009471] [C loss: 0.694229]\n",
      "6150 [D loss: 0.694231, acc.: 0.00%] [G loss: 0.004905] [C loss: 0.694227]\n",
      "6151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005802] [C loss: 0.694227]\n",
      "6152 [D loss: 0.694229, acc.: 0.00%] [G loss: 0.003777] [C loss: 0.694226]\n",
      "6153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007444] [C loss: 0.694226]\n",
      "6154 [D loss: 0.694228, acc.: 0.00%] [G loss: 0.004039] [C loss: 0.694225]\n",
      "6155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005496] [C loss: 0.694225]\n",
      "6156 [D loss: 0.694227, acc.: 0.00%] [G loss: 0.004853] [C loss: 0.694224]\n",
      "6157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007294] [C loss: 0.694224]\n",
      "6158 [D loss: 0.694226, acc.: 0.00%] [G loss: 0.004774] [C loss: 0.694222]\n",
      "6159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004397] [C loss: 0.694222]\n",
      "6160 [D loss: 0.694224, acc.: 0.00%] [G loss: 0.005314] [C loss: 0.694220]\n",
      "6161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007913] [C loss: 0.694220]\n",
      "6162 [D loss: 0.694222, acc.: 0.00%] [G loss: 0.006843] [C loss: 0.694220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005447] [C loss: 0.694220]\n",
      "6164 [D loss: 0.694222, acc.: 0.00%] [G loss: 0.005180] [C loss: 0.694216]\n",
      "6165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009822] [C loss: 0.694216]\n",
      "6166 [D loss: 0.694219, acc.: 0.00%] [G loss: 0.005020] [C loss: 0.694216]\n",
      "6167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005331] [C loss: 0.694216]\n",
      "6168 [D loss: 0.694218, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.694215]\n",
      "6169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005232] [C loss: 0.694215]\n",
      "6170 [D loss: 0.694218, acc.: 0.00%] [G loss: 0.006445] [C loss: 0.694214]\n",
      "6171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004527] [C loss: 0.694214]\n",
      "6172 [D loss: 0.694216, acc.: 0.00%] [G loss: 0.006466] [C loss: 0.694212]\n",
      "6173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006745] [C loss: 0.694212]\n",
      "6174 [D loss: 0.694215, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.694211]\n",
      "6175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.694211]\n",
      "6176 [D loss: 0.694213, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.694209]\n",
      "6177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007800] [C loss: 0.694209]\n",
      "6178 [D loss: 0.694211, acc.: 0.00%] [G loss: 0.006281] [C loss: 0.694207]\n",
      "6179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003897] [C loss: 0.694207]\n",
      "6180 [D loss: 0.694210, acc.: 0.00%] [G loss: 0.005850] [C loss: 0.694207]\n",
      "6181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006205] [C loss: 0.694207]\n",
      "6182 [D loss: 0.694210, acc.: 0.00%] [G loss: 0.005331] [C loss: 0.694204]\n",
      "6183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007762] [C loss: 0.694204]\n",
      "6184 [D loss: 0.694207, acc.: 0.00%] [G loss: 0.003652] [C loss: 0.694205]\n",
      "6185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005825] [C loss: 0.694205]\n",
      "6186 [D loss: 0.694208, acc.: 0.00%] [G loss: 0.007403] [C loss: 0.694201]\n",
      "6187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007336] [C loss: 0.694201]\n",
      "6188 [D loss: 0.694205, acc.: 0.00%] [G loss: 0.005761] [C loss: 0.694201]\n",
      "6189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005936] [C loss: 0.694201]\n",
      "6190 [D loss: 0.694203, acc.: 0.00%] [G loss: 0.004016] [C loss: 0.694200]\n",
      "6191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004304] [C loss: 0.694200]\n",
      "6192 [D loss: 0.694203, acc.: 0.00%] [G loss: 0.004380] [C loss: 0.694199]\n",
      "6193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005381] [C loss: 0.694199]\n",
      "6194 [D loss: 0.694201, acc.: 0.00%] [G loss: 0.003873] [C loss: 0.694197]\n",
      "6195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004695] [C loss: 0.694197]\n",
      "6196 [D loss: 0.694198, acc.: 0.00%] [G loss: 0.004990] [C loss: 0.694196]\n",
      "6197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003852] [C loss: 0.694196]\n",
      "6198 [D loss: 0.694198, acc.: 0.00%] [G loss: 0.005170] [C loss: 0.694195]\n",
      "6199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003792] [C loss: 0.694195]\n",
      "6200 [D loss: 0.694196, acc.: 0.00%] [G loss: 0.008272] [C loss: 0.694194]\n",
      "6201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006701] [C loss: 0.694194]\n",
      "6202 [D loss: 0.694195, acc.: 0.00%] [G loss: 0.003914] [C loss: 0.694191]\n",
      "6203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004129] [C loss: 0.694191]\n",
      "6204 [D loss: 0.694193, acc.: 0.00%] [G loss: 0.006438] [C loss: 0.694191]\n",
      "6205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020188] [C loss: 0.694191]\n",
      "6206 [D loss: 0.694193, acc.: 0.00%] [G loss: 0.004685] [C loss: 0.694189]\n",
      "6207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006939] [C loss: 0.694189]\n",
      "6208 [D loss: 0.694191, acc.: 0.00%] [G loss: 0.008050] [C loss: 0.694188]\n",
      "6209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008814] [C loss: 0.694188]\n",
      "6210 [D loss: 0.694190, acc.: 0.00%] [G loss: 0.005416] [C loss: 0.694186]\n",
      "6211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005305] [C loss: 0.694186]\n",
      "6212 [D loss: 0.694188, acc.: 0.00%] [G loss: 0.005113] [C loss: 0.694184]\n",
      "6213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006329] [C loss: 0.694184]\n",
      "6214 [D loss: 0.694187, acc.: 0.00%] [G loss: 0.007484] [C loss: 0.694184]\n",
      "6215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.694184]\n",
      "6216 [D loss: 0.694186, acc.: 0.00%] [G loss: 0.005140] [C loss: 0.694183]\n",
      "6217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005992] [C loss: 0.694183]\n",
      "6218 [D loss: 0.694184, acc.: 0.00%] [G loss: 0.007244] [C loss: 0.694181]\n",
      "6219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004667] [C loss: 0.694181]\n",
      "6220 [D loss: 0.694183, acc.: 0.00%] [G loss: 0.005941] [C loss: 0.694179]\n",
      "6221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006140] [C loss: 0.694179]\n",
      "6222 [D loss: 0.694181, acc.: 0.00%] [G loss: 0.004052] [C loss: 0.694179]\n",
      "6223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013694] [C loss: 0.694179]\n",
      "6224 [D loss: 0.694180, acc.: 0.00%] [G loss: 0.004468] [C loss: 0.694175]\n",
      "6225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004068] [C loss: 0.694175]\n",
      "6226 [D loss: 0.694178, acc.: 0.00%] [G loss: 0.007223] [C loss: 0.694174]\n",
      "6227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005664] [C loss: 0.694174]\n",
      "6228 [D loss: 0.694177, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.694173]\n",
      "6229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006487] [C loss: 0.694173]\n",
      "6230 [D loss: 0.694175, acc.: 0.00%] [G loss: 0.004918] [C loss: 0.694173]\n",
      "6231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006381] [C loss: 0.694173]\n",
      "6232 [D loss: 0.694174, acc.: 0.00%] [G loss: 0.004014] [C loss: 0.694172]\n",
      "6233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005149] [C loss: 0.694172]\n",
      "6234 [D loss: 0.694174, acc.: 0.00%] [G loss: 0.005103] [C loss: 0.694171]\n",
      "6235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007168] [C loss: 0.694171]\n",
      "6236 [D loss: 0.694174, acc.: 0.00%] [G loss: 0.003228] [C loss: 0.694168]\n",
      "6237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004084] [C loss: 0.694168]\n",
      "6238 [D loss: 0.694171, acc.: 0.00%] [G loss: 0.016645] [C loss: 0.694169]\n",
      "6239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005419] [C loss: 0.694169]\n",
      "6240 [D loss: 0.694170, acc.: 0.00%] [G loss: 0.008866] [C loss: 0.694166]\n",
      "6241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005910] [C loss: 0.694166]\n",
      "6242 [D loss: 0.694169, acc.: 0.00%] [G loss: 0.004231] [C loss: 0.694165]\n",
      "6243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006813] [C loss: 0.694165]\n",
      "6244 [D loss: 0.694167, acc.: 0.00%] [G loss: 0.005968] [C loss: 0.694163]\n",
      "6245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011775] [C loss: 0.694163]\n",
      "6246 [D loss: 0.694165, acc.: 0.00%] [G loss: 0.005032] [C loss: 0.694162]\n",
      "6247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007512] [C loss: 0.694162]\n",
      "6248 [D loss: 0.694165, acc.: 0.00%] [G loss: 0.009553] [C loss: 0.694161]\n",
      "6249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004546] [C loss: 0.694161]\n",
      "6250 [D loss: 0.694163, acc.: 0.00%] [G loss: 0.008376] [C loss: 0.694161]\n",
      "6251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004046] [C loss: 0.694161]\n",
      "6252 [D loss: 0.694162, acc.: 0.00%] [G loss: 0.005539] [C loss: 0.694159]\n",
      "6253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005586] [C loss: 0.694159]\n",
      "6254 [D loss: 0.694160, acc.: 0.00%] [G loss: 0.007766] [C loss: 0.694158]\n",
      "6255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005911] [C loss: 0.694158]\n",
      "6256 [D loss: 0.694160, acc.: 0.00%] [G loss: 0.006959] [C loss: 0.694155]\n",
      "6257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004721] [C loss: 0.694155]\n",
      "6258 [D loss: 0.694158, acc.: 0.00%] [G loss: 0.004579] [C loss: 0.694155]\n",
      "6259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004719] [C loss: 0.694155]\n",
      "6260 [D loss: 0.694156, acc.: 0.00%] [G loss: 0.005648] [C loss: 0.694152]\n",
      "6261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005951] [C loss: 0.694152]\n",
      "6262 [D loss: 0.694154, acc.: 0.00%] [G loss: 0.013304] [C loss: 0.694151]\n",
      "6263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003991] [C loss: 0.694151]\n",
      "6264 [D loss: 0.694153, acc.: 0.00%] [G loss: 0.006927] [C loss: 0.694151]\n",
      "6265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008112] [C loss: 0.694151]\n",
      "6266 [D loss: 0.694153, acc.: 0.00%] [G loss: 0.006202] [C loss: 0.694149]\n",
      "6267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007168] [C loss: 0.694149]\n",
      "6268 [D loss: 0.694151, acc.: 0.00%] [G loss: 0.006711] [C loss: 0.694148]\n",
      "6269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.694148]\n",
      "6270 [D loss: 0.694150, acc.: 0.00%] [G loss: 0.004009] [C loss: 0.694145]\n",
      "6271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005277] [C loss: 0.694145]\n",
      "6272 [D loss: 0.694147, acc.: 0.00%] [G loss: 0.004203] [C loss: 0.694145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005939] [C loss: 0.694145]\n",
      "6274 [D loss: 0.694147, acc.: 0.00%] [G loss: 0.005507] [C loss: 0.694143]\n",
      "6275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003913] [C loss: 0.694143]\n",
      "6276 [D loss: 0.694145, acc.: 0.00%] [G loss: 0.004097] [C loss: 0.694141]\n",
      "6277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006118] [C loss: 0.694141]\n",
      "6278 [D loss: 0.694144, acc.: 0.00%] [G loss: 0.004644] [C loss: 0.694140]\n",
      "6279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003617] [C loss: 0.694140]\n",
      "6280 [D loss: 0.694142, acc.: 0.00%] [G loss: 0.005276] [C loss: 0.694139]\n",
      "6281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006854] [C loss: 0.694139]\n",
      "6282 [D loss: 0.694141, acc.: 0.00%] [G loss: 0.004769] [C loss: 0.694137]\n",
      "6283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003952] [C loss: 0.694137]\n",
      "6284 [D loss: 0.694139, acc.: 0.00%] [G loss: 0.003194] [C loss: 0.694135]\n",
      "6285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004324] [C loss: 0.694135]\n",
      "6286 [D loss: 0.694137, acc.: 0.00%] [G loss: 0.007197] [C loss: 0.694135]\n",
      "6287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003969] [C loss: 0.694135]\n",
      "6288 [D loss: 0.694137, acc.: 0.00%] [G loss: 0.007109] [C loss: 0.694134]\n",
      "6289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004780] [C loss: 0.694134]\n",
      "6290 [D loss: 0.694136, acc.: 0.00%] [G loss: 0.005944] [C loss: 0.694132]\n",
      "6291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.694132]\n",
      "6292 [D loss: 0.694134, acc.: 0.00%] [G loss: 0.003963] [C loss: 0.694131]\n",
      "6293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005312] [C loss: 0.694131]\n",
      "6294 [D loss: 0.694133, acc.: 0.00%] [G loss: 0.004412] [C loss: 0.694129]\n",
      "6295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003618] [C loss: 0.694129]\n",
      "6296 [D loss: 0.694132, acc.: 0.00%] [G loss: 0.004919] [C loss: 0.694127]\n",
      "6297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004462] [C loss: 0.694127]\n",
      "6298 [D loss: 0.694130, acc.: 0.00%] [G loss: 0.005798] [C loss: 0.694127]\n",
      "6299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004723] [C loss: 0.694127]\n",
      "6300 [D loss: 0.694129, acc.: 0.00%] [G loss: 0.005789] [C loss: 0.694124]\n",
      "6301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005109] [C loss: 0.694124]\n",
      "6302 [D loss: 0.694127, acc.: 0.00%] [G loss: 0.005517] [C loss: 0.694124]\n",
      "6303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014201] [C loss: 0.694124]\n",
      "6304 [D loss: 0.694126, acc.: 0.00%] [G loss: 0.005732] [C loss: 0.694121]\n",
      "6305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005055] [C loss: 0.694121]\n",
      "6306 [D loss: 0.694124, acc.: 0.00%] [G loss: 0.005082] [C loss: 0.694121]\n",
      "6307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004037] [C loss: 0.694121]\n",
      "6308 [D loss: 0.694123, acc.: 0.00%] [G loss: 0.007987] [C loss: 0.694120]\n",
      "6309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005169] [C loss: 0.694120]\n",
      "6310 [D loss: 0.694122, acc.: 0.00%] [G loss: 0.005605] [C loss: 0.694118]\n",
      "6311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004640] [C loss: 0.694118]\n",
      "6312 [D loss: 0.694121, acc.: 0.00%] [G loss: 0.004661] [C loss: 0.694117]\n",
      "6313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004982] [C loss: 0.694117]\n",
      "6314 [D loss: 0.694119, acc.: 0.00%] [G loss: 0.011160] [C loss: 0.694115]\n",
      "6315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006561] [C loss: 0.694115]\n",
      "6316 [D loss: 0.694118, acc.: 0.00%] [G loss: 0.004977] [C loss: 0.694115]\n",
      "6317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004493] [C loss: 0.694115]\n",
      "6318 [D loss: 0.694117, acc.: 0.00%] [G loss: 0.006064] [C loss: 0.694115]\n",
      "6319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012067] [C loss: 0.694115]\n",
      "6320 [D loss: 0.694116, acc.: 0.00%] [G loss: 0.004840] [C loss: 0.694112]\n",
      "6321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009591] [C loss: 0.694112]\n",
      "6322 [D loss: 0.694115, acc.: 0.00%] [G loss: 0.005377] [C loss: 0.694112]\n",
      "6323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006257] [C loss: 0.694112]\n",
      "6324 [D loss: 0.694113, acc.: 0.00%] [G loss: 0.004182] [C loss: 0.694108]\n",
      "6325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004766] [C loss: 0.694108]\n",
      "6326 [D loss: 0.694111, acc.: 0.00%] [G loss: 0.007100] [C loss: 0.694108]\n",
      "6327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005192] [C loss: 0.694108]\n",
      "6328 [D loss: 0.694111, acc.: 0.00%] [G loss: 0.004795] [C loss: 0.694107]\n",
      "6329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004578] [C loss: 0.694107]\n",
      "6330 [D loss: 0.694109, acc.: 0.00%] [G loss: 0.005705] [C loss: 0.694106]\n",
      "6331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017712] [C loss: 0.694106]\n",
      "6332 [D loss: 0.694108, acc.: 0.00%] [G loss: 0.005103] [C loss: 0.694105]\n",
      "6333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006491] [C loss: 0.694105]\n",
      "6334 [D loss: 0.694106, acc.: 0.00%] [G loss: 0.004425] [C loss: 0.694103]\n",
      "6335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.694103]\n",
      "6336 [D loss: 0.694105, acc.: 0.00%] [G loss: 0.005422] [C loss: 0.694103]\n",
      "6337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005284] [C loss: 0.694103]\n",
      "6338 [D loss: 0.694104, acc.: 0.00%] [G loss: 0.004383] [C loss: 0.694101]\n",
      "6339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005224] [C loss: 0.694101]\n",
      "6340 [D loss: 0.694103, acc.: 0.00%] [G loss: 0.021139] [C loss: 0.694100]\n",
      "6341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004774] [C loss: 0.694100]\n",
      "6342 [D loss: 0.694102, acc.: 0.00%] [G loss: 0.006416] [C loss: 0.694098]\n",
      "6343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006884] [C loss: 0.694098]\n",
      "6344 [D loss: 0.694100, acc.: 0.00%] [G loss: 0.007884] [C loss: 0.694097]\n",
      "6345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004785] [C loss: 0.694097]\n",
      "6346 [D loss: 0.694099, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.694095]\n",
      "6347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005894] [C loss: 0.694095]\n",
      "6348 [D loss: 0.694097, acc.: 0.00%] [G loss: 0.004076] [C loss: 0.694094]\n",
      "6349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006430] [C loss: 0.694094]\n",
      "6350 [D loss: 0.694096, acc.: 0.00%] [G loss: 0.004531] [C loss: 0.694093]\n",
      "6351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005250] [C loss: 0.694093]\n",
      "6352 [D loss: 0.694095, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.694091]\n",
      "6353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006083] [C loss: 0.694091]\n",
      "6354 [D loss: 0.694093, acc.: 0.00%] [G loss: 0.004331] [C loss: 0.694089]\n",
      "6355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007210] [C loss: 0.694089]\n",
      "6356 [D loss: 0.694091, acc.: 0.00%] [G loss: 0.006395] [C loss: 0.694088]\n",
      "6357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004602] [C loss: 0.694088]\n",
      "6358 [D loss: 0.694091, acc.: 0.00%] [G loss: 0.005126] [C loss: 0.694088]\n",
      "6359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007216] [C loss: 0.694088]\n",
      "6360 [D loss: 0.694090, acc.: 0.00%] [G loss: 0.006899] [C loss: 0.694085]\n",
      "6361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006738] [C loss: 0.694085]\n",
      "6362 [D loss: 0.694088, acc.: 0.00%] [G loss: 0.005494] [C loss: 0.694085]\n",
      "6363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005591] [C loss: 0.694085]\n",
      "6364 [D loss: 0.694087, acc.: 0.00%] [G loss: 0.004296] [C loss: 0.694084]\n",
      "6365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006235] [C loss: 0.694084]\n",
      "6366 [D loss: 0.694086, acc.: 0.00%] [G loss: 0.004999] [C loss: 0.694082]\n",
      "6367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004992] [C loss: 0.694082]\n",
      "6368 [D loss: 0.694084, acc.: 0.00%] [G loss: 0.004526] [C loss: 0.694081]\n",
      "6369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007234] [C loss: 0.694081]\n",
      "6370 [D loss: 0.694083, acc.: 0.00%] [G loss: 0.003731] [C loss: 0.694080]\n",
      "6371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005492] [C loss: 0.694080]\n",
      "6372 [D loss: 0.694081, acc.: 0.00%] [G loss: 0.006718] [C loss: 0.694077]\n",
      "6373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005311] [C loss: 0.694077]\n",
      "6374 [D loss: 0.694080, acc.: 0.00%] [G loss: 0.004434] [C loss: 0.694077]\n",
      "6375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004064] [C loss: 0.694077]\n",
      "6376 [D loss: 0.694079, acc.: 0.00%] [G loss: 0.004362] [C loss: 0.694076]\n",
      "6377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004577] [C loss: 0.694076]\n",
      "6378 [D loss: 0.694077, acc.: 0.00%] [G loss: 0.006575] [C loss: 0.694076]\n",
      "6379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009211] [C loss: 0.694076]\n",
      "6380 [D loss: 0.694077, acc.: 0.00%] [G loss: 0.009426] [C loss: 0.694073]\n",
      "6381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004004] [C loss: 0.694073]\n",
      "6382 [D loss: 0.694074, acc.: 0.00%] [G loss: 0.003793] [C loss: 0.694073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005887] [C loss: 0.694073]\n",
      "6384 [D loss: 0.694074, acc.: 0.00%] [G loss: 0.005788] [C loss: 0.694072]\n",
      "6385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005644] [C loss: 0.694072]\n",
      "6386 [D loss: 0.694073, acc.: 0.00%] [G loss: 0.005545] [C loss: 0.694071]\n",
      "6387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004407] [C loss: 0.694071]\n",
      "6388 [D loss: 0.694071, acc.: 0.00%] [G loss: 0.004768] [C loss: 0.694068]\n",
      "6389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004181] [C loss: 0.694068]\n",
      "6390 [D loss: 0.694070, acc.: 0.00%] [G loss: 0.005089] [C loss: 0.694068]\n",
      "6391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005764] [C loss: 0.694068]\n",
      "6392 [D loss: 0.694069, acc.: 0.00%] [G loss: 0.006250] [C loss: 0.694065]\n",
      "6393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003882] [C loss: 0.694065]\n",
      "6394 [D loss: 0.694067, acc.: 0.00%] [G loss: 0.003268] [C loss: 0.694065]\n",
      "6395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008592] [C loss: 0.694065]\n",
      "6396 [D loss: 0.694067, acc.: 0.00%] [G loss: 0.004728] [C loss: 0.694062]\n",
      "6397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005419] [C loss: 0.694062]\n",
      "6398 [D loss: 0.694065, acc.: 0.00%] [G loss: 0.005743] [C loss: 0.694062]\n",
      "6399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005396] [C loss: 0.694062]\n",
      "6400 [D loss: 0.694064, acc.: 0.00%] [G loss: 0.006164] [C loss: 0.694060]\n",
      "6401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.694060]\n",
      "6402 [D loss: 0.694061, acc.: 0.00%] [G loss: 0.003819] [C loss: 0.694059]\n",
      "6403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005453] [C loss: 0.694059]\n",
      "6404 [D loss: 0.694061, acc.: 0.00%] [G loss: 0.006709] [C loss: 0.694057]\n",
      "6405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007349] [C loss: 0.694057]\n",
      "6406 [D loss: 0.694060, acc.: 0.00%] [G loss: 0.007221] [C loss: 0.694056]\n",
      "6407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007680] [C loss: 0.694056]\n",
      "6408 [D loss: 0.694059, acc.: 0.00%] [G loss: 0.013072] [C loss: 0.694056]\n",
      "6409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004117] [C loss: 0.694056]\n",
      "6410 [D loss: 0.694057, acc.: 0.00%] [G loss: 0.005622] [C loss: 0.694054]\n",
      "6411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004935] [C loss: 0.694054]\n",
      "6412 [D loss: 0.694056, acc.: 0.00%] [G loss: 0.004630] [C loss: 0.694052]\n",
      "6413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007344] [C loss: 0.694052]\n",
      "6414 [D loss: 0.694054, acc.: 0.00%] [G loss: 0.006575] [C loss: 0.694051]\n",
      "6415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004637] [C loss: 0.694051]\n",
      "6416 [D loss: 0.694053, acc.: 0.00%] [G loss: 0.007880] [C loss: 0.694049]\n",
      "6417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007270] [C loss: 0.694049]\n",
      "6418 [D loss: 0.694052, acc.: 0.00%] [G loss: 0.006287] [C loss: 0.694049]\n",
      "6419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004315] [C loss: 0.694049]\n",
      "6420 [D loss: 0.694050, acc.: 0.00%] [G loss: 0.008738] [C loss: 0.694046]\n",
      "6421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011507] [C loss: 0.694046]\n",
      "6422 [D loss: 0.694048, acc.: 0.00%] [G loss: 0.003878] [C loss: 0.694046]\n",
      "6423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006678] [C loss: 0.694046]\n",
      "6424 [D loss: 0.694048, acc.: 0.00%] [G loss: 0.003504] [C loss: 0.694045]\n",
      "6425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009321] [C loss: 0.694045]\n",
      "6426 [D loss: 0.694047, acc.: 0.00%] [G loss: 0.007773] [C loss: 0.694044]\n",
      "6427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017610] [C loss: 0.694044]\n",
      "6428 [D loss: 0.694046, acc.: 0.00%] [G loss: 0.007200] [C loss: 0.694043]\n",
      "6429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005724] [C loss: 0.694043]\n",
      "6430 [D loss: 0.694045, acc.: 0.00%] [G loss: 0.004393] [C loss: 0.694041]\n",
      "6431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006476] [C loss: 0.694041]\n",
      "6432 [D loss: 0.694043, acc.: 0.00%] [G loss: 0.004496] [C loss: 0.694040]\n",
      "6433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004078] [C loss: 0.694040]\n",
      "6434 [D loss: 0.694042, acc.: 0.00%] [G loss: 0.004434] [C loss: 0.694039]\n",
      "6435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005235] [C loss: 0.694039]\n",
      "6436 [D loss: 0.694040, acc.: 0.00%] [G loss: 0.006103] [C loss: 0.694037]\n",
      "6437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004638] [C loss: 0.694037]\n",
      "6438 [D loss: 0.694039, acc.: 0.00%] [G loss: 0.003761] [C loss: 0.694035]\n",
      "6439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011389] [C loss: 0.694035]\n",
      "6440 [D loss: 0.694037, acc.: 0.00%] [G loss: 0.004319] [C loss: 0.694035]\n",
      "6441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005065] [C loss: 0.694035]\n",
      "6442 [D loss: 0.694036, acc.: 0.00%] [G loss: 0.006105] [C loss: 0.694032]\n",
      "6443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006712] [C loss: 0.694032]\n",
      "6444 [D loss: 0.694034, acc.: 0.00%] [G loss: 0.006210] [C loss: 0.694032]\n",
      "6445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006011] [C loss: 0.694032]\n",
      "6446 [D loss: 0.694034, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.694030]\n",
      "6447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006338] [C loss: 0.694030]\n",
      "6448 [D loss: 0.694032, acc.: 0.00%] [G loss: 0.005740] [C loss: 0.694028]\n",
      "6449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004940] [C loss: 0.694028]\n",
      "6450 [D loss: 0.694031, acc.: 0.00%] [G loss: 0.005758] [C loss: 0.694029]\n",
      "6451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007161] [C loss: 0.694029]\n",
      "6452 [D loss: 0.694031, acc.: 0.00%] [G loss: 0.005649] [C loss: 0.694025]\n",
      "6453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006645] [C loss: 0.694025]\n",
      "6454 [D loss: 0.694029, acc.: 0.00%] [G loss: 0.006748] [C loss: 0.694026]\n",
      "6455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007280] [C loss: 0.694026]\n",
      "6456 [D loss: 0.694027, acc.: 0.00%] [G loss: 0.022001] [C loss: 0.694024]\n",
      "6457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005960] [C loss: 0.694024]\n",
      "6458 [D loss: 0.694027, acc.: 0.00%] [G loss: 0.005664] [C loss: 0.694024]\n",
      "6459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007381] [C loss: 0.694024]\n",
      "6460 [D loss: 0.694025, acc.: 0.00%] [G loss: 0.007142] [C loss: 0.694021]\n",
      "6461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006767] [C loss: 0.694021]\n",
      "6462 [D loss: 0.694023, acc.: 0.00%] [G loss: 0.003759] [C loss: 0.694021]\n",
      "6463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013256] [C loss: 0.694021]\n",
      "6464 [D loss: 0.694023, acc.: 0.00%] [G loss: 0.005705] [C loss: 0.694019]\n",
      "6465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005992] [C loss: 0.694019]\n",
      "6466 [D loss: 0.694022, acc.: 0.00%] [G loss: 0.006905] [C loss: 0.694017]\n",
      "6467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.694017]\n",
      "6468 [D loss: 0.694019, acc.: 0.00%] [G loss: 0.004727] [C loss: 0.694016]\n",
      "6469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004605] [C loss: 0.694016]\n",
      "6470 [D loss: 0.694018, acc.: 0.00%] [G loss: 0.004702] [C loss: 0.694016]\n",
      "6471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009184] [C loss: 0.694016]\n",
      "6472 [D loss: 0.694018, acc.: 0.00%] [G loss: 0.004252] [C loss: 0.694013]\n",
      "6473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006834] [C loss: 0.694013]\n",
      "6474 [D loss: 0.694016, acc.: 0.00%] [G loss: 0.005540] [C loss: 0.694012]\n",
      "6475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004666] [C loss: 0.694012]\n",
      "6476 [D loss: 0.694015, acc.: 0.00%] [G loss: 0.004637] [C loss: 0.694011]\n",
      "6477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003818] [C loss: 0.694011]\n",
      "6478 [D loss: 0.694013, acc.: 0.00%] [G loss: 0.004260] [C loss: 0.694010]\n",
      "6479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011056] [C loss: 0.694010]\n",
      "6480 [D loss: 0.694012, acc.: 0.00%] [G loss: 0.006982] [C loss: 0.694009]\n",
      "6481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006275] [C loss: 0.694009]\n",
      "6482 [D loss: 0.694011, acc.: 0.00%] [G loss: 0.004801] [C loss: 0.694007]\n",
      "6483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006692] [C loss: 0.694007]\n",
      "6484 [D loss: 0.694009, acc.: 0.00%] [G loss: 0.005233] [C loss: 0.694006]\n",
      "6485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006948] [C loss: 0.694006]\n",
      "6486 [D loss: 0.694009, acc.: 0.00%] [G loss: 0.005887] [C loss: 0.694006]\n",
      "6487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005474] [C loss: 0.694006]\n",
      "6488 [D loss: 0.694008, acc.: 0.00%] [G loss: 0.005138] [C loss: 0.694002]\n",
      "6489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004286] [C loss: 0.694002]\n",
      "6490 [D loss: 0.694005, acc.: 0.00%] [G loss: 0.006450] [C loss: 0.694003]\n",
      "6491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004867] [C loss: 0.694003]\n",
      "6492 [D loss: 0.694005, acc.: 0.00%] [G loss: 0.005221] [C loss: 0.694001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004336] [C loss: 0.694001]\n",
      "6494 [D loss: 0.694004, acc.: 0.00%] [G loss: 0.005809] [C loss: 0.694000]\n",
      "6495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005824] [C loss: 0.694000]\n",
      "6496 [D loss: 0.694002, acc.: 0.00%] [G loss: 0.006069] [C loss: 0.693998]\n",
      "6497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004310] [C loss: 0.693998]\n",
      "6498 [D loss: 0.694001, acc.: 0.00%] [G loss: 0.009261] [C loss: 0.693997]\n",
      "6499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004077] [C loss: 0.693997]\n",
      "6500 [D loss: 0.694000, acc.: 0.00%] [G loss: 0.004278] [C loss: 0.693997]\n",
      "6501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005324] [C loss: 0.693997]\n",
      "6502 [D loss: 0.693998, acc.: 0.00%] [G loss: 0.005056] [C loss: 0.693995]\n",
      "6503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004403] [C loss: 0.693995]\n",
      "6504 [D loss: 0.693996, acc.: 0.00%] [G loss: 0.004022] [C loss: 0.693994]\n",
      "6505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006044] [C loss: 0.693994]\n",
      "6506 [D loss: 0.693996, acc.: 0.00%] [G loss: 0.004044] [C loss: 0.693993]\n",
      "6507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004363] [C loss: 0.693993]\n",
      "6508 [D loss: 0.693995, acc.: 0.00%] [G loss: 0.006178] [C loss: 0.693991]\n",
      "6509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011878] [C loss: 0.693991]\n",
      "6510 [D loss: 0.693994, acc.: 0.00%] [G loss: 0.004439] [C loss: 0.693990]\n",
      "6511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004988] [C loss: 0.693990]\n",
      "6512 [D loss: 0.693993, acc.: 0.00%] [G loss: 0.004109] [C loss: 0.693988]\n",
      "6513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011729] [C loss: 0.693988]\n",
      "6514 [D loss: 0.693991, acc.: 0.00%] [G loss: 0.004809] [C loss: 0.693988]\n",
      "6515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006711] [C loss: 0.693988]\n",
      "6516 [D loss: 0.693990, acc.: 0.00%] [G loss: 0.007852] [C loss: 0.693988]\n",
      "6517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004107] [C loss: 0.693988]\n",
      "6518 [D loss: 0.693990, acc.: 0.00%] [G loss: 0.003985] [C loss: 0.693986]\n",
      "6519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012808] [C loss: 0.693986]\n",
      "6520 [D loss: 0.693988, acc.: 0.00%] [G loss: 0.005288] [C loss: 0.693985]\n",
      "6521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005423] [C loss: 0.693985]\n",
      "6522 [D loss: 0.693986, acc.: 0.00%] [G loss: 0.005777] [C loss: 0.693983]\n",
      "6523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006285] [C loss: 0.693983]\n",
      "6524 [D loss: 0.693986, acc.: 0.00%] [G loss: 0.003616] [C loss: 0.693983]\n",
      "6525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004825] [C loss: 0.693983]\n",
      "6526 [D loss: 0.693984, acc.: 0.00%] [G loss: 0.003661] [C loss: 0.693981]\n",
      "6527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015351] [C loss: 0.693981]\n",
      "6528 [D loss: 0.693983, acc.: 0.00%] [G loss: 0.004612] [C loss: 0.693979]\n",
      "6529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006876] [C loss: 0.693979]\n",
      "6530 [D loss: 0.693981, acc.: 0.00%] [G loss: 0.005418] [C loss: 0.693978]\n",
      "6531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018754] [C loss: 0.693978]\n",
      "6532 [D loss: 0.693980, acc.: 0.00%] [G loss: 0.003947] [C loss: 0.693977]\n",
      "6533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006029] [C loss: 0.693977]\n",
      "6534 [D loss: 0.693979, acc.: 0.00%] [G loss: 0.006158] [C loss: 0.693976]\n",
      "6535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006938] [C loss: 0.693976]\n",
      "6536 [D loss: 0.693978, acc.: 0.00%] [G loss: 0.004358] [C loss: 0.693976]\n",
      "6537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005401] [C loss: 0.693976]\n",
      "6538 [D loss: 0.693977, acc.: 0.00%] [G loss: 0.005641] [C loss: 0.693974]\n",
      "6539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006167] [C loss: 0.693974]\n",
      "6540 [D loss: 0.693975, acc.: 0.00%] [G loss: 0.004342] [C loss: 0.693973]\n",
      "6541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006707] [C loss: 0.693973]\n",
      "6542 [D loss: 0.693974, acc.: 0.00%] [G loss: 0.004057] [C loss: 0.693971]\n",
      "6543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005244] [C loss: 0.693971]\n",
      "6544 [D loss: 0.693972, acc.: 0.00%] [G loss: 0.003340] [C loss: 0.693970]\n",
      "6545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004051] [C loss: 0.693970]\n",
      "6546 [D loss: 0.693971, acc.: 0.00%] [G loss: 0.004538] [C loss: 0.693968]\n",
      "6547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011955] [C loss: 0.693968]\n",
      "6548 [D loss: 0.693970, acc.: 0.00%] [G loss: 0.004763] [C loss: 0.693968]\n",
      "6549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004383] [C loss: 0.693968]\n",
      "6550 [D loss: 0.693969, acc.: 0.00%] [G loss: 0.003923] [C loss: 0.693966]\n",
      "6551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005419] [C loss: 0.693966]\n",
      "6552 [D loss: 0.693967, acc.: 0.00%] [G loss: 0.004282] [C loss: 0.693964]\n",
      "6553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008051] [C loss: 0.693964]\n",
      "6554 [D loss: 0.693966, acc.: 0.00%] [G loss: 0.003963] [C loss: 0.693963]\n",
      "6555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003677] [C loss: 0.693963]\n",
      "6556 [D loss: 0.693965, acc.: 0.00%] [G loss: 0.009115] [C loss: 0.693962]\n",
      "6557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004461] [C loss: 0.693962]\n",
      "6558 [D loss: 0.693964, acc.: 0.00%] [G loss: 0.005267] [C loss: 0.693962]\n",
      "6559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006967] [C loss: 0.693962]\n",
      "6560 [D loss: 0.693963, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.693962]\n",
      "6561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005364] [C loss: 0.693962]\n",
      "6562 [D loss: 0.693963, acc.: 0.00%] [G loss: 0.004505] [C loss: 0.693959]\n",
      "6563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005818] [C loss: 0.693959]\n",
      "6564 [D loss: 0.693960, acc.: 0.00%] [G loss: 0.004347] [C loss: 0.693959]\n",
      "6565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.693959]\n",
      "6566 [D loss: 0.693960, acc.: 0.00%] [G loss: 0.004619] [C loss: 0.693955]\n",
      "6567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006036] [C loss: 0.693955]\n",
      "6568 [D loss: 0.693958, acc.: 0.00%] [G loss: 0.004185] [C loss: 0.693956]\n",
      "6569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011760] [C loss: 0.693956]\n",
      "6570 [D loss: 0.693958, acc.: 0.00%] [G loss: 0.005640] [C loss: 0.693954]\n",
      "6571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008720] [C loss: 0.693954]\n",
      "6572 [D loss: 0.693956, acc.: 0.00%] [G loss: 0.006998] [C loss: 0.693952]\n",
      "6573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005021] [C loss: 0.693952]\n",
      "6574 [D loss: 0.693954, acc.: 0.00%] [G loss: 0.006699] [C loss: 0.693951]\n",
      "6575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004850] [C loss: 0.693951]\n",
      "6576 [D loss: 0.693953, acc.: 0.00%] [G loss: 0.005687] [C loss: 0.693950]\n",
      "6577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005196] [C loss: 0.693950]\n",
      "6578 [D loss: 0.693951, acc.: 0.00%] [G loss: 0.006556] [C loss: 0.693949]\n",
      "6579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004517] [C loss: 0.693949]\n",
      "6580 [D loss: 0.693951, acc.: 0.00%] [G loss: 0.006065] [C loss: 0.693948]\n",
      "6581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012598] [C loss: 0.693948]\n",
      "6582 [D loss: 0.693950, acc.: 0.00%] [G loss: 0.005185] [C loss: 0.693946]\n",
      "6583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006032] [C loss: 0.693946]\n",
      "6584 [D loss: 0.693949, acc.: 0.00%] [G loss: 0.005753] [C loss: 0.693945]\n",
      "6585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005536] [C loss: 0.693945]\n",
      "6586 [D loss: 0.693948, acc.: 0.00%] [G loss: 0.005157] [C loss: 0.693943]\n",
      "6587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007812] [C loss: 0.693943]\n",
      "6588 [D loss: 0.693946, acc.: 0.00%] [G loss: 0.004289] [C loss: 0.693943]\n",
      "6589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007213] [C loss: 0.693943]\n",
      "6590 [D loss: 0.693945, acc.: 0.00%] [G loss: 0.005489] [C loss: 0.693942]\n",
      "6591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004395] [C loss: 0.693942]\n",
      "6592 [D loss: 0.693943, acc.: 0.00%] [G loss: 0.006073] [C loss: 0.693941]\n",
      "6593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006714] [C loss: 0.693941]\n",
      "6594 [D loss: 0.693942, acc.: 0.00%] [G loss: 0.005081] [C loss: 0.693939]\n",
      "6595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004836] [C loss: 0.693939]\n",
      "6596 [D loss: 0.693941, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.693937]\n",
      "6597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005051] [C loss: 0.693937]\n",
      "6598 [D loss: 0.693940, acc.: 0.00%] [G loss: 0.004670] [C loss: 0.693937]\n",
      "6599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004248] [C loss: 0.693937]\n",
      "6600 [D loss: 0.693939, acc.: 0.00%] [G loss: 0.009389] [C loss: 0.693935]\n",
      "6601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003645] [C loss: 0.693935]\n",
      "6602 [D loss: 0.693937, acc.: 0.00%] [G loss: 0.005591] [C loss: 0.693934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005916] [C loss: 0.693934]\n",
      "6604 [D loss: 0.693936, acc.: 0.00%] [G loss: 0.003640] [C loss: 0.693933]\n",
      "6605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020245] [C loss: 0.693933]\n",
      "6606 [D loss: 0.693935, acc.: 0.00%] [G loss: 0.004510] [C loss: 0.693932]\n",
      "6607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008636] [C loss: 0.693932]\n",
      "6608 [D loss: 0.693933, acc.: 0.00%] [G loss: 0.005009] [C loss: 0.693931]\n",
      "6609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006169] [C loss: 0.693931]\n",
      "6610 [D loss: 0.693933, acc.: 0.00%] [G loss: 0.006334] [C loss: 0.693929]\n",
      "6611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009462] [C loss: 0.693929]\n",
      "6612 [D loss: 0.693931, acc.: 0.00%] [G loss: 0.007634] [C loss: 0.693928]\n",
      "6613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005637] [C loss: 0.693928]\n",
      "6614 [D loss: 0.693930, acc.: 0.00%] [G loss: 0.004086] [C loss: 0.693927]\n",
      "6615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004937] [C loss: 0.693927]\n",
      "6616 [D loss: 0.693928, acc.: 0.00%] [G loss: 0.003392] [C loss: 0.693925]\n",
      "6617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005749] [C loss: 0.693925]\n",
      "6618 [D loss: 0.693927, acc.: 0.00%] [G loss: 0.005471] [C loss: 0.693923]\n",
      "6619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003981] [C loss: 0.693923]\n",
      "6620 [D loss: 0.693925, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.693923]\n",
      "6621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004021] [C loss: 0.693923]\n",
      "6622 [D loss: 0.693924, acc.: 0.00%] [G loss: 0.002732] [C loss: 0.693922]\n",
      "6623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006323] [C loss: 0.693922]\n",
      "6624 [D loss: 0.693925, acc.: 0.00%] [G loss: 0.010633] [C loss: 0.693920]\n",
      "6625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004395] [C loss: 0.693920]\n",
      "6626 [D loss: 0.693922, acc.: 0.00%] [G loss: 0.006719] [C loss: 0.693920]\n",
      "6627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006108] [C loss: 0.693920]\n",
      "6628 [D loss: 0.693921, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.693917]\n",
      "6629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005924] [C loss: 0.693917]\n",
      "6630 [D loss: 0.693920, acc.: 0.00%] [G loss: 0.005420] [C loss: 0.693917]\n",
      "6631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004387] [C loss: 0.693917]\n",
      "6632 [D loss: 0.693919, acc.: 0.00%] [G loss: 0.003129] [C loss: 0.693916]\n",
      "6633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004557] [C loss: 0.693916]\n",
      "6634 [D loss: 0.693918, acc.: 0.00%] [G loss: 0.005270] [C loss: 0.693915]\n",
      "6635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004439] [C loss: 0.693915]\n",
      "6636 [D loss: 0.693917, acc.: 0.00%] [G loss: 0.006674] [C loss: 0.693913]\n",
      "6637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.693913]\n",
      "6638 [D loss: 0.693915, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.693912]\n",
      "6639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003979] [C loss: 0.693912]\n",
      "6640 [D loss: 0.693914, acc.: 0.00%] [G loss: 0.004793] [C loss: 0.693911]\n",
      "6641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003601] [C loss: 0.693911]\n",
      "6642 [D loss: 0.693914, acc.: 0.00%] [G loss: 0.009784] [C loss: 0.693909]\n",
      "6643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008416] [C loss: 0.693909]\n",
      "6644 [D loss: 0.693911, acc.: 0.00%] [G loss: 0.005189] [C loss: 0.693909]\n",
      "6645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005335] [C loss: 0.693909]\n",
      "6646 [D loss: 0.693911, acc.: 0.00%] [G loss: 0.004009] [C loss: 0.693908]\n",
      "6647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003372] [C loss: 0.693908]\n",
      "6648 [D loss: 0.693909, acc.: 0.00%] [G loss: 0.006971] [C loss: 0.693906]\n",
      "6649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007069] [C loss: 0.693906]\n",
      "6650 [D loss: 0.693908, acc.: 0.00%] [G loss: 0.004876] [C loss: 0.693906]\n",
      "6651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006057] [C loss: 0.693906]\n",
      "6652 [D loss: 0.693908, acc.: 0.00%] [G loss: 0.004391] [C loss: 0.693904]\n",
      "6653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003939] [C loss: 0.693904]\n",
      "6654 [D loss: 0.693907, acc.: 0.00%] [G loss: 0.004920] [C loss: 0.693903]\n",
      "6655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004546] [C loss: 0.693903]\n",
      "6656 [D loss: 0.693904, acc.: 0.00%] [G loss: 0.004837] [C loss: 0.693902]\n",
      "6657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004159] [C loss: 0.693902]\n",
      "6658 [D loss: 0.693904, acc.: 0.00%] [G loss: 0.003768] [C loss: 0.693901]\n",
      "6659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007850] [C loss: 0.693901]\n",
      "6660 [D loss: 0.693903, acc.: 0.00%] [G loss: 0.016744] [C loss: 0.693900]\n",
      "6661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.693900]\n",
      "6662 [D loss: 0.693901, acc.: 0.00%] [G loss: 0.005131] [C loss: 0.693898]\n",
      "6663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006188] [C loss: 0.693898]\n",
      "6664 [D loss: 0.693901, acc.: 0.00%] [G loss: 0.004357] [C loss: 0.693899]\n",
      "6665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004431] [C loss: 0.693899]\n",
      "6666 [D loss: 0.693900, acc.: 0.00%] [G loss: 0.010118] [C loss: 0.693896]\n",
      "6667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004689] [C loss: 0.693896]\n",
      "6668 [D loss: 0.693897, acc.: 0.00%] [G loss: 0.005132] [C loss: 0.693896]\n",
      "6669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007024] [C loss: 0.693896]\n",
      "6670 [D loss: 0.693898, acc.: 0.00%] [G loss: 0.007550] [C loss: 0.693894]\n",
      "6671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006427] [C loss: 0.693894]\n",
      "6672 [D loss: 0.693896, acc.: 0.00%] [G loss: 0.005098] [C loss: 0.693892]\n",
      "6673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005299] [C loss: 0.693892]\n",
      "6674 [D loss: 0.693894, acc.: 0.00%] [G loss: 0.006746] [C loss: 0.693892]\n",
      "6675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007262] [C loss: 0.693892]\n",
      "6676 [D loss: 0.693894, acc.: 0.00%] [G loss: 0.004907] [C loss: 0.693890]\n",
      "6677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006247] [C loss: 0.693890]\n",
      "6678 [D loss: 0.693891, acc.: 0.00%] [G loss: 0.006018] [C loss: 0.693888]\n",
      "6679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018263] [C loss: 0.693888]\n",
      "6680 [D loss: 0.693890, acc.: 0.00%] [G loss: 0.003613] [C loss: 0.693888]\n",
      "6681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004614] [C loss: 0.693888]\n",
      "6682 [D loss: 0.693890, acc.: 0.00%] [G loss: 0.006796] [C loss: 0.693887]\n",
      "6683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005018] [C loss: 0.693887]\n",
      "6684 [D loss: 0.693889, acc.: 0.00%] [G loss: 0.005618] [C loss: 0.693885]\n",
      "6685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005397] [C loss: 0.693885]\n",
      "6686 [D loss: 0.693886, acc.: 0.00%] [G loss: 0.006210] [C loss: 0.693884]\n",
      "6687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006153] [C loss: 0.693884]\n",
      "6688 [D loss: 0.693886, acc.: 0.00%] [G loss: 0.005434] [C loss: 0.693883]\n",
      "6689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004509] [C loss: 0.693883]\n",
      "6690 [D loss: 0.693885, acc.: 0.00%] [G loss: 0.005618] [C loss: 0.693882]\n",
      "6691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005159] [C loss: 0.693882]\n",
      "6692 [D loss: 0.693883, acc.: 0.00%] [G loss: 0.004984] [C loss: 0.693882]\n",
      "6693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006618] [C loss: 0.693882]\n",
      "6694 [D loss: 0.693883, acc.: 0.00%] [G loss: 0.005316] [C loss: 0.693880]\n",
      "6695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006832] [C loss: 0.693880]\n",
      "6696 [D loss: 0.693882, acc.: 0.00%] [G loss: 0.006951] [C loss: 0.693877]\n",
      "6697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007907] [C loss: 0.693877]\n",
      "6698 [D loss: 0.693880, acc.: 0.00%] [G loss: 0.004960] [C loss: 0.693878]\n",
      "6699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006924] [C loss: 0.693878]\n",
      "6700 [D loss: 0.693879, acc.: 0.00%] [G loss: 0.005365] [C loss: 0.693876]\n",
      "6701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004790] [C loss: 0.693876]\n",
      "6702 [D loss: 0.693878, acc.: 0.00%] [G loss: 0.005680] [C loss: 0.693875]\n",
      "6703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005363] [C loss: 0.693875]\n",
      "6704 [D loss: 0.693877, acc.: 0.00%] [G loss: 0.004458] [C loss: 0.693874]\n",
      "6705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005506] [C loss: 0.693874]\n",
      "6706 [D loss: 0.693875, acc.: 0.00%] [G loss: 0.006425] [C loss: 0.693873]\n",
      "6707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004635] [C loss: 0.693873]\n",
      "6708 [D loss: 0.693874, acc.: 0.00%] [G loss: 0.005504] [C loss: 0.693872]\n",
      "6709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004808] [C loss: 0.693872]\n",
      "6710 [D loss: 0.693874, acc.: 0.00%] [G loss: 0.007143] [C loss: 0.693869]\n",
      "6711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004213] [C loss: 0.693869]\n",
      "6712 [D loss: 0.693872, acc.: 0.00%] [G loss: 0.005727] [C loss: 0.693869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005434] [C loss: 0.693869]\n",
      "6714 [D loss: 0.693871, acc.: 0.00%] [G loss: 0.004575] [C loss: 0.693868]\n",
      "6715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004795] [C loss: 0.693868]\n",
      "6716 [D loss: 0.693870, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.693865]\n",
      "6717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008104] [C loss: 0.693865]\n",
      "6718 [D loss: 0.693868, acc.: 0.00%] [G loss: 0.004599] [C loss: 0.693866]\n",
      "6719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005227] [C loss: 0.693866]\n",
      "6720 [D loss: 0.693868, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.693864]\n",
      "6721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004581] [C loss: 0.693864]\n",
      "6722 [D loss: 0.693866, acc.: 0.00%] [G loss: 0.007179] [C loss: 0.693864]\n",
      "6723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.693864]\n",
      "6724 [D loss: 0.693866, acc.: 0.00%] [G loss: 0.006953] [C loss: 0.693862]\n",
      "6725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005402] [C loss: 0.693862]\n",
      "6726 [D loss: 0.693864, acc.: 0.00%] [G loss: 0.007102] [C loss: 0.693861]\n",
      "6727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006434] [C loss: 0.693861]\n",
      "6728 [D loss: 0.693863, acc.: 0.00%] [G loss: 0.004260] [C loss: 0.693861]\n",
      "6729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008038] [C loss: 0.693861]\n",
      "6730 [D loss: 0.693862, acc.: 0.00%] [G loss: 0.008607] [C loss: 0.693857]\n",
      "6731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005383] [C loss: 0.693857]\n",
      "6732 [D loss: 0.693860, acc.: 0.00%] [G loss: 0.004150] [C loss: 0.693857]\n",
      "6733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015922] [C loss: 0.693857]\n",
      "6734 [D loss: 0.693859, acc.: 0.00%] [G loss: 0.008794] [C loss: 0.693856]\n",
      "6735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005012] [C loss: 0.693856]\n",
      "6736 [D loss: 0.693858, acc.: 0.00%] [G loss: 0.004958] [C loss: 0.693856]\n",
      "6737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006677] [C loss: 0.693856]\n",
      "6738 [D loss: 0.693858, acc.: 0.00%] [G loss: 0.004040] [C loss: 0.693854]\n",
      "6739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005210] [C loss: 0.693854]\n",
      "6740 [D loss: 0.693856, acc.: 0.00%] [G loss: 0.005291] [C loss: 0.693853]\n",
      "6741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004730] [C loss: 0.693853]\n",
      "6742 [D loss: 0.693854, acc.: 0.00%] [G loss: 0.008027] [C loss: 0.693851]\n",
      "6743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006961] [C loss: 0.693851]\n",
      "6744 [D loss: 0.693854, acc.: 0.00%] [G loss: 0.011202] [C loss: 0.693851]\n",
      "6745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003929] [C loss: 0.693851]\n",
      "6746 [D loss: 0.693852, acc.: 0.00%] [G loss: 0.005558] [C loss: 0.693849]\n",
      "6747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004436] [C loss: 0.693849]\n",
      "6748 [D loss: 0.693851, acc.: 0.00%] [G loss: 0.005848] [C loss: 0.693850]\n",
      "6749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004933] [C loss: 0.693850]\n",
      "6750 [D loss: 0.693851, acc.: 0.00%] [G loss: 0.006805] [C loss: 0.693848]\n",
      "6751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.693848]\n",
      "6752 [D loss: 0.693849, acc.: 0.00%] [G loss: 0.005778] [C loss: 0.693845]\n",
      "6753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005236] [C loss: 0.693845]\n",
      "6754 [D loss: 0.693847, acc.: 0.00%] [G loss: 0.003939] [C loss: 0.693844]\n",
      "6755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004523] [C loss: 0.693844]\n",
      "6756 [D loss: 0.693846, acc.: 0.00%] [G loss: 0.005675] [C loss: 0.693843]\n",
      "6757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004001] [C loss: 0.693843]\n",
      "6758 [D loss: 0.693845, acc.: 0.00%] [G loss: 0.006552] [C loss: 0.693842]\n",
      "6759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004294] [C loss: 0.693842]\n",
      "6760 [D loss: 0.693845, acc.: 0.00%] [G loss: 0.004588] [C loss: 0.693840]\n",
      "6761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006751] [C loss: 0.693840]\n",
      "6762 [D loss: 0.693843, acc.: 0.00%] [G loss: 0.003853] [C loss: 0.693840]\n",
      "6763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005744] [C loss: 0.693840]\n",
      "6764 [D loss: 0.693842, acc.: 0.00%] [G loss: 0.010624] [C loss: 0.693840]\n",
      "6765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004387] [C loss: 0.693840]\n",
      "6766 [D loss: 0.693842, acc.: 0.00%] [G loss: 0.006976] [C loss: 0.693837]\n",
      "6767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004973] [C loss: 0.693837]\n",
      "6768 [D loss: 0.693839, acc.: 0.00%] [G loss: 0.005570] [C loss: 0.693836]\n",
      "6769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014452] [C loss: 0.693836]\n",
      "6770 [D loss: 0.693839, acc.: 0.00%] [G loss: 0.004889] [C loss: 0.693837]\n",
      "6771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005778] [C loss: 0.693837]\n",
      "6772 [D loss: 0.693839, acc.: 0.00%] [G loss: 0.004306] [C loss: 0.693834]\n",
      "6773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005346] [C loss: 0.693834]\n",
      "6774 [D loss: 0.693836, acc.: 0.00%] [G loss: 0.004660] [C loss: 0.693835]\n",
      "6775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004368] [C loss: 0.693835]\n",
      "6776 [D loss: 0.693836, acc.: 0.00%] [G loss: 0.005438] [C loss: 0.693833]\n",
      "6777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003477] [C loss: 0.693833]\n",
      "6778 [D loss: 0.693835, acc.: 0.00%] [G loss: 0.005916] [C loss: 0.693831]\n",
      "6779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003740] [C loss: 0.693831]\n",
      "6780 [D loss: 0.693833, acc.: 0.00%] [G loss: 0.003991] [C loss: 0.693831]\n",
      "6781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005247] [C loss: 0.693831]\n",
      "6782 [D loss: 0.693833, acc.: 0.00%] [G loss: 0.006043] [C loss: 0.693830]\n",
      "6783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004056] [C loss: 0.693830]\n",
      "6784 [D loss: 0.693831, acc.: 0.00%] [G loss: 0.004281] [C loss: 0.693828]\n",
      "6785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003317] [C loss: 0.693828]\n",
      "6786 [D loss: 0.693830, acc.: 0.00%] [G loss: 0.004772] [C loss: 0.693826]\n",
      "6787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003999] [C loss: 0.693826]\n",
      "6788 [D loss: 0.693828, acc.: 0.00%] [G loss: 0.020046] [C loss: 0.693826]\n",
      "6789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006268] [C loss: 0.693826]\n",
      "6790 [D loss: 0.693827, acc.: 0.00%] [G loss: 0.004529] [C loss: 0.693825]\n",
      "6791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006946] [C loss: 0.693825]\n",
      "6792 [D loss: 0.693827, acc.: 0.00%] [G loss: 0.005242] [C loss: 0.693825]\n",
      "6793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009379] [C loss: 0.693825]\n",
      "6794 [D loss: 0.693826, acc.: 0.00%] [G loss: 0.005370] [C loss: 0.693824]\n",
      "6795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005748] [C loss: 0.693824]\n",
      "6796 [D loss: 0.693825, acc.: 0.00%] [G loss: 0.005041] [C loss: 0.693822]\n",
      "6797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004827] [C loss: 0.693822]\n",
      "6798 [D loss: 0.693824, acc.: 0.00%] [G loss: 0.005868] [C loss: 0.693822]\n",
      "6799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004859] [C loss: 0.693822]\n",
      "6800 [D loss: 0.693823, acc.: 0.00%] [G loss: 0.003893] [C loss: 0.693820]\n",
      "6801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005658] [C loss: 0.693820]\n",
      "6802 [D loss: 0.693821, acc.: 0.00%] [G loss: 0.005120] [C loss: 0.693819]\n",
      "6803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007284] [C loss: 0.693819]\n",
      "6804 [D loss: 0.693820, acc.: 0.00%] [G loss: 0.004404] [C loss: 0.693817]\n",
      "6805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004955] [C loss: 0.693817]\n",
      "6806 [D loss: 0.693819, acc.: 0.00%] [G loss: 0.004820] [C loss: 0.693816]\n",
      "6807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005520] [C loss: 0.693816]\n",
      "6808 [D loss: 0.693818, acc.: 0.00%] [G loss: 0.004636] [C loss: 0.693815]\n",
      "6809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004619] [C loss: 0.693815]\n",
      "6810 [D loss: 0.693816, acc.: 0.00%] [G loss: 0.010185] [C loss: 0.693814]\n",
      "6811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003659] [C loss: 0.693814]\n",
      "6812 [D loss: 0.693816, acc.: 0.00%] [G loss: 0.005788] [C loss: 0.693813]\n",
      "6813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005659] [C loss: 0.693813]\n",
      "6814 [D loss: 0.693815, acc.: 0.00%] [G loss: 0.004764] [C loss: 0.693811]\n",
      "6815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004154] [C loss: 0.693811]\n",
      "6816 [D loss: 0.693814, acc.: 0.00%] [G loss: 0.006506] [C loss: 0.693810]\n",
      "6817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006103] [C loss: 0.693810]\n",
      "6818 [D loss: 0.693812, acc.: 0.00%] [G loss: 0.006159] [C loss: 0.693808]\n",
      "6819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005154] [C loss: 0.693808]\n",
      "6820 [D loss: 0.693811, acc.: 0.00%] [G loss: 0.005858] [C loss: 0.693808]\n",
      "6821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005031] [C loss: 0.693808]\n",
      "6822 [D loss: 0.693810, acc.: 0.00%] [G loss: 0.003807] [C loss: 0.693808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.693808]\n",
      "6824 [D loss: 0.693809, acc.: 0.00%] [G loss: 0.004718] [C loss: 0.693807]\n",
      "6825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005255] [C loss: 0.693807]\n",
      "6826 [D loss: 0.693808, acc.: 0.00%] [G loss: 0.004051] [C loss: 0.693805]\n",
      "6827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003835] [C loss: 0.693805]\n",
      "6828 [D loss: 0.693806, acc.: 0.00%] [G loss: 0.005279] [C loss: 0.693803]\n",
      "6829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004549] [C loss: 0.693803]\n",
      "6830 [D loss: 0.693805, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.693803]\n",
      "6831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004837] [C loss: 0.693803]\n",
      "6832 [D loss: 0.693804, acc.: 0.00%] [G loss: 0.011341] [C loss: 0.693801]\n",
      "6833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003518] [C loss: 0.693801]\n",
      "6834 [D loss: 0.693802, acc.: 0.00%] [G loss: 0.006029] [C loss: 0.693800]\n",
      "6835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005228] [C loss: 0.693800]\n",
      "6836 [D loss: 0.693802, acc.: 0.00%] [G loss: 0.005203] [C loss: 0.693799]\n",
      "6837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004522] [C loss: 0.693799]\n",
      "6838 [D loss: 0.693801, acc.: 0.00%] [G loss: 0.005118] [C loss: 0.693798]\n",
      "6839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004191] [C loss: 0.693798]\n",
      "6840 [D loss: 0.693800, acc.: 0.00%] [G loss: 0.015915] [C loss: 0.693797]\n",
      "6841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004715] [C loss: 0.693797]\n",
      "6842 [D loss: 0.693798, acc.: 0.00%] [G loss: 0.005150] [C loss: 0.693797]\n",
      "6843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005112] [C loss: 0.693797]\n",
      "6844 [D loss: 0.693798, acc.: 0.00%] [G loss: 0.003959] [C loss: 0.693793]\n",
      "6845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006871] [C loss: 0.693793]\n",
      "6846 [D loss: 0.693796, acc.: 0.00%] [G loss: 0.004491] [C loss: 0.693795]\n",
      "6847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003971] [C loss: 0.693795]\n",
      "6848 [D loss: 0.693797, acc.: 0.00%] [G loss: 0.003423] [C loss: 0.693794]\n",
      "6849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006658] [C loss: 0.693794]\n",
      "6850 [D loss: 0.693795, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.693792]\n",
      "6851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003531] [C loss: 0.693792]\n",
      "6852 [D loss: 0.693793, acc.: 0.00%] [G loss: 0.010725] [C loss: 0.693790]\n",
      "6853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.693790]\n",
      "6854 [D loss: 0.693792, acc.: 0.00%] [G loss: 0.006732] [C loss: 0.693791]\n",
      "6855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004927] [C loss: 0.693791]\n",
      "6856 [D loss: 0.693792, acc.: 0.00%] [G loss: 0.006210] [C loss: 0.693789]\n",
      "6857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005141] [C loss: 0.693789]\n",
      "6858 [D loss: 0.693791, acc.: 0.00%] [G loss: 0.005253] [C loss: 0.693788]\n",
      "6859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.693788]\n",
      "6860 [D loss: 0.693789, acc.: 0.00%] [G loss: 0.006250] [C loss: 0.693786]\n",
      "6861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005380] [C loss: 0.693786]\n",
      "6862 [D loss: 0.693788, acc.: 0.00%] [G loss: 0.006721] [C loss: 0.693785]\n",
      "6863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006355] [C loss: 0.693785]\n",
      "6864 [D loss: 0.693787, acc.: 0.00%] [G loss: 0.006333] [C loss: 0.693784]\n",
      "6865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004960] [C loss: 0.693784]\n",
      "6866 [D loss: 0.693785, acc.: 0.00%] [G loss: 0.005963] [C loss: 0.693783]\n",
      "6867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004123] [C loss: 0.693783]\n",
      "6868 [D loss: 0.693784, acc.: 0.00%] [G loss: 0.005854] [C loss: 0.693781]\n",
      "6869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005244] [C loss: 0.693781]\n",
      "6870 [D loss: 0.693783, acc.: 0.00%] [G loss: 0.007921] [C loss: 0.693781]\n",
      "6871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005747] [C loss: 0.693781]\n",
      "6872 [D loss: 0.693783, acc.: 0.00%] [G loss: 0.004492] [C loss: 0.693780]\n",
      "6873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004688] [C loss: 0.693780]\n",
      "6874 [D loss: 0.693782, acc.: 0.00%] [G loss: 0.004379] [C loss: 0.693779]\n",
      "6875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004844] [C loss: 0.693779]\n",
      "6876 [D loss: 0.693781, acc.: 0.00%] [G loss: 0.008087] [C loss: 0.693778]\n",
      "6877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004200] [C loss: 0.693778]\n",
      "6878 [D loss: 0.693779, acc.: 0.00%] [G loss: 0.004437] [C loss: 0.693777]\n",
      "6879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005176] [C loss: 0.693777]\n",
      "6880 [D loss: 0.693778, acc.: 0.00%] [G loss: 0.007892] [C loss: 0.693776]\n",
      "6881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.693776]\n",
      "6882 [D loss: 0.693777, acc.: 0.00%] [G loss: 0.004682] [C loss: 0.693775]\n",
      "6883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006157] [C loss: 0.693775]\n",
      "6884 [D loss: 0.693776, acc.: 0.00%] [G loss: 0.004700] [C loss: 0.693772]\n",
      "6885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005297] [C loss: 0.693772]\n",
      "6886 [D loss: 0.693774, acc.: 0.00%] [G loss: 0.004964] [C loss: 0.693772]\n",
      "6887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.026539] [C loss: 0.693772]\n",
      "6888 [D loss: 0.693774, acc.: 0.00%] [G loss: 0.005578] [C loss: 0.693772]\n",
      "6889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008621] [C loss: 0.693772]\n",
      "6890 [D loss: 0.693773, acc.: 0.00%] [G loss: 0.006998] [C loss: 0.693770]\n",
      "6891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005480] [C loss: 0.693770]\n",
      "6892 [D loss: 0.693772, acc.: 0.00%] [G loss: 0.005659] [C loss: 0.693769]\n",
      "6893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005223] [C loss: 0.693769]\n",
      "6894 [D loss: 0.693772, acc.: 0.00%] [G loss: 0.005835] [C loss: 0.693768]\n",
      "6895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005779] [C loss: 0.693768]\n",
      "6896 [D loss: 0.693770, acc.: 0.00%] [G loss: 0.005723] [C loss: 0.693767]\n",
      "6897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005487] [C loss: 0.693767]\n",
      "6898 [D loss: 0.693769, acc.: 0.00%] [G loss: 0.005288] [C loss: 0.693766]\n",
      "6899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007380] [C loss: 0.693766]\n",
      "6900 [D loss: 0.693768, acc.: 0.00%] [G loss: 0.009320] [C loss: 0.693765]\n",
      "6901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005522] [C loss: 0.693765]\n",
      "6902 [D loss: 0.693766, acc.: 0.00%] [G loss: 0.007847] [C loss: 0.693764]\n",
      "6903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004920] [C loss: 0.693764]\n",
      "6904 [D loss: 0.693766, acc.: 0.00%] [G loss: 0.004840] [C loss: 0.693762]\n",
      "6905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005433] [C loss: 0.693762]\n",
      "6906 [D loss: 0.693764, acc.: 0.00%] [G loss: 0.008382] [C loss: 0.693762]\n",
      "6907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007010] [C loss: 0.693762]\n",
      "6908 [D loss: 0.693763, acc.: 0.00%] [G loss: 0.006319] [C loss: 0.693761]\n",
      "6909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005470] [C loss: 0.693761]\n",
      "6910 [D loss: 0.693763, acc.: 0.00%] [G loss: 0.004749] [C loss: 0.693759]\n",
      "6911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004955] [C loss: 0.693759]\n",
      "6912 [D loss: 0.693761, acc.: 0.00%] [G loss: 0.006626] [C loss: 0.693760]\n",
      "6913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004635] [C loss: 0.693760]\n",
      "6914 [D loss: 0.693761, acc.: 0.00%] [G loss: 0.004198] [C loss: 0.693758]\n",
      "6915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003898] [C loss: 0.693758]\n",
      "6916 [D loss: 0.693759, acc.: 0.00%] [G loss: 0.006266] [C loss: 0.693756]\n",
      "6917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005439] [C loss: 0.693756]\n",
      "6918 [D loss: 0.693758, acc.: 0.00%] [G loss: 0.004062] [C loss: 0.693757]\n",
      "6919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004908] [C loss: 0.693757]\n",
      "6920 [D loss: 0.693757, acc.: 0.00%] [G loss: 0.008824] [C loss: 0.693754]\n",
      "6921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693754]\n",
      "6922 [D loss: 0.693756, acc.: 0.00%] [G loss: 0.006820] [C loss: 0.693752]\n",
      "6923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005165] [C loss: 0.693752]\n",
      "6924 [D loss: 0.693755, acc.: 0.00%] [G loss: 0.004173] [C loss: 0.693752]\n",
      "6925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004506] [C loss: 0.693752]\n",
      "6926 [D loss: 0.693754, acc.: 0.00%] [G loss: 0.005346] [C loss: 0.693751]\n",
      "6927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009531] [C loss: 0.693751]\n",
      "6928 [D loss: 0.693752, acc.: 0.00%] [G loss: 0.006257] [C loss: 0.693750]\n",
      "6929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004057] [C loss: 0.693750]\n",
      "6930 [D loss: 0.693752, acc.: 0.00%] [G loss: 0.004028] [C loss: 0.693749]\n",
      "6931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004439] [C loss: 0.693749]\n",
      "6932 [D loss: 0.693750, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.693748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005918] [C loss: 0.693748]\n",
      "6934 [D loss: 0.693750, acc.: 0.00%] [G loss: 0.006959] [C loss: 0.693747]\n",
      "6935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005837] [C loss: 0.693747]\n",
      "6936 [D loss: 0.693749, acc.: 0.00%] [G loss: 0.013166] [C loss: 0.693747]\n",
      "6937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.693747]\n",
      "6938 [D loss: 0.693748, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.693745]\n",
      "6939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004337] [C loss: 0.693745]\n",
      "6940 [D loss: 0.693747, acc.: 0.00%] [G loss: 0.006565] [C loss: 0.693744]\n",
      "6941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003750] [C loss: 0.693744]\n",
      "6942 [D loss: 0.693746, acc.: 0.00%] [G loss: 0.005059] [C loss: 0.693743]\n",
      "6943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005989] [C loss: 0.693743]\n",
      "6944 [D loss: 0.693745, acc.: 0.00%] [G loss: 0.003309] [C loss: 0.693742]\n",
      "6945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003652] [C loss: 0.693742]\n",
      "6946 [D loss: 0.693744, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.693741]\n",
      "6947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004937] [C loss: 0.693741]\n",
      "6948 [D loss: 0.693743, acc.: 0.00%] [G loss: 0.006931] [C loss: 0.693740]\n",
      "6949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006593] [C loss: 0.693740]\n",
      "6950 [D loss: 0.693742, acc.: 0.00%] [G loss: 0.005044] [C loss: 0.693739]\n",
      "6951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006124] [C loss: 0.693739]\n",
      "6952 [D loss: 0.693741, acc.: 0.00%] [G loss: 0.005601] [C loss: 0.693737]\n",
      "6953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005998] [C loss: 0.693737]\n",
      "6954 [D loss: 0.693739, acc.: 0.00%] [G loss: 0.004499] [C loss: 0.693738]\n",
      "6955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005381] [C loss: 0.693738]\n",
      "6956 [D loss: 0.693739, acc.: 0.00%] [G loss: 0.005613] [C loss: 0.693735]\n",
      "6957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.693735]\n",
      "6958 [D loss: 0.693737, acc.: 0.00%] [G loss: 0.005600] [C loss: 0.693734]\n",
      "6959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014238] [C loss: 0.693734]\n",
      "6960 [D loss: 0.693736, acc.: 0.00%] [G loss: 0.004706] [C loss: 0.693733]\n",
      "6961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005484] [C loss: 0.693733]\n",
      "6962 [D loss: 0.693735, acc.: 0.00%] [G loss: 0.006911] [C loss: 0.693732]\n",
      "6963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006581] [C loss: 0.693732]\n",
      "6964 [D loss: 0.693734, acc.: 0.00%] [G loss: 0.004444] [C loss: 0.693732]\n",
      "6965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011656] [C loss: 0.693732]\n",
      "6966 [D loss: 0.693733, acc.: 0.00%] [G loss: 0.005370] [C loss: 0.693730]\n",
      "6967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.693730]\n",
      "6968 [D loss: 0.693732, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.693729]\n",
      "6969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005573] [C loss: 0.693729]\n",
      "6970 [D loss: 0.693731, acc.: 0.00%] [G loss: 0.004363] [C loss: 0.693727]\n",
      "6971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006329] [C loss: 0.693727]\n",
      "6972 [D loss: 0.693730, acc.: 0.00%] [G loss: 0.003898] [C loss: 0.693727]\n",
      "6973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005554] [C loss: 0.693727]\n",
      "6974 [D loss: 0.693729, acc.: 0.00%] [G loss: 0.009787] [C loss: 0.693727]\n",
      "6975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004423] [C loss: 0.693727]\n",
      "6976 [D loss: 0.693727, acc.: 0.00%] [G loss: 0.005092] [C loss: 0.693726]\n",
      "6977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004705] [C loss: 0.693726]\n",
      "6978 [D loss: 0.693727, acc.: 0.00%] [G loss: 0.005334] [C loss: 0.693725]\n",
      "6979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005858] [C loss: 0.693725]\n",
      "6980 [D loss: 0.693726, acc.: 0.00%] [G loss: 0.012025] [C loss: 0.693724]\n",
      "6981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004581] [C loss: 0.693724]\n",
      "6982 [D loss: 0.693725, acc.: 0.00%] [G loss: 0.004367] [C loss: 0.693722]\n",
      "6983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006955] [C loss: 0.693722]\n",
      "6984 [D loss: 0.693724, acc.: 0.00%] [G loss: 0.005072] [C loss: 0.693720]\n",
      "6985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006527] [C loss: 0.693720]\n",
      "6986 [D loss: 0.693723, acc.: 0.00%] [G loss: 0.004537] [C loss: 0.693720]\n",
      "6987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020209] [C loss: 0.693720]\n",
      "6988 [D loss: 0.693722, acc.: 0.00%] [G loss: 0.003641] [C loss: 0.693719]\n",
      "6989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006190] [C loss: 0.693719]\n",
      "6990 [D loss: 0.693721, acc.: 0.00%] [G loss: 0.005791] [C loss: 0.693719]\n",
      "6991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004221] [C loss: 0.693719]\n",
      "6992 [D loss: 0.693720, acc.: 0.00%] [G loss: 0.004475] [C loss: 0.693718]\n",
      "6993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003939] [C loss: 0.693718]\n",
      "6994 [D loss: 0.693719, acc.: 0.00%] [G loss: 0.005674] [C loss: 0.693717]\n",
      "6995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004385] [C loss: 0.693717]\n",
      "6996 [D loss: 0.693719, acc.: 0.00%] [G loss: 0.004291] [C loss: 0.693716]\n",
      "6997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004178] [C loss: 0.693716]\n",
      "6998 [D loss: 0.693717, acc.: 0.00%] [G loss: 0.004438] [C loss: 0.693715]\n",
      "6999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006530] [C loss: 0.693715]\n",
      "7000 [D loss: 0.693716, acc.: 0.00%] [G loss: 0.004431] [C loss: 0.693713]\n",
      "7001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004090] [C loss: 0.693713]\n",
      "7002 [D loss: 0.693715, acc.: 0.00%] [G loss: 0.004948] [C loss: 0.693713]\n",
      "7003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006397] [C loss: 0.693713]\n",
      "7004 [D loss: 0.693714, acc.: 0.00%] [G loss: 0.005172] [C loss: 0.693712]\n",
      "7005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004359] [C loss: 0.693712]\n",
      "7006 [D loss: 0.693713, acc.: 0.00%] [G loss: 0.019673] [C loss: 0.693711]\n",
      "7007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005449] [C loss: 0.693711]\n",
      "7008 [D loss: 0.693712, acc.: 0.00%] [G loss: 0.007625] [C loss: 0.693710]\n",
      "7009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005942] [C loss: 0.693710]\n",
      "7010 [D loss: 0.693711, acc.: 0.00%] [G loss: 0.006133] [C loss: 0.693710]\n",
      "7011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005401] [C loss: 0.693710]\n",
      "7012 [D loss: 0.693710, acc.: 0.00%] [G loss: 0.005372] [C loss: 0.693708]\n",
      "7013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004842] [C loss: 0.693708]\n",
      "7014 [D loss: 0.693709, acc.: 0.00%] [G loss: 0.011201] [C loss: 0.693708]\n",
      "7015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006239] [C loss: 0.693708]\n",
      "7016 [D loss: 0.693709, acc.: 0.00%] [G loss: 0.006421] [C loss: 0.693705]\n",
      "7017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020380] [C loss: 0.693705]\n",
      "7018 [D loss: 0.693707, acc.: 0.00%] [G loss: 0.004680] [C loss: 0.693704]\n",
      "7019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006483] [C loss: 0.693704]\n",
      "7020 [D loss: 0.693706, acc.: 0.00%] [G loss: 0.006537] [C loss: 0.693704]\n",
      "7021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008003] [C loss: 0.693704]\n",
      "7022 [D loss: 0.693706, acc.: 0.00%] [G loss: 0.004035] [C loss: 0.693703]\n",
      "7023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.693703]\n",
      "7024 [D loss: 0.693704, acc.: 0.00%] [G loss: 0.005490] [C loss: 0.693701]\n",
      "7025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006237] [C loss: 0.693701]\n",
      "7026 [D loss: 0.693703, acc.: 0.00%] [G loss: 0.005361] [C loss: 0.693701]\n",
      "7027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.693701]\n",
      "7028 [D loss: 0.693702, acc.: 0.00%] [G loss: 0.019262] [C loss: 0.693700]\n",
      "7029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006461] [C loss: 0.693700]\n",
      "7030 [D loss: 0.693702, acc.: 0.00%] [G loss: 0.007299] [C loss: 0.693698]\n",
      "7031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007993] [C loss: 0.693698]\n",
      "7032 [D loss: 0.693700, acc.: 0.00%] [G loss: 0.006261] [C loss: 0.693698]\n",
      "7033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005269] [C loss: 0.693698]\n",
      "7034 [D loss: 0.693700, acc.: 0.00%] [G loss: 0.008336] [C loss: 0.693695]\n",
      "7035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.693695]\n",
      "7036 [D loss: 0.693698, acc.: 0.00%] [G loss: 0.006353] [C loss: 0.693695]\n",
      "7037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005155] [C loss: 0.693695]\n",
      "7038 [D loss: 0.693696, acc.: 0.00%] [G loss: 0.007805] [C loss: 0.693694]\n",
      "7039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005998] [C loss: 0.693694]\n",
      "7040 [D loss: 0.693696, acc.: 0.00%] [G loss: 0.005217] [C loss: 0.693693]\n",
      "7041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004880] [C loss: 0.693693]\n",
      "7042 [D loss: 0.693694, acc.: 0.00%] [G loss: 0.004906] [C loss: 0.693692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007073] [C loss: 0.693692]\n",
      "7044 [D loss: 0.693694, acc.: 0.00%] [G loss: 0.003787] [C loss: 0.693692]\n",
      "7045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.693692]\n",
      "7046 [D loss: 0.693693, acc.: 0.00%] [G loss: 0.004144] [C loss: 0.693690]\n",
      "7047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005096] [C loss: 0.693690]\n",
      "7048 [D loss: 0.693692, acc.: 0.00%] [G loss: 0.004678] [C loss: 0.693690]\n",
      "7049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005338] [C loss: 0.693690]\n",
      "7050 [D loss: 0.693691, acc.: 0.00%] [G loss: 0.005578] [C loss: 0.693690]\n",
      "7051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003514] [C loss: 0.693690]\n",
      "7052 [D loss: 0.693691, acc.: 0.00%] [G loss: 0.004622] [C loss: 0.693688]\n",
      "7053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006768] [C loss: 0.693688]\n",
      "7054 [D loss: 0.693689, acc.: 0.00%] [G loss: 0.005389] [C loss: 0.693685]\n",
      "7055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006293] [C loss: 0.693685]\n",
      "7056 [D loss: 0.693687, acc.: 0.00%] [G loss: 0.006474] [C loss: 0.693686]\n",
      "7057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004017] [C loss: 0.693686]\n",
      "7058 [D loss: 0.693688, acc.: 0.00%] [G loss: 0.007507] [C loss: 0.693686]\n",
      "7059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006024] [C loss: 0.693686]\n",
      "7060 [D loss: 0.693687, acc.: 0.00%] [G loss: 0.004954] [C loss: 0.693684]\n",
      "7061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005896] [C loss: 0.693684]\n",
      "7062 [D loss: 0.693685, acc.: 0.00%] [G loss: 0.006404] [C loss: 0.693682]\n",
      "7063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004204] [C loss: 0.693682]\n",
      "7064 [D loss: 0.693684, acc.: 0.00%] [G loss: 0.005890] [C loss: 0.693682]\n",
      "7065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003749] [C loss: 0.693682]\n",
      "7066 [D loss: 0.693683, acc.: 0.00%] [G loss: 0.005521] [C loss: 0.693681]\n",
      "7067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005162] [C loss: 0.693681]\n",
      "7068 [D loss: 0.693683, acc.: 0.00%] [G loss: 0.005656] [C loss: 0.693680]\n",
      "7069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005646] [C loss: 0.693680]\n",
      "7070 [D loss: 0.693681, acc.: 0.00%] [G loss: 0.006770] [C loss: 0.693679]\n",
      "7071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005344] [C loss: 0.693679]\n",
      "7072 [D loss: 0.693681, acc.: 0.00%] [G loss: 0.008324] [C loss: 0.693678]\n",
      "7073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014411] [C loss: 0.693678]\n",
      "7074 [D loss: 0.693680, acc.: 0.00%] [G loss: 0.003969] [C loss: 0.693678]\n",
      "7075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007483] [C loss: 0.693678]\n",
      "7076 [D loss: 0.693679, acc.: 0.00%] [G loss: 0.008309] [C loss: 0.693677]\n",
      "7077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005844] [C loss: 0.693677]\n",
      "7078 [D loss: 0.693678, acc.: 0.00%] [G loss: 0.005940] [C loss: 0.693675]\n",
      "7079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006026] [C loss: 0.693675]\n",
      "7080 [D loss: 0.693677, acc.: 0.00%] [G loss: 0.005261] [C loss: 0.693674]\n",
      "7081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005116] [C loss: 0.693674]\n",
      "7082 [D loss: 0.693676, acc.: 0.00%] [G loss: 0.007566] [C loss: 0.693673]\n",
      "7083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004388] [C loss: 0.693673]\n",
      "7084 [D loss: 0.693675, acc.: 0.00%] [G loss: 0.007526] [C loss: 0.693673]\n",
      "7085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004862] [C loss: 0.693673]\n",
      "7086 [D loss: 0.693674, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.693671]\n",
      "7087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005327] [C loss: 0.693671]\n",
      "7088 [D loss: 0.693673, acc.: 0.00%] [G loss: 0.006000] [C loss: 0.693671]\n",
      "7089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004157] [C loss: 0.693671]\n",
      "7090 [D loss: 0.693672, acc.: 0.00%] [G loss: 0.019209] [C loss: 0.693669]\n",
      "7091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005522] [C loss: 0.693669]\n",
      "7092 [D loss: 0.693671, acc.: 0.00%] [G loss: 0.005694] [C loss: 0.693668]\n",
      "7093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005148] [C loss: 0.693668]\n",
      "7094 [D loss: 0.693670, acc.: 0.00%] [G loss: 0.004555] [C loss: 0.693668]\n",
      "7095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006081] [C loss: 0.693668]\n",
      "7096 [D loss: 0.693669, acc.: 0.00%] [G loss: 0.004878] [C loss: 0.693667]\n",
      "7097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004742] [C loss: 0.693667]\n",
      "7098 [D loss: 0.693668, acc.: 0.00%] [G loss: 0.003985] [C loss: 0.693665]\n",
      "7099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004798] [C loss: 0.693665]\n",
      "7100 [D loss: 0.693667, acc.: 0.00%] [G loss: 0.004510] [C loss: 0.693665]\n",
      "7101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004768] [C loss: 0.693665]\n",
      "7102 [D loss: 0.693666, acc.: 0.00%] [G loss: 0.005872] [C loss: 0.693664]\n",
      "7103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004032] [C loss: 0.693664]\n",
      "7104 [D loss: 0.693665, acc.: 0.00%] [G loss: 0.005028] [C loss: 0.693663]\n",
      "7105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007022] [C loss: 0.693663]\n",
      "7106 [D loss: 0.693665, acc.: 0.00%] [G loss: 0.005651] [C loss: 0.693661]\n",
      "7107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004331] [C loss: 0.693661]\n",
      "7108 [D loss: 0.693663, acc.: 0.00%] [G loss: 0.005574] [C loss: 0.693662]\n",
      "7109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005583] [C loss: 0.693662]\n",
      "7110 [D loss: 0.693663, acc.: 0.00%] [G loss: 0.003964] [C loss: 0.693661]\n",
      "7111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010059] [C loss: 0.693661]\n",
      "7112 [D loss: 0.693662, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.693658]\n",
      "7113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006961] [C loss: 0.693658]\n",
      "7114 [D loss: 0.693660, acc.: 0.00%] [G loss: 0.006456] [C loss: 0.693658]\n",
      "7115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005593] [C loss: 0.693658]\n",
      "7116 [D loss: 0.693659, acc.: 0.00%] [G loss: 0.006239] [C loss: 0.693657]\n",
      "7117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.693657]\n",
      "7118 [D loss: 0.693658, acc.: 0.00%] [G loss: 0.005285] [C loss: 0.693655]\n",
      "7119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005918] [C loss: 0.693655]\n",
      "7120 [D loss: 0.693657, acc.: 0.00%] [G loss: 0.005474] [C loss: 0.693654]\n",
      "7121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004697] [C loss: 0.693654]\n",
      "7122 [D loss: 0.693657, acc.: 0.00%] [G loss: 0.003741] [C loss: 0.693654]\n",
      "7123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007410] [C loss: 0.693654]\n",
      "7124 [D loss: 0.693655, acc.: 0.00%] [G loss: 0.012738] [C loss: 0.693653]\n",
      "7125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004896] [C loss: 0.693653]\n",
      "7126 [D loss: 0.693655, acc.: 0.00%] [G loss: 0.004912] [C loss: 0.693653]\n",
      "7127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.693653]\n",
      "7128 [D loss: 0.693654, acc.: 0.00%] [G loss: 0.005436] [C loss: 0.693650]\n",
      "7129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007984] [C loss: 0.693650]\n",
      "7130 [D loss: 0.693652, acc.: 0.00%] [G loss: 0.004499] [C loss: 0.693651]\n",
      "7131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005101] [C loss: 0.693651]\n",
      "7132 [D loss: 0.693652, acc.: 0.00%] [G loss: 0.007945] [C loss: 0.693649]\n",
      "7133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005470] [C loss: 0.693649]\n",
      "7134 [D loss: 0.693651, acc.: 0.00%] [G loss: 0.003155] [C loss: 0.693649]\n",
      "7135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004649] [C loss: 0.693649]\n",
      "7136 [D loss: 0.693651, acc.: 0.00%] [G loss: 0.005481] [C loss: 0.693647]\n",
      "7137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006170] [C loss: 0.693647]\n",
      "7138 [D loss: 0.693649, acc.: 0.00%] [G loss: 0.009943] [C loss: 0.693647]\n",
      "7139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004065] [C loss: 0.693647]\n",
      "7140 [D loss: 0.693649, acc.: 0.00%] [G loss: 0.005931] [C loss: 0.693646]\n",
      "7141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004329] [C loss: 0.693646]\n",
      "7142 [D loss: 0.693647, acc.: 0.00%] [G loss: 0.004072] [C loss: 0.693645]\n",
      "7143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004765] [C loss: 0.693645]\n",
      "7144 [D loss: 0.693646, acc.: 0.00%] [G loss: 0.004977] [C loss: 0.693644]\n",
      "7145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005892] [C loss: 0.693644]\n",
      "7146 [D loss: 0.693645, acc.: 0.00%] [G loss: 0.005068] [C loss: 0.693643]\n",
      "7147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005679] [C loss: 0.693643]\n",
      "7148 [D loss: 0.693645, acc.: 0.00%] [G loss: 0.004413] [C loss: 0.693641]\n",
      "7149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006895] [C loss: 0.693641]\n",
      "7150 [D loss: 0.693644, acc.: 0.00%] [G loss: 0.005109] [C loss: 0.693640]\n",
      "7151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006213] [C loss: 0.693640]\n",
      "7152 [D loss: 0.693642, acc.: 0.00%] [G loss: 0.005537] [C loss: 0.693640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009626] [C loss: 0.693640]\n",
      "7154 [D loss: 0.693641, acc.: 0.00%] [G loss: 0.005715] [C loss: 0.693638]\n",
      "7155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003432] [C loss: 0.693638]\n",
      "7156 [D loss: 0.693640, acc.: 0.00%] [G loss: 0.003483] [C loss: 0.693638]\n",
      "7157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004246] [C loss: 0.693638]\n",
      "7158 [D loss: 0.693640, acc.: 0.00%] [G loss: 0.004092] [C loss: 0.693638]\n",
      "7159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003814] [C loss: 0.693638]\n",
      "7160 [D loss: 0.693639, acc.: 0.00%] [G loss: 0.003984] [C loss: 0.693636]\n",
      "7161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.023259] [C loss: 0.693636]\n",
      "7162 [D loss: 0.693638, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.693635]\n",
      "7163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004551] [C loss: 0.693635]\n",
      "7164 [D loss: 0.693637, acc.: 0.00%] [G loss: 0.005864] [C loss: 0.693633]\n",
      "7165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004289] [C loss: 0.693633]\n",
      "7166 [D loss: 0.693635, acc.: 0.00%] [G loss: 0.009982] [C loss: 0.693634]\n",
      "7167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004778] [C loss: 0.693634]\n",
      "7168 [D loss: 0.693636, acc.: 0.00%] [G loss: 0.004282] [C loss: 0.693633]\n",
      "7169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006702] [C loss: 0.693633]\n",
      "7170 [D loss: 0.693634, acc.: 0.00%] [G loss: 0.005983] [C loss: 0.693632]\n",
      "7171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004070] [C loss: 0.693632]\n",
      "7172 [D loss: 0.693633, acc.: 0.00%] [G loss: 0.006185] [C loss: 0.693631]\n",
      "7173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005719] [C loss: 0.693631]\n",
      "7174 [D loss: 0.693633, acc.: 0.00%] [G loss: 0.006370] [C loss: 0.693630]\n",
      "7175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004444] [C loss: 0.693630]\n",
      "7176 [D loss: 0.693632, acc.: 0.00%] [G loss: 0.004934] [C loss: 0.693630]\n",
      "7177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005498] [C loss: 0.693630]\n",
      "7178 [D loss: 0.693631, acc.: 0.00%] [G loss: 0.004055] [C loss: 0.693628]\n",
      "7179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004596] [C loss: 0.693628]\n",
      "7180 [D loss: 0.693631, acc.: 0.00%] [G loss: 0.008773] [C loss: 0.693628]\n",
      "7181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004533] [C loss: 0.693628]\n",
      "7182 [D loss: 0.693629, acc.: 0.00%] [G loss: 0.004063] [C loss: 0.693627]\n",
      "7183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005890] [C loss: 0.693627]\n",
      "7184 [D loss: 0.693628, acc.: 0.00%] [G loss: 0.009290] [C loss: 0.693625]\n",
      "7185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005125] [C loss: 0.693625]\n",
      "7186 [D loss: 0.693627, acc.: 0.00%] [G loss: 0.005317] [C loss: 0.693625]\n",
      "7187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005056] [C loss: 0.693625]\n",
      "7188 [D loss: 0.693626, acc.: 0.00%] [G loss: 0.004675] [C loss: 0.693624]\n",
      "7189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005188] [C loss: 0.693624]\n",
      "7190 [D loss: 0.693625, acc.: 0.00%] [G loss: 0.005549] [C loss: 0.693623]\n",
      "7191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006622] [C loss: 0.693623]\n",
      "7192 [D loss: 0.693625, acc.: 0.00%] [G loss: 0.005448] [C loss: 0.693622]\n",
      "7193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005377] [C loss: 0.693622]\n",
      "7194 [D loss: 0.693623, acc.: 0.00%] [G loss: 0.005384] [C loss: 0.693621]\n",
      "7195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004599] [C loss: 0.693621]\n",
      "7196 [D loss: 0.693623, acc.: 0.00%] [G loss: 0.003791] [C loss: 0.693620]\n",
      "7197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005195] [C loss: 0.693620]\n",
      "7198 [D loss: 0.693622, acc.: 0.00%] [G loss: 0.004011] [C loss: 0.693620]\n",
      "7199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005590] [C loss: 0.693620]\n",
      "7200 [D loss: 0.693620, acc.: 0.00%] [G loss: 0.005246] [C loss: 0.693618]\n",
      "7201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004768] [C loss: 0.693618]\n",
      "7202 [D loss: 0.693619, acc.: 0.00%] [G loss: 0.004050] [C loss: 0.693617]\n",
      "7203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003726] [C loss: 0.693617]\n",
      "7204 [D loss: 0.693619, acc.: 0.00%] [G loss: 0.004748] [C loss: 0.693616]\n",
      "7205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004159] [C loss: 0.693616]\n",
      "7206 [D loss: 0.693618, acc.: 0.00%] [G loss: 0.005400] [C loss: 0.693617]\n",
      "7207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004161] [C loss: 0.693617]\n",
      "7208 [D loss: 0.693617, acc.: 0.00%] [G loss: 0.003311] [C loss: 0.693615]\n",
      "7209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008627] [C loss: 0.693615]\n",
      "7210 [D loss: 0.693617, acc.: 0.00%] [G loss: 0.005124] [C loss: 0.693615]\n",
      "7211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003974] [C loss: 0.693615]\n",
      "7212 [D loss: 0.693616, acc.: 0.00%] [G loss: 0.003721] [C loss: 0.693614]\n",
      "7213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005322] [C loss: 0.693614]\n",
      "7214 [D loss: 0.693615, acc.: 0.00%] [G loss: 0.006186] [C loss: 0.693612]\n",
      "7215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006965] [C loss: 0.693612]\n",
      "7216 [D loss: 0.693614, acc.: 0.00%] [G loss: 0.006358] [C loss: 0.693610]\n",
      "7217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004223] [C loss: 0.693610]\n",
      "7218 [D loss: 0.693612, acc.: 0.00%] [G loss: 0.005685] [C loss: 0.693609]\n",
      "7219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005883] [C loss: 0.693609]\n",
      "7220 [D loss: 0.693611, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.693610]\n",
      "7221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004065] [C loss: 0.693610]\n",
      "7222 [D loss: 0.693611, acc.: 0.00%] [G loss: 0.004807] [C loss: 0.693609]\n",
      "7223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004214] [C loss: 0.693609]\n",
      "7224 [D loss: 0.693610, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.693608]\n",
      "7225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005064] [C loss: 0.693608]\n",
      "7226 [D loss: 0.693609, acc.: 0.00%] [G loss: 0.004833] [C loss: 0.693608]\n",
      "7227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004766] [C loss: 0.693608]\n",
      "7228 [D loss: 0.693609, acc.: 0.00%] [G loss: 0.003347] [C loss: 0.693606]\n",
      "7229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004371] [C loss: 0.693606]\n",
      "7230 [D loss: 0.693607, acc.: 0.00%] [G loss: 0.005119] [C loss: 0.693606]\n",
      "7231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005454] [C loss: 0.693606]\n",
      "7232 [D loss: 0.693607, acc.: 0.00%] [G loss: 0.005946] [C loss: 0.693603]\n",
      "7233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006188] [C loss: 0.693603]\n",
      "7234 [D loss: 0.693605, acc.: 0.00%] [G loss: 0.006093] [C loss: 0.693602]\n",
      "7235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004790] [C loss: 0.693602]\n",
      "7236 [D loss: 0.693604, acc.: 0.00%] [G loss: 0.005732] [C loss: 0.693601]\n",
      "7237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003225] [C loss: 0.693601]\n",
      "7238 [D loss: 0.693603, acc.: 0.00%] [G loss: 0.005054] [C loss: 0.693601]\n",
      "7239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006307] [C loss: 0.693601]\n",
      "7240 [D loss: 0.693603, acc.: 0.00%] [G loss: 0.015964] [C loss: 0.693600]\n",
      "7241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006433] [C loss: 0.693600]\n",
      "7242 [D loss: 0.693603, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.693600]\n",
      "7243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005944] [C loss: 0.693600]\n",
      "7244 [D loss: 0.693601, acc.: 0.00%] [G loss: 0.006117] [C loss: 0.693600]\n",
      "7245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004186] [C loss: 0.693600]\n",
      "7246 [D loss: 0.693600, acc.: 0.00%] [G loss: 0.005463] [C loss: 0.693599]\n",
      "7247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007023] [C loss: 0.693599]\n",
      "7248 [D loss: 0.693600, acc.: 0.00%] [G loss: 0.004630] [C loss: 0.693598]\n",
      "7249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004937] [C loss: 0.693598]\n",
      "7250 [D loss: 0.693599, acc.: 0.00%] [G loss: 0.005392] [C loss: 0.693596]\n",
      "7251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004732] [C loss: 0.693596]\n",
      "7252 [D loss: 0.693598, acc.: 0.00%] [G loss: 0.015793] [C loss: 0.693597]\n",
      "7253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004663] [C loss: 0.693597]\n",
      "7254 [D loss: 0.693597, acc.: 0.00%] [G loss: 0.012155] [C loss: 0.693595]\n",
      "7255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004808] [C loss: 0.693595]\n",
      "7256 [D loss: 0.693596, acc.: 0.00%] [G loss: 0.010598] [C loss: 0.693593]\n",
      "7257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005309] [C loss: 0.693593]\n",
      "7258 [D loss: 0.693595, acc.: 0.00%] [G loss: 0.006859] [C loss: 0.693593]\n",
      "7259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.693593]\n",
      "7260 [D loss: 0.693594, acc.: 0.00%] [G loss: 0.007049] [C loss: 0.693592]\n",
      "7261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005649] [C loss: 0.693592]\n",
      "7262 [D loss: 0.693594, acc.: 0.00%] [G loss: 0.005843] [C loss: 0.693592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004561] [C loss: 0.693592]\n",
      "7264 [D loss: 0.693593, acc.: 0.00%] [G loss: 0.005927] [C loss: 0.693590]\n",
      "7265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005328] [C loss: 0.693590]\n",
      "7266 [D loss: 0.693592, acc.: 0.00%] [G loss: 0.006053] [C loss: 0.693590]\n",
      "7267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005730] [C loss: 0.693590]\n",
      "7268 [D loss: 0.693591, acc.: 0.00%] [G loss: 0.006472] [C loss: 0.693588]\n",
      "7269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005043] [C loss: 0.693588]\n",
      "7270 [D loss: 0.693590, acc.: 0.00%] [G loss: 0.006148] [C loss: 0.693586]\n",
      "7271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004889] [C loss: 0.693586]\n",
      "7272 [D loss: 0.693588, acc.: 0.00%] [G loss: 0.003682] [C loss: 0.693586]\n",
      "7273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004228] [C loss: 0.693586]\n",
      "7274 [D loss: 0.693587, acc.: 0.00%] [G loss: 0.003765] [C loss: 0.693586]\n",
      "7275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004415] [C loss: 0.693586]\n",
      "7276 [D loss: 0.693587, acc.: 0.00%] [G loss: 0.004873] [C loss: 0.693585]\n",
      "7277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.693585]\n",
      "7278 [D loss: 0.693587, acc.: 0.00%] [G loss: 0.008766] [C loss: 0.693586]\n",
      "7279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004053] [C loss: 0.693586]\n",
      "7280 [D loss: 0.693586, acc.: 0.00%] [G loss: 0.003716] [C loss: 0.693583]\n",
      "7281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004532] [C loss: 0.693583]\n",
      "7282 [D loss: 0.693585, acc.: 0.00%] [G loss: 0.009761] [C loss: 0.693584]\n",
      "7283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005585] [C loss: 0.693584]\n",
      "7284 [D loss: 0.693585, acc.: 0.00%] [G loss: 0.005241] [C loss: 0.693583]\n",
      "7285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004841] [C loss: 0.693583]\n",
      "7286 [D loss: 0.693584, acc.: 0.00%] [G loss: 0.005996] [C loss: 0.693582]\n",
      "7287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003251] [C loss: 0.693582]\n",
      "7288 [D loss: 0.693583, acc.: 0.00%] [G loss: 0.009489] [C loss: 0.693580]\n",
      "7289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004738] [C loss: 0.693580]\n",
      "7290 [D loss: 0.693582, acc.: 0.00%] [G loss: 0.005697] [C loss: 0.693581]\n",
      "7291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005825] [C loss: 0.693581]\n",
      "7292 [D loss: 0.693582, acc.: 0.00%] [G loss: 0.004725] [C loss: 0.693579]\n",
      "7293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008687] [C loss: 0.693579]\n",
      "7294 [D loss: 0.693580, acc.: 0.00%] [G loss: 0.006244] [C loss: 0.693578]\n",
      "7295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004580] [C loss: 0.693578]\n",
      "7296 [D loss: 0.693579, acc.: 0.00%] [G loss: 0.005283] [C loss: 0.693577]\n",
      "7297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005332] [C loss: 0.693577]\n",
      "7298 [D loss: 0.693578, acc.: 0.00%] [G loss: 0.006561] [C loss: 0.693575]\n",
      "7299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005642] [C loss: 0.693575]\n",
      "7300 [D loss: 0.693577, acc.: 0.00%] [G loss: 0.004655] [C loss: 0.693574]\n",
      "7301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004625] [C loss: 0.693574]\n",
      "7302 [D loss: 0.693576, acc.: 0.00%] [G loss: 0.005397] [C loss: 0.693573]\n",
      "7303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005772] [C loss: 0.693573]\n",
      "7304 [D loss: 0.693575, acc.: 0.00%] [G loss: 0.004080] [C loss: 0.693573]\n",
      "7305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004876] [C loss: 0.693573]\n",
      "7306 [D loss: 0.693574, acc.: 0.00%] [G loss: 0.005455] [C loss: 0.693573]\n",
      "7307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004415] [C loss: 0.693573]\n",
      "7308 [D loss: 0.693574, acc.: 0.00%] [G loss: 0.004421] [C loss: 0.693572]\n",
      "7309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006783] [C loss: 0.693572]\n",
      "7310 [D loss: 0.693573, acc.: 0.00%] [G loss: 0.004313] [C loss: 0.693571]\n",
      "7311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007580] [C loss: 0.693571]\n",
      "7312 [D loss: 0.693572, acc.: 0.00%] [G loss: 0.011022] [C loss: 0.693570]\n",
      "7313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007323] [C loss: 0.693570]\n",
      "7314 [D loss: 0.693572, acc.: 0.00%] [G loss: 0.005657] [C loss: 0.693569]\n",
      "7315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005571] [C loss: 0.693569]\n",
      "7316 [D loss: 0.693570, acc.: 0.00%] [G loss: 0.005296] [C loss: 0.693568]\n",
      "7317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693568]\n",
      "7318 [D loss: 0.693569, acc.: 0.00%] [G loss: 0.004720] [C loss: 0.693568]\n",
      "7319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007892] [C loss: 0.693568]\n",
      "7320 [D loss: 0.693569, acc.: 0.00%] [G loss: 0.005378] [C loss: 0.693567]\n",
      "7321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003284] [C loss: 0.693567]\n",
      "7322 [D loss: 0.693568, acc.: 0.00%] [G loss: 0.017745] [C loss: 0.693566]\n",
      "7323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003970] [C loss: 0.693566]\n",
      "7324 [D loss: 0.693567, acc.: 0.00%] [G loss: 0.004762] [C loss: 0.693565]\n",
      "7325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015856] [C loss: 0.693565]\n",
      "7326 [D loss: 0.693567, acc.: 0.00%] [G loss: 0.006725] [C loss: 0.693564]\n",
      "7327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006966] [C loss: 0.693564]\n",
      "7328 [D loss: 0.693566, acc.: 0.00%] [G loss: 0.005428] [C loss: 0.693564]\n",
      "7329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007989] [C loss: 0.693564]\n",
      "7330 [D loss: 0.693565, acc.: 0.00%] [G loss: 0.005327] [C loss: 0.693562]\n",
      "7331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004100] [C loss: 0.693562]\n",
      "7332 [D loss: 0.693564, acc.: 0.00%] [G loss: 0.007860] [C loss: 0.693562]\n",
      "7333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005356] [C loss: 0.693562]\n",
      "7334 [D loss: 0.693563, acc.: 0.00%] [G loss: 0.005197] [C loss: 0.693561]\n",
      "7335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006194] [C loss: 0.693561]\n",
      "7336 [D loss: 0.693562, acc.: 0.00%] [G loss: 0.004971] [C loss: 0.693559]\n",
      "7337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005643] [C loss: 0.693559]\n",
      "7338 [D loss: 0.693561, acc.: 0.00%] [G loss: 0.004721] [C loss: 0.693560]\n",
      "7339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004649] [C loss: 0.693560]\n",
      "7340 [D loss: 0.693561, acc.: 0.00%] [G loss: 0.003949] [C loss: 0.693558]\n",
      "7341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007047] [C loss: 0.693558]\n",
      "7342 [D loss: 0.693560, acc.: 0.00%] [G loss: 0.004227] [C loss: 0.693557]\n",
      "7343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004228] [C loss: 0.693557]\n",
      "7344 [D loss: 0.693559, acc.: 0.00%] [G loss: 0.004659] [C loss: 0.693557]\n",
      "7345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017052] [C loss: 0.693557]\n",
      "7346 [D loss: 0.693559, acc.: 0.00%] [G loss: 0.006134] [C loss: 0.693556]\n",
      "7347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005324] [C loss: 0.693556]\n",
      "7348 [D loss: 0.693557, acc.: 0.00%] [G loss: 0.005294] [C loss: 0.693555]\n",
      "7349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017407] [C loss: 0.693555]\n",
      "7350 [D loss: 0.693557, acc.: 0.00%] [G loss: 0.004056] [C loss: 0.693555]\n",
      "7351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005325] [C loss: 0.693555]\n",
      "7352 [D loss: 0.693556, acc.: 0.00%] [G loss: 0.004426] [C loss: 0.693554]\n",
      "7353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005444] [C loss: 0.693554]\n",
      "7354 [D loss: 0.693555, acc.: 0.00%] [G loss: 0.006414] [C loss: 0.693553]\n",
      "7355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009010] [C loss: 0.693553]\n",
      "7356 [D loss: 0.693554, acc.: 0.00%] [G loss: 0.004113] [C loss: 0.693550]\n",
      "7357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004388] [C loss: 0.693550]\n",
      "7358 [D loss: 0.693553, acc.: 0.00%] [G loss: 0.005467] [C loss: 0.693551]\n",
      "7359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004033] [C loss: 0.693551]\n",
      "7360 [D loss: 0.693552, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.693550]\n",
      "7361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006438] [C loss: 0.693550]\n",
      "7362 [D loss: 0.693551, acc.: 0.00%] [G loss: 0.003825] [C loss: 0.693549]\n",
      "7363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008360] [C loss: 0.693549]\n",
      "7364 [D loss: 0.693551, acc.: 0.00%] [G loss: 0.004842] [C loss: 0.693549]\n",
      "7365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005178] [C loss: 0.693549]\n",
      "7366 [D loss: 0.693551, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.693548]\n",
      "7367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021216] [C loss: 0.693548]\n",
      "7368 [D loss: 0.693550, acc.: 0.00%] [G loss: 0.005483] [C loss: 0.693547]\n",
      "7369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005905] [C loss: 0.693547]\n",
      "7370 [D loss: 0.693549, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.693547]\n",
      "7371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004140] [C loss: 0.693547]\n",
      "7372 [D loss: 0.693548, acc.: 0.00%] [G loss: 0.004922] [C loss: 0.693545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004483] [C loss: 0.693545]\n",
      "7374 [D loss: 0.693547, acc.: 0.00%] [G loss: 0.005282] [C loss: 0.693545]\n",
      "7375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006129] [C loss: 0.693545]\n",
      "7376 [D loss: 0.693546, acc.: 0.00%] [G loss: 0.006793] [C loss: 0.693544]\n",
      "7377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005251] [C loss: 0.693544]\n",
      "7378 [D loss: 0.693545, acc.: 0.00%] [G loss: 0.003586] [C loss: 0.693542]\n",
      "7379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010187] [C loss: 0.693542]\n",
      "7380 [D loss: 0.693544, acc.: 0.00%] [G loss: 0.004778] [C loss: 0.693542]\n",
      "7381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006254] [C loss: 0.693542]\n",
      "7382 [D loss: 0.693544, acc.: 0.00%] [G loss: 0.009846] [C loss: 0.693541]\n",
      "7383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004083] [C loss: 0.693541]\n",
      "7384 [D loss: 0.693543, acc.: 0.00%] [G loss: 0.002933] [C loss: 0.693540]\n",
      "7385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004023] [C loss: 0.693540]\n",
      "7386 [D loss: 0.693542, acc.: 0.00%] [G loss: 0.006313] [C loss: 0.693540]\n",
      "7387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005588] [C loss: 0.693540]\n",
      "7388 [D loss: 0.693542, acc.: 0.00%] [G loss: 0.005452] [C loss: 0.693539]\n",
      "7389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006226] [C loss: 0.693539]\n",
      "7390 [D loss: 0.693541, acc.: 0.00%] [G loss: 0.004606] [C loss: 0.693537]\n",
      "7391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003856] [C loss: 0.693537]\n",
      "7392 [D loss: 0.693540, acc.: 0.00%] [G loss: 0.004661] [C loss: 0.693538]\n",
      "7393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004771] [C loss: 0.693538]\n",
      "7394 [D loss: 0.693539, acc.: 0.00%] [G loss: 0.005421] [C loss: 0.693537]\n",
      "7395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009602] [C loss: 0.693537]\n",
      "7396 [D loss: 0.693539, acc.: 0.00%] [G loss: 0.005230] [C loss: 0.693536]\n",
      "7397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005639] [C loss: 0.693536]\n",
      "7398 [D loss: 0.693537, acc.: 0.00%] [G loss: 0.005232] [C loss: 0.693536]\n",
      "7399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004112] [C loss: 0.693536]\n",
      "7400 [D loss: 0.693537, acc.: 0.00%] [G loss: 0.008149] [C loss: 0.693534]\n",
      "7401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004576] [C loss: 0.693534]\n",
      "7402 [D loss: 0.693536, acc.: 0.00%] [G loss: 0.005691] [C loss: 0.693534]\n",
      "7403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006028] [C loss: 0.693534]\n",
      "7404 [D loss: 0.693535, acc.: 0.00%] [G loss: 0.005062] [C loss: 0.693533]\n",
      "7405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004495] [C loss: 0.693533]\n",
      "7406 [D loss: 0.693535, acc.: 0.00%] [G loss: 0.008332] [C loss: 0.693533]\n",
      "7407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004285] [C loss: 0.693533]\n",
      "7408 [D loss: 0.693534, acc.: 0.00%] [G loss: 0.003852] [C loss: 0.693530]\n",
      "7409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004719] [C loss: 0.693530]\n",
      "7410 [D loss: 0.693533, acc.: 0.00%] [G loss: 0.005842] [C loss: 0.693531]\n",
      "7411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010393] [C loss: 0.693531]\n",
      "7412 [D loss: 0.693532, acc.: 0.00%] [G loss: 0.005719] [C loss: 0.693530]\n",
      "7413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005427] [C loss: 0.693530]\n",
      "7414 [D loss: 0.693532, acc.: 0.00%] [G loss: 0.004577] [C loss: 0.693529]\n",
      "7415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006359] [C loss: 0.693529]\n",
      "7416 [D loss: 0.693531, acc.: 0.00%] [G loss: 0.005402] [C loss: 0.693529]\n",
      "7417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006413] [C loss: 0.693529]\n",
      "7418 [D loss: 0.693530, acc.: 0.00%] [G loss: 0.004955] [C loss: 0.693528]\n",
      "7419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006623] [C loss: 0.693528]\n",
      "7420 [D loss: 0.693529, acc.: 0.00%] [G loss: 0.004346] [C loss: 0.693527]\n",
      "7421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006019] [C loss: 0.693527]\n",
      "7422 [D loss: 0.693529, acc.: 0.00%] [G loss: 0.004141] [C loss: 0.693526]\n",
      "7423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006453] [C loss: 0.693526]\n",
      "7424 [D loss: 0.693528, acc.: 0.00%] [G loss: 0.004315] [C loss: 0.693525]\n",
      "7425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.693525]\n",
      "7426 [D loss: 0.693526, acc.: 0.00%] [G loss: 0.005429] [C loss: 0.693525]\n",
      "7427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005274] [C loss: 0.693525]\n",
      "7428 [D loss: 0.693526, acc.: 0.00%] [G loss: 0.005838] [C loss: 0.693524]\n",
      "7429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004894] [C loss: 0.693524]\n",
      "7430 [D loss: 0.693525, acc.: 0.00%] [G loss: 0.004794] [C loss: 0.693522]\n",
      "7431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004840] [C loss: 0.693522]\n",
      "7432 [D loss: 0.693524, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.693523]\n",
      "7433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005326] [C loss: 0.693523]\n",
      "7434 [D loss: 0.693524, acc.: 0.00%] [G loss: 0.004722] [C loss: 0.693521]\n",
      "7435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006242] [C loss: 0.693521]\n",
      "7436 [D loss: 0.693523, acc.: 0.00%] [G loss: 0.004022] [C loss: 0.693521]\n",
      "7437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004460] [C loss: 0.693521]\n",
      "7438 [D loss: 0.693522, acc.: 0.00%] [G loss: 0.009231] [C loss: 0.693520]\n",
      "7439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.693520]\n",
      "7440 [D loss: 0.693521, acc.: 0.00%] [G loss: 0.005759] [C loss: 0.693519]\n",
      "7441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007346] [C loss: 0.693519]\n",
      "7442 [D loss: 0.693520, acc.: 0.00%] [G loss: 0.004877] [C loss: 0.693517]\n",
      "7443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003582] [C loss: 0.693517]\n",
      "7444 [D loss: 0.693519, acc.: 0.00%] [G loss: 0.004894] [C loss: 0.693517]\n",
      "7445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007383] [C loss: 0.693517]\n",
      "7446 [D loss: 0.693519, acc.: 0.00%] [G loss: 0.004545] [C loss: 0.693517]\n",
      "7447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003983] [C loss: 0.693517]\n",
      "7448 [D loss: 0.693518, acc.: 0.00%] [G loss: 0.005095] [C loss: 0.693516]\n",
      "7449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003963] [C loss: 0.693516]\n",
      "7450 [D loss: 0.693517, acc.: 0.00%] [G loss: 0.004967] [C loss: 0.693515]\n",
      "7451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003770] [C loss: 0.693515]\n",
      "7452 [D loss: 0.693517, acc.: 0.00%] [G loss: 0.008521] [C loss: 0.693514]\n",
      "7453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003638] [C loss: 0.693514]\n",
      "7454 [D loss: 0.693516, acc.: 0.00%] [G loss: 0.006068] [C loss: 0.693514]\n",
      "7455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003984] [C loss: 0.693514]\n",
      "7456 [D loss: 0.693515, acc.: 0.00%] [G loss: 0.004732] [C loss: 0.693512]\n",
      "7457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005538] [C loss: 0.693512]\n",
      "7458 [D loss: 0.693515, acc.: 0.00%] [G loss: 0.004860] [C loss: 0.693511]\n",
      "7459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005847] [C loss: 0.693511]\n",
      "7460 [D loss: 0.693514, acc.: 0.00%] [G loss: 0.004361] [C loss: 0.693510]\n",
      "7461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005746] [C loss: 0.693510]\n",
      "7462 [D loss: 0.693512, acc.: 0.00%] [G loss: 0.005035] [C loss: 0.693510]\n",
      "7463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013576] [C loss: 0.693510]\n",
      "7464 [D loss: 0.693512, acc.: 0.00%] [G loss: 0.004780] [C loss: 0.693510]\n",
      "7465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013769] [C loss: 0.693510]\n",
      "7466 [D loss: 0.693512, acc.: 0.00%] [G loss: 0.003801] [C loss: 0.693509]\n",
      "7467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006405] [C loss: 0.693509]\n",
      "7468 [D loss: 0.693511, acc.: 0.00%] [G loss: 0.003911] [C loss: 0.693508]\n",
      "7469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004555] [C loss: 0.693508]\n",
      "7470 [D loss: 0.693510, acc.: 0.00%] [G loss: 0.006374] [C loss: 0.693508]\n",
      "7471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005037] [C loss: 0.693508]\n",
      "7472 [D loss: 0.693509, acc.: 0.00%] [G loss: 0.006916] [C loss: 0.693507]\n",
      "7473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005462] [C loss: 0.693507]\n",
      "7474 [D loss: 0.693509, acc.: 0.00%] [G loss: 0.004302] [C loss: 0.693507]\n",
      "7475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008135] [C loss: 0.693507]\n",
      "7476 [D loss: 0.693508, acc.: 0.00%] [G loss: 0.003690] [C loss: 0.693505]\n",
      "7477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004402] [C loss: 0.693505]\n",
      "7478 [D loss: 0.693507, acc.: 0.00%] [G loss: 0.005949] [C loss: 0.693505]\n",
      "7479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005538] [C loss: 0.693505]\n",
      "7480 [D loss: 0.693507, acc.: 0.00%] [G loss: 0.004352] [C loss: 0.693504]\n",
      "7481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015058] [C loss: 0.693504]\n",
      "7482 [D loss: 0.693505, acc.: 0.00%] [G loss: 0.004111] [C loss: 0.693503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008482] [C loss: 0.693503]\n",
      "7484 [D loss: 0.693505, acc.: 0.00%] [G loss: 0.003493] [C loss: 0.693502]\n",
      "7485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005138] [C loss: 0.693502]\n",
      "7486 [D loss: 0.693504, acc.: 0.00%] [G loss: 0.004263] [C loss: 0.693502]\n",
      "7487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005443] [C loss: 0.693502]\n",
      "7488 [D loss: 0.693504, acc.: 0.00%] [G loss: 0.004629] [C loss: 0.693500]\n",
      "7489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005250] [C loss: 0.693500]\n",
      "7490 [D loss: 0.693502, acc.: 0.00%] [G loss: 0.004368] [C loss: 0.693500]\n",
      "7491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004260] [C loss: 0.693500]\n",
      "7492 [D loss: 0.693502, acc.: 0.00%] [G loss: 0.009144] [C loss: 0.693500]\n",
      "7493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003575] [C loss: 0.693500]\n",
      "7494 [D loss: 0.693502, acc.: 0.00%] [G loss: 0.005335] [C loss: 0.693498]\n",
      "7495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004008] [C loss: 0.693498]\n",
      "7496 [D loss: 0.693500, acc.: 0.00%] [G loss: 0.003588] [C loss: 0.693498]\n",
      "7497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004681] [C loss: 0.693498]\n",
      "7498 [D loss: 0.693501, acc.: 0.00%] [G loss: 0.005186] [C loss: 0.693497]\n",
      "7499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005230] [C loss: 0.693497]\n",
      "7500 [D loss: 0.693499, acc.: 0.00%] [G loss: 0.005015] [C loss: 0.693497]\n",
      "7501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006348] [C loss: 0.693497]\n",
      "7502 [D loss: 0.693498, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.693496]\n",
      "7503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005431] [C loss: 0.693496]\n",
      "7504 [D loss: 0.693498, acc.: 0.00%] [G loss: 0.013436] [C loss: 0.693495]\n",
      "7505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005517] [C loss: 0.693495]\n",
      "7506 [D loss: 0.693496, acc.: 0.00%] [G loss: 0.005519] [C loss: 0.693495]\n",
      "7507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008548] [C loss: 0.693495]\n",
      "7508 [D loss: 0.693497, acc.: 0.00%] [G loss: 0.005647] [C loss: 0.693494]\n",
      "7509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.693494]\n",
      "7510 [D loss: 0.693496, acc.: 0.00%] [G loss: 0.019176] [C loss: 0.693493]\n",
      "7511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005155] [C loss: 0.693493]\n",
      "7512 [D loss: 0.693494, acc.: 0.00%] [G loss: 0.013808] [C loss: 0.693491]\n",
      "7513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005585] [C loss: 0.693491]\n",
      "7514 [D loss: 0.693493, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.693491]\n",
      "7515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004361] [C loss: 0.693491]\n",
      "7516 [D loss: 0.693493, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.693490]\n",
      "7517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005413] [C loss: 0.693490]\n",
      "7518 [D loss: 0.693492, acc.: 0.00%] [G loss: 0.006125] [C loss: 0.693489]\n",
      "7519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004923] [C loss: 0.693489]\n",
      "7520 [D loss: 0.693491, acc.: 0.00%] [G loss: 0.004645] [C loss: 0.693489]\n",
      "7521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004104] [C loss: 0.693489]\n",
      "7522 [D loss: 0.693491, acc.: 0.00%] [G loss: 0.018019] [C loss: 0.693488]\n",
      "7523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.693488]\n",
      "7524 [D loss: 0.693490, acc.: 0.00%] [G loss: 0.004968] [C loss: 0.693488]\n",
      "7525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005185] [C loss: 0.693488]\n",
      "7526 [D loss: 0.693489, acc.: 0.00%] [G loss: 0.008331] [C loss: 0.693486]\n",
      "7527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004337] [C loss: 0.693486]\n",
      "7528 [D loss: 0.693488, acc.: 0.00%] [G loss: 0.006340] [C loss: 0.693485]\n",
      "7529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005820] [C loss: 0.693485]\n",
      "7530 [D loss: 0.693488, acc.: 0.00%] [G loss: 0.004432] [C loss: 0.693485]\n",
      "7531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.693485]\n",
      "7532 [D loss: 0.693487, acc.: 0.00%] [G loss: 0.004448] [C loss: 0.693484]\n",
      "7533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004133] [C loss: 0.693484]\n",
      "7534 [D loss: 0.693486, acc.: 0.00%] [G loss: 0.008640] [C loss: 0.693484]\n",
      "7535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008266] [C loss: 0.693484]\n",
      "7536 [D loss: 0.693485, acc.: 0.00%] [G loss: 0.004245] [C loss: 0.693484]\n",
      "7537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007236] [C loss: 0.693484]\n",
      "7538 [D loss: 0.693486, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.693482]\n",
      "7539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006238] [C loss: 0.693482]\n",
      "7540 [D loss: 0.693484, acc.: 0.00%] [G loss: 0.007732] [C loss: 0.693481]\n",
      "7541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005586] [C loss: 0.693481]\n",
      "7542 [D loss: 0.693483, acc.: 0.00%] [G loss: 0.005117] [C loss: 0.693481]\n",
      "7543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006603] [C loss: 0.693481]\n",
      "7544 [D loss: 0.693484, acc.: 0.00%] [G loss: 0.005093] [C loss: 0.693481]\n",
      "7545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004531] [C loss: 0.693481]\n",
      "7546 [D loss: 0.693482, acc.: 0.00%] [G loss: 0.006717] [C loss: 0.693479]\n",
      "7547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005051] [C loss: 0.693479]\n",
      "7548 [D loss: 0.693481, acc.: 0.00%] [G loss: 0.004509] [C loss: 0.693479]\n",
      "7549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005054] [C loss: 0.693479]\n",
      "7550 [D loss: 0.693481, acc.: 0.00%] [G loss: 0.005409] [C loss: 0.693479]\n",
      "7551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004514] [C loss: 0.693479]\n",
      "7552 [D loss: 0.693480, acc.: 0.00%] [G loss: 0.005805] [C loss: 0.693479]\n",
      "7553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005124] [C loss: 0.693479]\n",
      "7554 [D loss: 0.693480, acc.: 0.00%] [G loss: 0.006575] [C loss: 0.693476]\n",
      "7555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003266] [C loss: 0.693476]\n",
      "7556 [D loss: 0.693478, acc.: 0.00%] [G loss: 0.004195] [C loss: 0.693475]\n",
      "7557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004531] [C loss: 0.693475]\n",
      "7558 [D loss: 0.693477, acc.: 0.00%] [G loss: 0.004244] [C loss: 0.693475]\n",
      "7559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005634] [C loss: 0.693475]\n",
      "7560 [D loss: 0.693477, acc.: 0.00%] [G loss: 0.003768] [C loss: 0.693474]\n",
      "7561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003924] [C loss: 0.693474]\n",
      "7562 [D loss: 0.693477, acc.: 0.00%] [G loss: 0.007108] [C loss: 0.693474]\n",
      "7563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004716] [C loss: 0.693474]\n",
      "7564 [D loss: 0.693476, acc.: 0.00%] [G loss: 0.004227] [C loss: 0.693473]\n",
      "7565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004231] [C loss: 0.693473]\n",
      "7566 [D loss: 0.693475, acc.: 0.00%] [G loss: 0.003718] [C loss: 0.693471]\n",
      "7567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005258] [C loss: 0.693471]\n",
      "7568 [D loss: 0.693473, acc.: 0.00%] [G loss: 0.005314] [C loss: 0.693470]\n",
      "7569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.693470]\n",
      "7570 [D loss: 0.693473, acc.: 0.00%] [G loss: 0.004964] [C loss: 0.693470]\n",
      "7571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005241] [C loss: 0.693470]\n",
      "7572 [D loss: 0.693473, acc.: 0.00%] [G loss: 0.004853] [C loss: 0.693470]\n",
      "7573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.693470]\n",
      "7574 [D loss: 0.693472, acc.: 0.00%] [G loss: 0.004927] [C loss: 0.693471]\n",
      "7575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008202] [C loss: 0.693471]\n",
      "7576 [D loss: 0.693472, acc.: 0.00%] [G loss: 0.004660] [C loss: 0.693469]\n",
      "7577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005708] [C loss: 0.693469]\n",
      "7578 [D loss: 0.693471, acc.: 0.00%] [G loss: 0.004622] [C loss: 0.693468]\n",
      "7579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004203] [C loss: 0.693468]\n",
      "7580 [D loss: 0.693470, acc.: 0.00%] [G loss: 0.006052] [C loss: 0.693467]\n",
      "7581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005130] [C loss: 0.693467]\n",
      "7582 [D loss: 0.693470, acc.: 0.00%] [G loss: 0.004026] [C loss: 0.693466]\n",
      "7583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004122] [C loss: 0.693466]\n",
      "7584 [D loss: 0.693469, acc.: 0.00%] [G loss: 0.005005] [C loss: 0.693466]\n",
      "7585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004639] [C loss: 0.693466]\n",
      "7586 [D loss: 0.693469, acc.: 0.00%] [G loss: 0.006522] [C loss: 0.693466]\n",
      "7587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.693466]\n",
      "7588 [D loss: 0.693467, acc.: 0.00%] [G loss: 0.004677] [C loss: 0.693465]\n",
      "7589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009839] [C loss: 0.693465]\n",
      "7590 [D loss: 0.693466, acc.: 0.00%] [G loss: 0.004931] [C loss: 0.693464]\n",
      "7591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004181] [C loss: 0.693464]\n",
      "7592 [D loss: 0.693466, acc.: 0.00%] [G loss: 0.004056] [C loss: 0.693465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004634] [C loss: 0.693465]\n",
      "7594 [D loss: 0.693466, acc.: 0.00%] [G loss: 0.012276] [C loss: 0.693463]\n",
      "7595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006120] [C loss: 0.693463]\n",
      "7596 [D loss: 0.693464, acc.: 0.00%] [G loss: 0.003960] [C loss: 0.693462]\n",
      "7597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005808] [C loss: 0.693462]\n",
      "7598 [D loss: 0.693464, acc.: 0.00%] [G loss: 0.004507] [C loss: 0.693462]\n",
      "7599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004440] [C loss: 0.693462]\n",
      "7600 [D loss: 0.693464, acc.: 0.00%] [G loss: 0.003923] [C loss: 0.693460]\n",
      "7601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005308] [C loss: 0.693460]\n",
      "7602 [D loss: 0.693463, acc.: 0.00%] [G loss: 0.006770] [C loss: 0.693459]\n",
      "7603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003967] [C loss: 0.693459]\n",
      "7604 [D loss: 0.693461, acc.: 0.00%] [G loss: 0.005328] [C loss: 0.693460]\n",
      "7605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003786] [C loss: 0.693460]\n",
      "7606 [D loss: 0.693462, acc.: 0.00%] [G loss: 0.005872] [C loss: 0.693459]\n",
      "7607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009434] [C loss: 0.693459]\n",
      "7608 [D loss: 0.693461, acc.: 0.00%] [G loss: 0.007607] [C loss: 0.693458]\n",
      "7609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006463] [C loss: 0.693458]\n",
      "7610 [D loss: 0.693460, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.693457]\n",
      "7611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004070] [C loss: 0.693457]\n",
      "7612 [D loss: 0.693460, acc.: 0.00%] [G loss: 0.005471] [C loss: 0.693458]\n",
      "7613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005762] [C loss: 0.693458]\n",
      "7614 [D loss: 0.693459, acc.: 0.00%] [G loss: 0.008916] [C loss: 0.693455]\n",
      "7615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004362] [C loss: 0.693455]\n",
      "7616 [D loss: 0.693458, acc.: 0.00%] [G loss: 0.006073] [C loss: 0.693455]\n",
      "7617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004569] [C loss: 0.693455]\n",
      "7618 [D loss: 0.693457, acc.: 0.00%] [G loss: 0.005518] [C loss: 0.693453]\n",
      "7619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007085] [C loss: 0.693453]\n",
      "7620 [D loss: 0.693456, acc.: 0.00%] [G loss: 0.004668] [C loss: 0.693453]\n",
      "7621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005199] [C loss: 0.693453]\n",
      "7622 [D loss: 0.693456, acc.: 0.00%] [G loss: 0.005158] [C loss: 0.693452]\n",
      "7623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007968] [C loss: 0.693452]\n",
      "7624 [D loss: 0.693455, acc.: 0.00%] [G loss: 0.006501] [C loss: 0.693452]\n",
      "7625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019791] [C loss: 0.693452]\n",
      "7626 [D loss: 0.693455, acc.: 0.00%] [G loss: 0.005518] [C loss: 0.693451]\n",
      "7627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004406] [C loss: 0.693451]\n",
      "7628 [D loss: 0.693454, acc.: 0.00%] [G loss: 0.004185] [C loss: 0.693452]\n",
      "7629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011940] [C loss: 0.693452]\n",
      "7630 [D loss: 0.693454, acc.: 0.00%] [G loss: 0.004963] [C loss: 0.693450]\n",
      "7631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006339] [C loss: 0.693450]\n",
      "7632 [D loss: 0.693452, acc.: 0.00%] [G loss: 0.004299] [C loss: 0.693450]\n",
      "7633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007435] [C loss: 0.693450]\n",
      "7634 [D loss: 0.693452, acc.: 0.00%] [G loss: 0.005567] [C loss: 0.693448]\n",
      "7635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006285] [C loss: 0.693448]\n",
      "7636 [D loss: 0.693451, acc.: 0.00%] [G loss: 0.018176] [C loss: 0.693448]\n",
      "7637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005233] [C loss: 0.693448]\n",
      "7638 [D loss: 0.693451, acc.: 0.00%] [G loss: 0.004669] [C loss: 0.693448]\n",
      "7639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006957] [C loss: 0.693448]\n",
      "7640 [D loss: 0.693450, acc.: 0.00%] [G loss: 0.007082] [C loss: 0.693448]\n",
      "7641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.693448]\n",
      "7642 [D loss: 0.693450, acc.: 0.00%] [G loss: 0.003600] [C loss: 0.693448]\n",
      "7643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005382] [C loss: 0.693448]\n",
      "7644 [D loss: 0.693449, acc.: 0.00%] [G loss: 0.005766] [C loss: 0.693447]\n",
      "7645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005405] [C loss: 0.693447]\n",
      "7646 [D loss: 0.693449, acc.: 0.00%] [G loss: 0.003513] [C loss: 0.693445]\n",
      "7647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003672] [C loss: 0.693445]\n",
      "7648 [D loss: 0.693447, acc.: 0.00%] [G loss: 0.005724] [C loss: 0.693444]\n",
      "7649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003888] [C loss: 0.693444]\n",
      "7650 [D loss: 0.693447, acc.: 0.00%] [G loss: 0.004999] [C loss: 0.693443]\n",
      "7651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007353] [C loss: 0.693443]\n",
      "7652 [D loss: 0.693446, acc.: 0.00%] [G loss: 0.004144] [C loss: 0.693443]\n",
      "7653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004700] [C loss: 0.693443]\n",
      "7654 [D loss: 0.693446, acc.: 0.00%] [G loss: 0.021470] [C loss: 0.693443]\n",
      "7655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003691] [C loss: 0.693443]\n",
      "7656 [D loss: 0.693445, acc.: 0.00%] [G loss: 0.004676] [C loss: 0.693441]\n",
      "7657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004401] [C loss: 0.693441]\n",
      "7658 [D loss: 0.693444, acc.: 0.00%] [G loss: 0.005987] [C loss: 0.693441]\n",
      "7659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005794] [C loss: 0.693441]\n",
      "7660 [D loss: 0.693443, acc.: 0.00%] [G loss: 0.004152] [C loss: 0.693440]\n",
      "7661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004304] [C loss: 0.693440]\n",
      "7662 [D loss: 0.693443, acc.: 0.00%] [G loss: 0.003036] [C loss: 0.693440]\n",
      "7663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004072] [C loss: 0.693440]\n",
      "7664 [D loss: 0.693443, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.693440]\n",
      "7665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006460] [C loss: 0.693440]\n",
      "7666 [D loss: 0.693442, acc.: 0.00%] [G loss: 0.005333] [C loss: 0.693439]\n",
      "7667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005740] [C loss: 0.693439]\n",
      "7668 [D loss: 0.693442, acc.: 0.00%] [G loss: 0.006388] [C loss: 0.693438]\n",
      "7669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004676] [C loss: 0.693438]\n",
      "7670 [D loss: 0.693440, acc.: 0.00%] [G loss: 0.008863] [C loss: 0.693437]\n",
      "7671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005089] [C loss: 0.693437]\n",
      "7672 [D loss: 0.693440, acc.: 0.00%] [G loss: 0.005924] [C loss: 0.693437]\n",
      "7673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005410] [C loss: 0.693437]\n",
      "7674 [D loss: 0.693439, acc.: 0.00%] [G loss: 0.006557] [C loss: 0.693435]\n",
      "7675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004784] [C loss: 0.693435]\n",
      "7676 [D loss: 0.693439, acc.: 0.00%] [G loss: 0.005076] [C loss: 0.693435]\n",
      "7677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004674] [C loss: 0.693435]\n",
      "7678 [D loss: 0.693438, acc.: 0.00%] [G loss: 0.004184] [C loss: 0.693435]\n",
      "7679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005547] [C loss: 0.693435]\n",
      "7680 [D loss: 0.693437, acc.: 0.00%] [G loss: 0.004373] [C loss: 0.693434]\n",
      "7681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004434] [C loss: 0.693434]\n",
      "7682 [D loss: 0.693436, acc.: 0.00%] [G loss: 0.004394] [C loss: 0.693433]\n",
      "7683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005784] [C loss: 0.693433]\n",
      "7684 [D loss: 0.693436, acc.: 0.00%] [G loss: 0.004772] [C loss: 0.693434]\n",
      "7685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003573] [C loss: 0.693434]\n",
      "7686 [D loss: 0.693436, acc.: 0.00%] [G loss: 0.010008] [C loss: 0.693433]\n",
      "7687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008054] [C loss: 0.693433]\n",
      "7688 [D loss: 0.693435, acc.: 0.00%] [G loss: 0.006957] [C loss: 0.693432]\n",
      "7689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004113] [C loss: 0.693432]\n",
      "7690 [D loss: 0.693434, acc.: 0.00%] [G loss: 0.006591] [C loss: 0.693432]\n",
      "7691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005371] [C loss: 0.693432]\n",
      "7692 [D loss: 0.693435, acc.: 0.00%] [G loss: 0.006488] [C loss: 0.693430]\n",
      "7693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004537] [C loss: 0.693430]\n",
      "7694 [D loss: 0.693434, acc.: 0.00%] [G loss: 0.005383] [C loss: 0.693430]\n",
      "7695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003346] [C loss: 0.693430]\n",
      "7696 [D loss: 0.693432, acc.: 0.00%] [G loss: 0.004620] [C loss: 0.693429]\n",
      "7697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.693429]\n",
      "7698 [D loss: 0.693432, acc.: 0.00%] [G loss: 0.023368] [C loss: 0.693428]\n",
      "7699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006419] [C loss: 0.693428]\n",
      "7700 [D loss: 0.693431, acc.: 0.00%] [G loss: 0.004641] [C loss: 0.693427]\n",
      "7701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005341] [C loss: 0.693427]\n",
      "7702 [D loss: 0.693430, acc.: 0.00%] [G loss: 0.005957] [C loss: 0.693427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007041] [C loss: 0.693427]\n",
      "7704 [D loss: 0.693430, acc.: 0.00%] [G loss: 0.005678] [C loss: 0.693426]\n",
      "7705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005940] [C loss: 0.693426]\n",
      "7706 [D loss: 0.693429, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.693426]\n",
      "7707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004369] [C loss: 0.693426]\n",
      "7708 [D loss: 0.693429, acc.: 0.00%] [G loss: 0.006481] [C loss: 0.693425]\n",
      "7709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004426] [C loss: 0.693425]\n",
      "7710 [D loss: 0.693428, acc.: 0.00%] [G loss: 0.004358] [C loss: 0.693424]\n",
      "7711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004957] [C loss: 0.693424]\n",
      "7712 [D loss: 0.693427, acc.: 0.00%] [G loss: 0.004937] [C loss: 0.693424]\n",
      "7713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004480] [C loss: 0.693424]\n",
      "7714 [D loss: 0.693426, acc.: 0.00%] [G loss: 0.007208] [C loss: 0.693423]\n",
      "7715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004664] [C loss: 0.693423]\n",
      "7716 [D loss: 0.693426, acc.: 0.00%] [G loss: 0.005906] [C loss: 0.693423]\n",
      "7717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006080] [C loss: 0.693423]\n",
      "7718 [D loss: 0.693426, acc.: 0.00%] [G loss: 0.003708] [C loss: 0.693422]\n",
      "7719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003829] [C loss: 0.693422]\n",
      "7720 [D loss: 0.693425, acc.: 0.00%] [G loss: 0.017220] [C loss: 0.693422]\n",
      "7721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003894] [C loss: 0.693422]\n",
      "7722 [D loss: 0.693424, acc.: 0.00%] [G loss: 0.010169] [C loss: 0.693420]\n",
      "7723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006435] [C loss: 0.693420]\n",
      "7724 [D loss: 0.693423, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.693420]\n",
      "7725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005159] [C loss: 0.693420]\n",
      "7726 [D loss: 0.693422, acc.: 0.00%] [G loss: 0.005442] [C loss: 0.693420]\n",
      "7727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004918] [C loss: 0.693420]\n",
      "7728 [D loss: 0.693423, acc.: 0.00%] [G loss: 0.005056] [C loss: 0.693419]\n",
      "7729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004873] [C loss: 0.693419]\n",
      "7730 [D loss: 0.693422, acc.: 0.00%] [G loss: 0.006905] [C loss: 0.693418]\n",
      "7731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004049] [C loss: 0.693418]\n",
      "7732 [D loss: 0.693421, acc.: 0.00%] [G loss: 0.005738] [C loss: 0.693417]\n",
      "7733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004347] [C loss: 0.693417]\n",
      "7734 [D loss: 0.693420, acc.: 0.00%] [G loss: 0.004232] [C loss: 0.693417]\n",
      "7735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003807] [C loss: 0.693417]\n",
      "7736 [D loss: 0.693420, acc.: 0.00%] [G loss: 0.003545] [C loss: 0.693416]\n",
      "7737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003695] [C loss: 0.693416]\n",
      "7738 [D loss: 0.693419, acc.: 0.00%] [G loss: 0.006121] [C loss: 0.693416]\n",
      "7739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004322] [C loss: 0.693416]\n",
      "7740 [D loss: 0.693419, acc.: 0.00%] [G loss: 0.015726] [C loss: 0.693415]\n",
      "7741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004108] [C loss: 0.693415]\n",
      "7742 [D loss: 0.693418, acc.: 0.00%] [G loss: 0.005012] [C loss: 0.693415]\n",
      "7743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004343] [C loss: 0.693415]\n",
      "7744 [D loss: 0.693418, acc.: 0.00%] [G loss: 0.004276] [C loss: 0.693413]\n",
      "7745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009988] [C loss: 0.693413]\n",
      "7746 [D loss: 0.693417, acc.: 0.00%] [G loss: 0.006726] [C loss: 0.693414]\n",
      "7747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006789] [C loss: 0.693414]\n",
      "7748 [D loss: 0.693417, acc.: 0.00%] [G loss: 0.007014] [C loss: 0.693413]\n",
      "7749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004494] [C loss: 0.693413]\n",
      "7750 [D loss: 0.693416, acc.: 0.00%] [G loss: 0.006598] [C loss: 0.693413]\n",
      "7751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005319] [C loss: 0.693413]\n",
      "7752 [D loss: 0.693416, acc.: 0.00%] [G loss: 0.005771] [C loss: 0.693411]\n",
      "7753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.693411]\n",
      "7754 [D loss: 0.693414, acc.: 0.00%] [G loss: 0.005897] [C loss: 0.693410]\n",
      "7755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005327] [C loss: 0.693410]\n",
      "7756 [D loss: 0.693414, acc.: 0.00%] [G loss: 0.004254] [C loss: 0.693411]\n",
      "7757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007078] [C loss: 0.693411]\n",
      "7758 [D loss: 0.693413, acc.: 0.00%] [G loss: 0.003817] [C loss: 0.693410]\n",
      "7759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006639] [C loss: 0.693410]\n",
      "7760 [D loss: 0.693413, acc.: 0.00%] [G loss: 0.004902] [C loss: 0.693410]\n",
      "7761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006546] [C loss: 0.693410]\n",
      "7762 [D loss: 0.693413, acc.: 0.00%] [G loss: 0.011548] [C loss: 0.693409]\n",
      "7763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004407] [C loss: 0.693409]\n",
      "7764 [D loss: 0.693412, acc.: 0.00%] [G loss: 0.008404] [C loss: 0.693408]\n",
      "7765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004669] [C loss: 0.693408]\n",
      "7766 [D loss: 0.693411, acc.: 0.00%] [G loss: 0.004109] [C loss: 0.693408]\n",
      "7767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005149] [C loss: 0.693408]\n",
      "7768 [D loss: 0.693411, acc.: 0.00%] [G loss: 0.005561] [C loss: 0.693407]\n",
      "7769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004115] [C loss: 0.693407]\n",
      "7770 [D loss: 0.693410, acc.: 0.00%] [G loss: 0.005264] [C loss: 0.693407]\n",
      "7771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005827] [C loss: 0.693407]\n",
      "7772 [D loss: 0.693410, acc.: 0.00%] [G loss: 0.010267] [C loss: 0.693405]\n",
      "7773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008404] [C loss: 0.693405]\n",
      "7774 [D loss: 0.693408, acc.: 0.00%] [G loss: 0.006759] [C loss: 0.693405]\n",
      "7775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005583] [C loss: 0.693405]\n",
      "7776 [D loss: 0.693408, acc.: 0.00%] [G loss: 0.003747] [C loss: 0.693404]\n",
      "7777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006477] [C loss: 0.693404]\n",
      "7778 [D loss: 0.693408, acc.: 0.00%] [G loss: 0.004208] [C loss: 0.693404]\n",
      "7779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005693] [C loss: 0.693404]\n",
      "7780 [D loss: 0.693408, acc.: 0.00%] [G loss: 0.004242] [C loss: 0.693403]\n",
      "7781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010338] [C loss: 0.693403]\n",
      "7782 [D loss: 0.693406, acc.: 0.00%] [G loss: 0.004405] [C loss: 0.693403]\n",
      "7783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004682] [C loss: 0.693403]\n",
      "7784 [D loss: 0.693406, acc.: 0.00%] [G loss: 0.004206] [C loss: 0.693402]\n",
      "7785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004862] [C loss: 0.693402]\n",
      "7786 [D loss: 0.693405, acc.: 0.00%] [G loss: 0.003995] [C loss: 0.693401]\n",
      "7787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004781] [C loss: 0.693401]\n",
      "7788 [D loss: 0.693405, acc.: 0.00%] [G loss: 0.006382] [C loss: 0.693401]\n",
      "7789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.693401]\n",
      "7790 [D loss: 0.693404, acc.: 0.00%] [G loss: 0.008767] [C loss: 0.693401]\n",
      "7791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003997] [C loss: 0.693401]\n",
      "7792 [D loss: 0.693404, acc.: 0.00%] [G loss: 0.007438] [C loss: 0.693400]\n",
      "7793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003799] [C loss: 0.693400]\n",
      "7794 [D loss: 0.693403, acc.: 0.00%] [G loss: 0.003726] [C loss: 0.693399]\n",
      "7795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003500] [C loss: 0.693399]\n",
      "7796 [D loss: 0.693403, acc.: 0.00%] [G loss: 0.005208] [C loss: 0.693399]\n",
      "7797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003687] [C loss: 0.693399]\n",
      "7798 [D loss: 0.693402, acc.: 0.00%] [G loss: 0.005883] [C loss: 0.693399]\n",
      "7799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004933] [C loss: 0.693399]\n",
      "7800 [D loss: 0.693402, acc.: 0.00%] [G loss: 0.005181] [C loss: 0.693398]\n",
      "7801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004445] [C loss: 0.693398]\n",
      "7802 [D loss: 0.693401, acc.: 0.00%] [G loss: 0.005206] [C loss: 0.693397]\n",
      "7803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004725] [C loss: 0.693397]\n",
      "7804 [D loss: 0.693400, acc.: 0.00%] [G loss: 0.005887] [C loss: 0.693396]\n",
      "7805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004091] [C loss: 0.693396]\n",
      "7806 [D loss: 0.693399, acc.: 0.00%] [G loss: 0.004068] [C loss: 0.693396]\n",
      "7807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016557] [C loss: 0.693396]\n",
      "7808 [D loss: 0.693399, acc.: 0.00%] [G loss: 0.005381] [C loss: 0.693395]\n",
      "7809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005177] [C loss: 0.693395]\n",
      "7810 [D loss: 0.693399, acc.: 0.00%] [G loss: 0.005805] [C loss: 0.693395]\n",
      "7811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005900] [C loss: 0.693395]\n",
      "7812 [D loss: 0.693398, acc.: 0.00%] [G loss: 0.003627] [C loss: 0.693394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005925] [C loss: 0.693394]\n",
      "7814 [D loss: 0.693397, acc.: 0.00%] [G loss: 0.006148] [C loss: 0.693393]\n",
      "7815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004914] [C loss: 0.693393]\n",
      "7816 [D loss: 0.693397, acc.: 0.00%] [G loss: 0.005589] [C loss: 0.693393]\n",
      "7817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005334] [C loss: 0.693393]\n",
      "7818 [D loss: 0.693396, acc.: 0.00%] [G loss: 0.004078] [C loss: 0.693392]\n",
      "7819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004423] [C loss: 0.693392]\n",
      "7820 [D loss: 0.693395, acc.: 0.00%] [G loss: 0.003585] [C loss: 0.693392]\n",
      "7821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004115] [C loss: 0.693392]\n",
      "7822 [D loss: 0.693395, acc.: 0.00%] [G loss: 0.004964] [C loss: 0.693390]\n",
      "7823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005092] [C loss: 0.693390]\n",
      "7824 [D loss: 0.693394, acc.: 0.00%] [G loss: 0.006560] [C loss: 0.693390]\n",
      "7825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008384] [C loss: 0.693390]\n",
      "7826 [D loss: 0.693394, acc.: 0.00%] [G loss: 0.004179] [C loss: 0.693388]\n",
      "7827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004625] [C loss: 0.693388]\n",
      "7828 [D loss: 0.693392, acc.: 0.00%] [G loss: 0.006173] [C loss: 0.693389]\n",
      "7829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003933] [C loss: 0.693389]\n",
      "7830 [D loss: 0.693393, acc.: 0.00%] [G loss: 0.004436] [C loss: 0.693388]\n",
      "7831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006229] [C loss: 0.693388]\n",
      "7832 [D loss: 0.693392, acc.: 0.00%] [G loss: 0.004742] [C loss: 0.693388]\n",
      "7833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004545] [C loss: 0.693388]\n",
      "7834 [D loss: 0.693391, acc.: 0.00%] [G loss: 0.005554] [C loss: 0.693388]\n",
      "7835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003002] [C loss: 0.693388]\n",
      "7836 [D loss: 0.693391, acc.: 0.00%] [G loss: 0.004764] [C loss: 0.693386]\n",
      "7837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006297] [C loss: 0.693386]\n",
      "7838 [D loss: 0.693390, acc.: 0.00%] [G loss: 0.004494] [C loss: 0.693386]\n",
      "7839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003946] [C loss: 0.693386]\n",
      "7840 [D loss: 0.693390, acc.: 0.00%] [G loss: 0.006971] [C loss: 0.693386]\n",
      "7841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006425] [C loss: 0.693386]\n",
      "7842 [D loss: 0.693389, acc.: 0.00%] [G loss: 0.005048] [C loss: 0.693385]\n",
      "7843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005081] [C loss: 0.693385]\n",
      "7844 [D loss: 0.693389, acc.: 0.00%] [G loss: 0.005852] [C loss: 0.693384]\n",
      "7845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006190] [C loss: 0.693384]\n",
      "7846 [D loss: 0.693388, acc.: 0.00%] [G loss: 0.006261] [C loss: 0.693384]\n",
      "7847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004576] [C loss: 0.693384]\n",
      "7848 [D loss: 0.693388, acc.: 0.00%] [G loss: 0.004068] [C loss: 0.693383]\n",
      "7849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004563] [C loss: 0.693383]\n",
      "7850 [D loss: 0.693387, acc.: 0.00%] [G loss: 0.004760] [C loss: 0.693383]\n",
      "7851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004910] [C loss: 0.693383]\n",
      "7852 [D loss: 0.693387, acc.: 0.00%] [G loss: 0.004661] [C loss: 0.693382]\n",
      "7853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004557] [C loss: 0.693382]\n",
      "7854 [D loss: 0.693386, acc.: 0.00%] [G loss: 0.005208] [C loss: 0.693382]\n",
      "7855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.693382]\n",
      "7856 [D loss: 0.693385, acc.: 0.00%] [G loss: 0.003935] [C loss: 0.693381]\n",
      "7857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003664] [C loss: 0.693381]\n",
      "7858 [D loss: 0.693385, acc.: 0.00%] [G loss: 0.011167] [C loss: 0.693381]\n",
      "7859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004446] [C loss: 0.693381]\n",
      "7860 [D loss: 0.693384, acc.: 0.00%] [G loss: 0.003713] [C loss: 0.693380]\n",
      "7861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006945] [C loss: 0.693380]\n",
      "7862 [D loss: 0.693384, acc.: 0.00%] [G loss: 0.008842] [C loss: 0.693380]\n",
      "7863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003757] [C loss: 0.693380]\n",
      "7864 [D loss: 0.693384, acc.: 0.00%] [G loss: 0.008708] [C loss: 0.693379]\n",
      "7865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005222] [C loss: 0.693379]\n",
      "7866 [D loss: 0.693384, acc.: 0.00%] [G loss: 0.004129] [C loss: 0.693379]\n",
      "7867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004379] [C loss: 0.693379]\n",
      "7868 [D loss: 0.693383, acc.: 0.00%] [G loss: 0.009366] [C loss: 0.693378]\n",
      "7869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004088] [C loss: 0.693378]\n",
      "7870 [D loss: 0.693382, acc.: 0.00%] [G loss: 0.004938] [C loss: 0.693377]\n",
      "7871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009335] [C loss: 0.693377]\n",
      "7872 [D loss: 0.693382, acc.: 0.00%] [G loss: 0.004660] [C loss: 0.693377]\n",
      "7873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008900] [C loss: 0.693377]\n",
      "7874 [D loss: 0.693381, acc.: 0.00%] [G loss: 0.005127] [C loss: 0.693377]\n",
      "7875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006307] [C loss: 0.693377]\n",
      "7876 [D loss: 0.693380, acc.: 0.00%] [G loss: 0.003678] [C loss: 0.693376]\n",
      "7877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004552] [C loss: 0.693376]\n",
      "7878 [D loss: 0.693380, acc.: 0.00%] [G loss: 0.007888] [C loss: 0.693376]\n",
      "7879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004056] [C loss: 0.693376]\n",
      "7880 [D loss: 0.693380, acc.: 0.00%] [G loss: 0.008477] [C loss: 0.693376]\n",
      "7881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004827] [C loss: 0.693376]\n",
      "7882 [D loss: 0.693379, acc.: 0.00%] [G loss: 0.009369] [C loss: 0.693374]\n",
      "7883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004542] [C loss: 0.693374]\n",
      "7884 [D loss: 0.693378, acc.: 0.00%] [G loss: 0.008243] [C loss: 0.693373]\n",
      "7885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005605] [C loss: 0.693373]\n",
      "7886 [D loss: 0.693377, acc.: 0.00%] [G loss: 0.005230] [C loss: 0.693373]\n",
      "7887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005315] [C loss: 0.693373]\n",
      "7888 [D loss: 0.693377, acc.: 0.00%] [G loss: 0.006928] [C loss: 0.693372]\n",
      "7889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005190] [C loss: 0.693372]\n",
      "7890 [D loss: 0.693377, acc.: 0.00%] [G loss: 0.005495] [C loss: 0.693372]\n",
      "7891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008639] [C loss: 0.693372]\n",
      "7892 [D loss: 0.693376, acc.: 0.00%] [G loss: 0.008987] [C loss: 0.693372]\n",
      "7893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018447] [C loss: 0.693372]\n",
      "7894 [D loss: 0.693375, acc.: 0.00%] [G loss: 0.006691] [C loss: 0.693370]\n",
      "7895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007976] [C loss: 0.693370]\n",
      "7896 [D loss: 0.693375, acc.: 0.00%] [G loss: 0.006050] [C loss: 0.693370]\n",
      "7897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003704] [C loss: 0.693370]\n",
      "7898 [D loss: 0.693374, acc.: 0.00%] [G loss: 0.005195] [C loss: 0.693370]\n",
      "7899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007217] [C loss: 0.693370]\n",
      "7900 [D loss: 0.693374, acc.: 0.00%] [G loss: 0.005818] [C loss: 0.693370]\n",
      "7901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006227] [C loss: 0.693370]\n",
      "7902 [D loss: 0.693374, acc.: 0.00%] [G loss: 0.004509] [C loss: 0.693368]\n",
      "7903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004390] [C loss: 0.693368]\n",
      "7904 [D loss: 0.693373, acc.: 0.00%] [G loss: 0.006496] [C loss: 0.693368]\n",
      "7905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004682] [C loss: 0.693368]\n",
      "7906 [D loss: 0.693372, acc.: 0.00%] [G loss: 0.005695] [C loss: 0.693368]\n",
      "7907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005348] [C loss: 0.693368]\n",
      "7908 [D loss: 0.693372, acc.: 0.00%] [G loss: 0.006115] [C loss: 0.693366]\n",
      "7909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004744] [C loss: 0.693366]\n",
      "7910 [D loss: 0.693371, acc.: 0.00%] [G loss: 0.004286] [C loss: 0.693367]\n",
      "7911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009376] [C loss: 0.693367]\n",
      "7912 [D loss: 0.693371, acc.: 0.00%] [G loss: 0.004927] [C loss: 0.693366]\n",
      "7913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004169] [C loss: 0.693366]\n",
      "7914 [D loss: 0.693371, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.693365]\n",
      "7915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009853] [C loss: 0.693365]\n",
      "7916 [D loss: 0.693370, acc.: 0.00%] [G loss: 0.007019] [C loss: 0.693365]\n",
      "7917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004795] [C loss: 0.693365]\n",
      "7918 [D loss: 0.693369, acc.: 0.00%] [G loss: 0.005324] [C loss: 0.693364]\n",
      "7919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005485] [C loss: 0.693364]\n",
      "7920 [D loss: 0.693369, acc.: 0.00%] [G loss: 0.006171] [C loss: 0.693365]\n",
      "7921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008834] [C loss: 0.693365]\n",
      "7922 [D loss: 0.693369, acc.: 0.00%] [G loss: 0.006238] [C loss: 0.693364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005523] [C loss: 0.693364]\n",
      "7924 [D loss: 0.693368, acc.: 0.00%] [G loss: 0.005804] [C loss: 0.693363]\n",
      "7925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004872] [C loss: 0.693363]\n",
      "7926 [D loss: 0.693367, acc.: 0.00%] [G loss: 0.004900] [C loss: 0.693363]\n",
      "7927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007275] [C loss: 0.693363]\n",
      "7928 [D loss: 0.693367, acc.: 0.00%] [G loss: 0.003731] [C loss: 0.693361]\n",
      "7929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005520] [C loss: 0.693361]\n",
      "7930 [D loss: 0.693366, acc.: 0.00%] [G loss: 0.003808] [C loss: 0.693361]\n",
      "7931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008946] [C loss: 0.693361]\n",
      "7932 [D loss: 0.693365, acc.: 0.00%] [G loss: 0.004270] [C loss: 0.693361]\n",
      "7933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005396] [C loss: 0.693361]\n",
      "7934 [D loss: 0.693365, acc.: 0.00%] [G loss: 0.005705] [C loss: 0.693359]\n",
      "7935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004428] [C loss: 0.693359]\n",
      "7936 [D loss: 0.693364, acc.: 0.00%] [G loss: 0.003666] [C loss: 0.693359]\n",
      "7937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004425] [C loss: 0.693359]\n",
      "7938 [D loss: 0.693364, acc.: 0.00%] [G loss: 0.004202] [C loss: 0.693359]\n",
      "7939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006174] [C loss: 0.693359]\n",
      "7940 [D loss: 0.693364, acc.: 0.00%] [G loss: 0.005161] [C loss: 0.693358]\n",
      "7941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005287] [C loss: 0.693358]\n",
      "7942 [D loss: 0.693363, acc.: 0.00%] [G loss: 0.004442] [C loss: 0.693358]\n",
      "7943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004802] [C loss: 0.693358]\n",
      "7944 [D loss: 0.693363, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.693357]\n",
      "7945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004383] [C loss: 0.693357]\n",
      "7946 [D loss: 0.693362, acc.: 0.00%] [G loss: 0.006594] [C loss: 0.693356]\n",
      "7947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006811] [C loss: 0.693356]\n",
      "7948 [D loss: 0.693361, acc.: 0.00%] [G loss: 0.005987] [C loss: 0.693356]\n",
      "7949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021759] [C loss: 0.693356]\n",
      "7950 [D loss: 0.693361, acc.: 0.00%] [G loss: 0.004785] [C loss: 0.693355]\n",
      "7951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005281] [C loss: 0.693355]\n",
      "7952 [D loss: 0.693360, acc.: 0.00%] [G loss: 0.007291] [C loss: 0.693355]\n",
      "7953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008278] [C loss: 0.693355]\n",
      "7954 [D loss: 0.693360, acc.: 0.00%] [G loss: 0.005922] [C loss: 0.693355]\n",
      "7955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005815] [C loss: 0.693355]\n",
      "7956 [D loss: 0.693359, acc.: 0.00%] [G loss: 0.006326] [C loss: 0.693356]\n",
      "7957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005468] [C loss: 0.693356]\n",
      "7958 [D loss: 0.693360, acc.: 0.00%] [G loss: 0.004841] [C loss: 0.693354]\n",
      "7959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004637] [C loss: 0.693354]\n",
      "7960 [D loss: 0.693359, acc.: 0.00%] [G loss: 0.004441] [C loss: 0.693353]\n",
      "7961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006940] [C loss: 0.693353]\n",
      "7962 [D loss: 0.693358, acc.: 0.00%] [G loss: 0.004546] [C loss: 0.693353]\n",
      "7963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004269] [C loss: 0.693353]\n",
      "7964 [D loss: 0.693357, acc.: 0.00%] [G loss: 0.005397] [C loss: 0.693352]\n",
      "7965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006169] [C loss: 0.693352]\n",
      "7966 [D loss: 0.693357, acc.: 0.00%] [G loss: 0.005254] [C loss: 0.693351]\n",
      "7967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005824] [C loss: 0.693351]\n",
      "7968 [D loss: 0.693356, acc.: 0.00%] [G loss: 0.006745] [C loss: 0.693351]\n",
      "7969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011371] [C loss: 0.693351]\n",
      "7970 [D loss: 0.693357, acc.: 0.00%] [G loss: 0.004891] [C loss: 0.693351]\n",
      "7971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005309] [C loss: 0.693351]\n",
      "7972 [D loss: 0.693356, acc.: 0.00%] [G loss: 0.006684] [C loss: 0.693351]\n",
      "7973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006398] [C loss: 0.693351]\n",
      "7974 [D loss: 0.693356, acc.: 0.00%] [G loss: 0.004809] [C loss: 0.693349]\n",
      "7975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004786] [C loss: 0.693349]\n",
      "7976 [D loss: 0.693354, acc.: 0.00%] [G loss: 0.013927] [C loss: 0.693350]\n",
      "7977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007633] [C loss: 0.693350]\n",
      "7978 [D loss: 0.693355, acc.: 0.00%] [G loss: 0.004539] [C loss: 0.693349]\n",
      "7979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006258] [C loss: 0.693349]\n",
      "7980 [D loss: 0.693354, acc.: 0.00%] [G loss: 0.004406] [C loss: 0.693348]\n",
      "7981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005665] [C loss: 0.693348]\n",
      "7982 [D loss: 0.693353, acc.: 0.00%] [G loss: 0.003950] [C loss: 0.693348]\n",
      "7983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004220] [C loss: 0.693348]\n",
      "7984 [D loss: 0.693353, acc.: 0.00%] [G loss: 0.007540] [C loss: 0.693348]\n",
      "7985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005859] [C loss: 0.693348]\n",
      "7986 [D loss: 0.693352, acc.: 0.00%] [G loss: 0.004385] [C loss: 0.693346]\n",
      "7987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004591] [C loss: 0.693346]\n",
      "7988 [D loss: 0.693352, acc.: 0.00%] [G loss: 0.005859] [C loss: 0.693345]\n",
      "7989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004215] [C loss: 0.693345]\n",
      "7990 [D loss: 0.693351, acc.: 0.00%] [G loss: 0.003378] [C loss: 0.693345]\n",
      "7991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007509] [C loss: 0.693345]\n",
      "7992 [D loss: 0.693350, acc.: 0.00%] [G loss: 0.006789] [C loss: 0.693345]\n",
      "7993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003648] [C loss: 0.693345]\n",
      "7994 [D loss: 0.693350, acc.: 0.00%] [G loss: 0.005550] [C loss: 0.693344]\n",
      "7995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009925] [C loss: 0.693344]\n",
      "7996 [D loss: 0.693350, acc.: 0.00%] [G loss: 0.004893] [C loss: 0.693344]\n",
      "7997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003956] [C loss: 0.693344]\n",
      "7998 [D loss: 0.693350, acc.: 0.00%] [G loss: 0.004459] [C loss: 0.693344]\n",
      "7999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004045] [C loss: 0.693344]\n",
      "8000 [D loss: 0.693350, acc.: 0.00%] [G loss: 0.013947] [C loss: 0.693343]\n",
      "8001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005800] [C loss: 0.693343]\n",
      "8002 [D loss: 0.693349, acc.: 0.00%] [G loss: 0.005252] [C loss: 0.693342]\n",
      "8003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004056] [C loss: 0.693342]\n",
      "8004 [D loss: 0.693348, acc.: 0.00%] [G loss: 0.008720] [C loss: 0.693342]\n",
      "8005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006468] [C loss: 0.693342]\n",
      "8006 [D loss: 0.693348, acc.: 0.00%] [G loss: 0.005511] [C loss: 0.693341]\n",
      "8007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004233] [C loss: 0.693341]\n",
      "8008 [D loss: 0.693347, acc.: 0.00%] [G loss: 0.007541] [C loss: 0.693341]\n",
      "8009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004482] [C loss: 0.693341]\n",
      "8010 [D loss: 0.693347, acc.: 0.00%] [G loss: 0.003703] [C loss: 0.693341]\n",
      "8011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022787] [C loss: 0.693341]\n",
      "8012 [D loss: 0.693346, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.693340]\n",
      "8013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003072] [C loss: 0.693340]\n",
      "8014 [D loss: 0.693346, acc.: 0.00%] [G loss: 0.004803] [C loss: 0.693339]\n",
      "8015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003504] [C loss: 0.693339]\n",
      "8016 [D loss: 0.693345, acc.: 0.00%] [G loss: 0.004234] [C loss: 0.693339]\n",
      "8017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004495] [C loss: 0.693339]\n",
      "8018 [D loss: 0.693344, acc.: 0.00%] [G loss: 0.004726] [C loss: 0.693338]\n",
      "8019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011757] [C loss: 0.693338]\n",
      "8020 [D loss: 0.693344, acc.: 0.00%] [G loss: 0.004719] [C loss: 0.693338]\n",
      "8021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006179] [C loss: 0.693338]\n",
      "8022 [D loss: 0.693344, acc.: 0.00%] [G loss: 0.004132] [C loss: 0.693338]\n",
      "8023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005784] [C loss: 0.693338]\n",
      "8024 [D loss: 0.693344, acc.: 0.00%] [G loss: 0.006469] [C loss: 0.693338]\n",
      "8025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004229] [C loss: 0.693338]\n",
      "8026 [D loss: 0.693343, acc.: 0.00%] [G loss: 0.004815] [C loss: 0.693337]\n",
      "8027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005001] [C loss: 0.693337]\n",
      "8028 [D loss: 0.693342, acc.: 0.00%] [G loss: 0.004896] [C loss: 0.693336]\n",
      "8029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006336] [C loss: 0.693336]\n",
      "8030 [D loss: 0.693342, acc.: 0.00%] [G loss: 0.006830] [C loss: 0.693336]\n",
      "8031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008461] [C loss: 0.693336]\n",
      "8032 [D loss: 0.693341, acc.: 0.00%] [G loss: 0.005475] [C loss: 0.693334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005774] [C loss: 0.693334]\n",
      "8034 [D loss: 0.693340, acc.: 0.00%] [G loss: 0.013122] [C loss: 0.693335]\n",
      "8035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004849] [C loss: 0.693335]\n",
      "8036 [D loss: 0.693341, acc.: 0.00%] [G loss: 0.005088] [C loss: 0.693334]\n",
      "8037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005976] [C loss: 0.693334]\n",
      "8038 [D loss: 0.693340, acc.: 0.00%] [G loss: 0.004852] [C loss: 0.693333]\n",
      "8039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007701] [C loss: 0.693333]\n",
      "8040 [D loss: 0.693339, acc.: 0.00%] [G loss: 0.004935] [C loss: 0.693333]\n",
      "8041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004672] [C loss: 0.693333]\n",
      "8042 [D loss: 0.693339, acc.: 0.00%] [G loss: 0.005980] [C loss: 0.693333]\n",
      "8043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004987] [C loss: 0.693333]\n",
      "8044 [D loss: 0.693339, acc.: 0.00%] [G loss: 0.004489] [C loss: 0.693332]\n",
      "8045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004549] [C loss: 0.693332]\n",
      "8046 [D loss: 0.693338, acc.: 0.00%] [G loss: 0.005645] [C loss: 0.693332]\n",
      "8047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010849] [C loss: 0.693332]\n",
      "8048 [D loss: 0.693338, acc.: 0.00%] [G loss: 0.007773] [C loss: 0.693331]\n",
      "8049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004782] [C loss: 0.693331]\n",
      "8050 [D loss: 0.693337, acc.: 0.00%] [G loss: 0.003515] [C loss: 0.693331]\n",
      "8051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008337] [C loss: 0.693331]\n",
      "8052 [D loss: 0.693337, acc.: 0.00%] [G loss: 0.005704] [C loss: 0.693331]\n",
      "8053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005736] [C loss: 0.693331]\n",
      "8054 [D loss: 0.693337, acc.: 0.00%] [G loss: 0.004413] [C loss: 0.693330]\n",
      "8055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004703] [C loss: 0.693330]\n",
      "8056 [D loss: 0.693336, acc.: 0.00%] [G loss: 0.003971] [C loss: 0.693330]\n",
      "8057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005844] [C loss: 0.693330]\n",
      "8058 [D loss: 0.693336, acc.: 0.00%] [G loss: 0.004837] [C loss: 0.693329]\n",
      "8059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018433] [C loss: 0.693329]\n",
      "8060 [D loss: 0.693335, acc.: 0.00%] [G loss: 0.003283] [C loss: 0.693329]\n",
      "8061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008791] [C loss: 0.693329]\n",
      "8062 [D loss: 0.693335, acc.: 0.00%] [G loss: 0.005322] [C loss: 0.693328]\n",
      "8063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004739] [C loss: 0.693328]\n",
      "8064 [D loss: 0.693335, acc.: 0.00%] [G loss: 0.005035] [C loss: 0.693328]\n",
      "8065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004110] [C loss: 0.693328]\n",
      "8066 [D loss: 0.693334, acc.: 0.00%] [G loss: 0.005650] [C loss: 0.693327]\n",
      "8067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007188] [C loss: 0.693327]\n",
      "8068 [D loss: 0.693334, acc.: 0.00%] [G loss: 0.005223] [C loss: 0.693327]\n",
      "8069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005781] [C loss: 0.693327]\n",
      "8070 [D loss: 0.693333, acc.: 0.00%] [G loss: 0.003996] [C loss: 0.693327]\n",
      "8071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.693327]\n",
      "8072 [D loss: 0.693333, acc.: 0.00%] [G loss: 0.005282] [C loss: 0.693327]\n",
      "8073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005182] [C loss: 0.693327]\n",
      "8074 [D loss: 0.693333, acc.: 0.00%] [G loss: 0.004486] [C loss: 0.693326]\n",
      "8075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003604] [C loss: 0.693326]\n",
      "8076 [D loss: 0.693332, acc.: 0.00%] [G loss: 0.004863] [C loss: 0.693325]\n",
      "8077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004374] [C loss: 0.693325]\n",
      "8078 [D loss: 0.693331, acc.: 0.00%] [G loss: 0.006907] [C loss: 0.693325]\n",
      "8079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004508] [C loss: 0.693325]\n",
      "8080 [D loss: 0.693331, acc.: 0.00%] [G loss: 0.004670] [C loss: 0.693324]\n",
      "8081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004152] [C loss: 0.693324]\n",
      "8082 [D loss: 0.693331, acc.: 0.00%] [G loss: 0.005034] [C loss: 0.693323]\n",
      "8083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004157] [C loss: 0.693323]\n",
      "8084 [D loss: 0.693330, acc.: 0.00%] [G loss: 0.004827] [C loss: 0.693323]\n",
      "8085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003483] [C loss: 0.693323]\n",
      "8086 [D loss: 0.693330, acc.: 0.00%] [G loss: 0.005999] [C loss: 0.693321]\n",
      "8087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004976] [C loss: 0.693321]\n",
      "8088 [D loss: 0.693329, acc.: 0.00%] [G loss: 0.005414] [C loss: 0.693322]\n",
      "8089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004664] [C loss: 0.693322]\n",
      "8090 [D loss: 0.693328, acc.: 0.00%] [G loss: 0.002835] [C loss: 0.693322]\n",
      "8091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004645] [C loss: 0.693322]\n",
      "8092 [D loss: 0.693329, acc.: 0.00%] [G loss: 0.004829] [C loss: 0.693321]\n",
      "8093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006187] [C loss: 0.693321]\n",
      "8094 [D loss: 0.693327, acc.: 0.00%] [G loss: 0.008646] [C loss: 0.693321]\n",
      "8095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005514] [C loss: 0.693321]\n",
      "8096 [D loss: 0.693328, acc.: 0.00%] [G loss: 0.005992] [C loss: 0.693321]\n",
      "8097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.693321]\n",
      "8098 [D loss: 0.693327, acc.: 0.00%] [G loss: 0.007984] [C loss: 0.693320]\n",
      "8099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004842] [C loss: 0.693320]\n",
      "8100 [D loss: 0.693326, acc.: 0.00%] [G loss: 0.006201] [C loss: 0.693320]\n",
      "8101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010025] [C loss: 0.693320]\n",
      "8102 [D loss: 0.693326, acc.: 0.00%] [G loss: 0.004167] [C loss: 0.693319]\n",
      "8103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019300] [C loss: 0.693319]\n",
      "8104 [D loss: 0.693325, acc.: 0.00%] [G loss: 0.003845] [C loss: 0.693318]\n",
      "8105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005215] [C loss: 0.693318]\n",
      "8106 [D loss: 0.693325, acc.: 0.00%] [G loss: 0.005285] [C loss: 0.693318]\n",
      "8107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004537] [C loss: 0.693318]\n",
      "8108 [D loss: 0.693325, acc.: 0.00%] [G loss: 0.005594] [C loss: 0.693318]\n",
      "8109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005912] [C loss: 0.693318]\n",
      "8110 [D loss: 0.693324, acc.: 0.00%] [G loss: 0.006557] [C loss: 0.693317]\n",
      "8111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005091] [C loss: 0.693317]\n",
      "8112 [D loss: 0.693324, acc.: 0.00%] [G loss: 0.007334] [C loss: 0.693316]\n",
      "8113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006428] [C loss: 0.693316]\n",
      "8114 [D loss: 0.693323, acc.: 0.00%] [G loss: 0.005287] [C loss: 0.693316]\n",
      "8115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005733] [C loss: 0.693316]\n",
      "8116 [D loss: 0.693323, acc.: 0.00%] [G loss: 0.006273] [C loss: 0.693315]\n",
      "8117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005903] [C loss: 0.693315]\n",
      "8118 [D loss: 0.693322, acc.: 0.00%] [G loss: 0.005047] [C loss: 0.693316]\n",
      "8119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005691] [C loss: 0.693316]\n",
      "8120 [D loss: 0.693322, acc.: 0.00%] [G loss: 0.005307] [C loss: 0.693315]\n",
      "8121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005389] [C loss: 0.693315]\n",
      "8122 [D loss: 0.693321, acc.: 0.00%] [G loss: 0.006701] [C loss: 0.693314]\n",
      "8123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004872] [C loss: 0.693314]\n",
      "8124 [D loss: 0.693321, acc.: 0.00%] [G loss: 0.006225] [C loss: 0.693314]\n",
      "8125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004427] [C loss: 0.693314]\n",
      "8126 [D loss: 0.693321, acc.: 0.00%] [G loss: 0.004078] [C loss: 0.693314]\n",
      "8127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004983] [C loss: 0.693314]\n",
      "8128 [D loss: 0.693320, acc.: 0.00%] [G loss: 0.006166] [C loss: 0.693313]\n",
      "8129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005216] [C loss: 0.693313]\n",
      "8130 [D loss: 0.693320, acc.: 0.00%] [G loss: 0.004161] [C loss: 0.693312]\n",
      "8131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004868] [C loss: 0.693312]\n",
      "8132 [D loss: 0.693319, acc.: 0.00%] [G loss: 0.008789] [C loss: 0.693312]\n",
      "8133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004906] [C loss: 0.693312]\n",
      "8134 [D loss: 0.693320, acc.: 0.00%] [G loss: 0.003271] [C loss: 0.693312]\n",
      "8135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004684] [C loss: 0.693312]\n",
      "8136 [D loss: 0.693319, acc.: 0.00%] [G loss: 0.007503] [C loss: 0.693311]\n",
      "8137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006901] [C loss: 0.693311]\n",
      "8138 [D loss: 0.693318, acc.: 0.00%] [G loss: 0.006625] [C loss: 0.693311]\n",
      "8139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003768] [C loss: 0.693311]\n",
      "8140 [D loss: 0.693318, acc.: 0.00%] [G loss: 0.006115] [C loss: 0.693310]\n",
      "8141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003784] [C loss: 0.693310]\n",
      "8142 [D loss: 0.693317, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.693310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003769] [C loss: 0.693310]\n",
      "8144 [D loss: 0.693317, acc.: 0.00%] [G loss: 0.006311] [C loss: 0.693310]\n",
      "8145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004287] [C loss: 0.693310]\n",
      "8146 [D loss: 0.693317, acc.: 0.00%] [G loss: 0.006601] [C loss: 0.693309]\n",
      "8147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004812] [C loss: 0.693309]\n",
      "8148 [D loss: 0.693316, acc.: 0.00%] [G loss: 0.004119] [C loss: 0.693308]\n",
      "8149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003771] [C loss: 0.693308]\n",
      "8150 [D loss: 0.693316, acc.: 0.00%] [G loss: 0.004063] [C loss: 0.693308]\n",
      "8151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005919] [C loss: 0.693308]\n",
      "8152 [D loss: 0.693315, acc.: 0.00%] [G loss: 0.005351] [C loss: 0.693308]\n",
      "8153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005264] [C loss: 0.693308]\n",
      "8154 [D loss: 0.693315, acc.: 0.00%] [G loss: 0.004661] [C loss: 0.693307]\n",
      "8155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006108] [C loss: 0.693307]\n",
      "8156 [D loss: 0.693315, acc.: 0.00%] [G loss: 0.005685] [C loss: 0.693307]\n",
      "8157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005276] [C loss: 0.693307]\n",
      "8158 [D loss: 0.693315, acc.: 0.00%] [G loss: 0.006115] [C loss: 0.693306]\n",
      "8159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006086] [C loss: 0.693306]\n",
      "8160 [D loss: 0.693313, acc.: 0.00%] [G loss: 0.015740] [C loss: 0.693307]\n",
      "8161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006025] [C loss: 0.693307]\n",
      "8162 [D loss: 0.693314, acc.: 0.00%] [G loss: 0.005862] [C loss: 0.693306]\n",
      "8163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005456] [C loss: 0.693306]\n",
      "8164 [D loss: 0.693313, acc.: 0.00%] [G loss: 0.004005] [C loss: 0.693306]\n",
      "8165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005135] [C loss: 0.693306]\n",
      "8166 [D loss: 0.693313, acc.: 0.00%] [G loss: 0.003767] [C loss: 0.693305]\n",
      "8167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005635] [C loss: 0.693305]\n",
      "8168 [D loss: 0.693313, acc.: 0.00%] [G loss: 0.004834] [C loss: 0.693304]\n",
      "8169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003437] [C loss: 0.693304]\n",
      "8170 [D loss: 0.693312, acc.: 0.00%] [G loss: 0.012578] [C loss: 0.693304]\n",
      "8171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007287] [C loss: 0.693304]\n",
      "8172 [D loss: 0.693312, acc.: 0.00%] [G loss: 0.002933] [C loss: 0.693304]\n",
      "8173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005382] [C loss: 0.693304]\n",
      "8174 [D loss: 0.693311, acc.: 0.00%] [G loss: 0.004591] [C loss: 0.693303]\n",
      "8175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004111] [C loss: 0.693303]\n",
      "8176 [D loss: 0.693310, acc.: 0.00%] [G loss: 0.004178] [C loss: 0.693303]\n",
      "8177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011314] [C loss: 0.693303]\n",
      "8178 [D loss: 0.693311, acc.: 0.00%] [G loss: 0.005654] [C loss: 0.693302]\n",
      "8179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.693302]\n",
      "8180 [D loss: 0.693310, acc.: 0.00%] [G loss: 0.003300] [C loss: 0.693301]\n",
      "8181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009059] [C loss: 0.693301]\n",
      "8182 [D loss: 0.693309, acc.: 0.00%] [G loss: 0.004999] [C loss: 0.693301]\n",
      "8183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.013613] [C loss: 0.693301]\n",
      "8184 [D loss: 0.693309, acc.: 0.00%] [G loss: 0.004898] [C loss: 0.693301]\n",
      "8185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008060] [C loss: 0.693301]\n",
      "8186 [D loss: 0.693309, acc.: 0.00%] [G loss: 0.004335] [C loss: 0.693300]\n",
      "8187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005110] [C loss: 0.693300]\n",
      "8188 [D loss: 0.693308, acc.: 0.00%] [G loss: 0.003581] [C loss: 0.693299]\n",
      "8189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005178] [C loss: 0.693299]\n",
      "8190 [D loss: 0.693308, acc.: 0.00%] [G loss: 0.004638] [C loss: 0.693300]\n",
      "8191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020190] [C loss: 0.693300]\n",
      "8192 [D loss: 0.693308, acc.: 0.00%] [G loss: 0.004672] [C loss: 0.693299]\n",
      "8193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009093] [C loss: 0.693299]\n",
      "8194 [D loss: 0.693307, acc.: 0.00%] [G loss: 0.003941] [C loss: 0.693298]\n",
      "8195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008116] [C loss: 0.693298]\n",
      "8196 [D loss: 0.693307, acc.: 0.00%] [G loss: 0.010957] [C loss: 0.693298]\n",
      "8197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004516] [C loss: 0.693298]\n",
      "8198 [D loss: 0.693306, acc.: 0.00%] [G loss: 0.005851] [C loss: 0.693298]\n",
      "8199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005429] [C loss: 0.693298]\n",
      "8200 [D loss: 0.693306, acc.: 0.00%] [G loss: 0.004920] [C loss: 0.693297]\n",
      "8201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.693297]\n",
      "8202 [D loss: 0.693306, acc.: 0.00%] [G loss: 0.004912] [C loss: 0.693297]\n",
      "8203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008528] [C loss: 0.693297]\n",
      "8204 [D loss: 0.693305, acc.: 0.00%] [G loss: 0.021799] [C loss: 0.693296]\n",
      "8205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004401] [C loss: 0.693296]\n",
      "8206 [D loss: 0.693305, acc.: 0.00%] [G loss: 0.016350] [C loss: 0.693295]\n",
      "8207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006421] [C loss: 0.693295]\n",
      "8208 [D loss: 0.693304, acc.: 0.00%] [G loss: 0.004939] [C loss: 0.693296]\n",
      "8209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005738] [C loss: 0.693296]\n",
      "8210 [D loss: 0.693304, acc.: 0.00%] [G loss: 0.005599] [C loss: 0.693295]\n",
      "8211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018110] [C loss: 0.693295]\n",
      "8212 [D loss: 0.693304, acc.: 0.00%] [G loss: 0.005242] [C loss: 0.693294]\n",
      "8213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004996] [C loss: 0.693294]\n",
      "8214 [D loss: 0.693303, acc.: 0.00%] [G loss: 0.007026] [C loss: 0.693294]\n",
      "8215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005193] [C loss: 0.693294]\n",
      "8216 [D loss: 0.693303, acc.: 0.00%] [G loss: 0.009473] [C loss: 0.693294]\n",
      "8217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005671] [C loss: 0.693294]\n",
      "8218 [D loss: 0.693303, acc.: 0.00%] [G loss: 0.004140] [C loss: 0.693293]\n",
      "8219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005831] [C loss: 0.693293]\n",
      "8220 [D loss: 0.693302, acc.: 0.00%] [G loss: 0.004844] [C loss: 0.693293]\n",
      "8221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004991] [C loss: 0.693293]\n",
      "8222 [D loss: 0.693302, acc.: 0.00%] [G loss: 0.004529] [C loss: 0.693293]\n",
      "8223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010744] [C loss: 0.693293]\n",
      "8224 [D loss: 0.693302, acc.: 0.00%] [G loss: 0.005787] [C loss: 0.693293]\n",
      "8225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004516] [C loss: 0.693293]\n",
      "8226 [D loss: 0.693301, acc.: 0.00%] [G loss: 0.005350] [C loss: 0.693292]\n",
      "8227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004717] [C loss: 0.693292]\n",
      "8228 [D loss: 0.693301, acc.: 0.00%] [G loss: 0.005593] [C loss: 0.693292]\n",
      "8229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005719] [C loss: 0.693292]\n",
      "8230 [D loss: 0.693301, acc.: 0.00%] [G loss: 0.004828] [C loss: 0.693291]\n",
      "8231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006020] [C loss: 0.693291]\n",
      "8232 [D loss: 0.693300, acc.: 0.00%] [G loss: 0.008476] [C loss: 0.693291]\n",
      "8233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006587] [C loss: 0.693291]\n",
      "8234 [D loss: 0.693300, acc.: 0.00%] [G loss: 0.006319] [C loss: 0.693290]\n",
      "8235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004900] [C loss: 0.693290]\n",
      "8236 [D loss: 0.693299, acc.: 0.00%] [G loss: 0.013217] [C loss: 0.693290]\n",
      "8237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004076] [C loss: 0.693290]\n",
      "8238 [D loss: 0.693299, acc.: 0.00%] [G loss: 0.004841] [C loss: 0.693290]\n",
      "8239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006354] [C loss: 0.693290]\n",
      "8240 [D loss: 0.693299, acc.: 0.00%] [G loss: 0.006295] [C loss: 0.693289]\n",
      "8241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004521] [C loss: 0.693289]\n",
      "8242 [D loss: 0.693298, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.693289]\n",
      "8243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.693289]\n",
      "8244 [D loss: 0.693298, acc.: 0.00%] [G loss: 0.007547] [C loss: 0.693289]\n",
      "8245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004727] [C loss: 0.693289]\n",
      "8246 [D loss: 0.693298, acc.: 0.00%] [G loss: 0.004456] [C loss: 0.693288]\n",
      "8247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004921] [C loss: 0.693288]\n",
      "8248 [D loss: 0.693297, acc.: 0.00%] [G loss: 0.003533] [C loss: 0.693288]\n",
      "8249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.693288]\n",
      "8250 [D loss: 0.693297, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693287]\n",
      "8251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003307] [C loss: 0.693287]\n",
      "8252 [D loss: 0.693296, acc.: 0.00%] [G loss: 0.005097] [C loss: 0.693286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004031] [C loss: 0.693286]\n",
      "8254 [D loss: 0.693296, acc.: 0.00%] [G loss: 0.006117] [C loss: 0.693286]\n",
      "8255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008858] [C loss: 0.693286]\n",
      "8256 [D loss: 0.693295, acc.: 0.00%] [G loss: 0.005542] [C loss: 0.693286]\n",
      "8257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010716] [C loss: 0.693286]\n",
      "8258 [D loss: 0.693295, acc.: 0.00%] [G loss: 0.004503] [C loss: 0.693285]\n",
      "8259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004498] [C loss: 0.693285]\n",
      "8260 [D loss: 0.693295, acc.: 0.00%] [G loss: 0.008401] [C loss: 0.693284]\n",
      "8261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004892] [C loss: 0.693284]\n",
      "8262 [D loss: 0.693294, acc.: 0.00%] [G loss: 0.004321] [C loss: 0.693284]\n",
      "8263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004849] [C loss: 0.693284]\n",
      "8264 [D loss: 0.693294, acc.: 0.00%] [G loss: 0.008662] [C loss: 0.693284]\n",
      "8265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008078] [C loss: 0.693284]\n",
      "8266 [D loss: 0.693294, acc.: 0.00%] [G loss: 0.007117] [C loss: 0.693283]\n",
      "8267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005320] [C loss: 0.693283]\n",
      "8268 [D loss: 0.693293, acc.: 0.00%] [G loss: 0.006208] [C loss: 0.693283]\n",
      "8269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006237] [C loss: 0.693283]\n",
      "8270 [D loss: 0.693293, acc.: 0.00%] [G loss: 0.008948] [C loss: 0.693283]\n",
      "8271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005938] [C loss: 0.693283]\n",
      "8272 [D loss: 0.693293, acc.: 0.00%] [G loss: 0.004680] [C loss: 0.693283]\n",
      "8273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006568] [C loss: 0.693283]\n",
      "8274 [D loss: 0.693292, acc.: 0.00%] [G loss: 0.004810] [C loss: 0.693282]\n",
      "8275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014502] [C loss: 0.693282]\n",
      "8276 [D loss: 0.693292, acc.: 0.00%] [G loss: 0.008678] [C loss: 0.693281]\n",
      "8277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005342] [C loss: 0.693281]\n",
      "8278 [D loss: 0.693292, acc.: 0.00%] [G loss: 0.006260] [C loss: 0.693281]\n",
      "8279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004879] [C loss: 0.693281]\n",
      "8280 [D loss: 0.693291, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.693281]\n",
      "8281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006338] [C loss: 0.693281]\n",
      "8282 [D loss: 0.693292, acc.: 0.00%] [G loss: 0.007587] [C loss: 0.693281]\n",
      "8283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005205] [C loss: 0.693281]\n",
      "8284 [D loss: 0.693291, acc.: 0.00%] [G loss: 0.004100] [C loss: 0.693280]\n",
      "8285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003588] [C loss: 0.693280]\n",
      "8286 [D loss: 0.693290, acc.: 0.00%] [G loss: 0.005378] [C loss: 0.693280]\n",
      "8287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003692] [C loss: 0.693280]\n",
      "8288 [D loss: 0.693290, acc.: 0.00%] [G loss: 0.005380] [C loss: 0.693280]\n",
      "8289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006532] [C loss: 0.693280]\n",
      "8290 [D loss: 0.693289, acc.: 0.00%] [G loss: 0.005989] [C loss: 0.693279]\n",
      "8291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007286] [C loss: 0.693279]\n",
      "8292 [D loss: 0.693289, acc.: 0.00%] [G loss: 0.003859] [C loss: 0.693278]\n",
      "8293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004534] [C loss: 0.693278]\n",
      "8294 [D loss: 0.693289, acc.: 0.00%] [G loss: 0.003613] [C loss: 0.693278]\n",
      "8295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008562] [C loss: 0.693278]\n",
      "8296 [D loss: 0.693289, acc.: 0.00%] [G loss: 0.004803] [C loss: 0.693278]\n",
      "8297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003416] [C loss: 0.693278]\n",
      "8298 [D loss: 0.693288, acc.: 0.00%] [G loss: 0.004544] [C loss: 0.693278]\n",
      "8299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003959] [C loss: 0.693278]\n",
      "8300 [D loss: 0.693288, acc.: 0.00%] [G loss: 0.003842] [C loss: 0.693277]\n",
      "8301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004401] [C loss: 0.693277]\n",
      "8302 [D loss: 0.693287, acc.: 0.00%] [G loss: 0.004481] [C loss: 0.693276]\n",
      "8303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012094] [C loss: 0.693276]\n",
      "8304 [D loss: 0.693287, acc.: 0.00%] [G loss: 0.004718] [C loss: 0.693276]\n",
      "8305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004002] [C loss: 0.693276]\n",
      "8306 [D loss: 0.693286, acc.: 0.00%] [G loss: 0.005325] [C loss: 0.693276]\n",
      "8307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005614] [C loss: 0.693276]\n",
      "8308 [D loss: 0.693286, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.693275]\n",
      "8309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005619] [C loss: 0.693275]\n",
      "8310 [D loss: 0.693286, acc.: 0.00%] [G loss: 0.003338] [C loss: 0.693274]\n",
      "8311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005974] [C loss: 0.693274]\n",
      "8312 [D loss: 0.693285, acc.: 0.00%] [G loss: 0.004415] [C loss: 0.693275]\n",
      "8313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007479] [C loss: 0.693275]\n",
      "8314 [D loss: 0.693286, acc.: 0.00%] [G loss: 0.004712] [C loss: 0.693274]\n",
      "8315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005507] [C loss: 0.693274]\n",
      "8316 [D loss: 0.693285, acc.: 0.00%] [G loss: 0.005166] [C loss: 0.693274]\n",
      "8317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008046] [C loss: 0.693274]\n",
      "8318 [D loss: 0.693285, acc.: 0.00%] [G loss: 0.003816] [C loss: 0.693273]\n",
      "8319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007231] [C loss: 0.693273]\n",
      "8320 [D loss: 0.693284, acc.: 0.00%] [G loss: 0.005266] [C loss: 0.693273]\n",
      "8321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004830] [C loss: 0.693273]\n",
      "8322 [D loss: 0.693283, acc.: 0.00%] [G loss: 0.005547] [C loss: 0.693273]\n",
      "8323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006772] [C loss: 0.693273]\n",
      "8324 [D loss: 0.693284, acc.: 0.00%] [G loss: 0.004204] [C loss: 0.693272]\n",
      "8325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004293] [C loss: 0.693272]\n",
      "8326 [D loss: 0.693283, acc.: 0.00%] [G loss: 0.007150] [C loss: 0.693272]\n",
      "8327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005264] [C loss: 0.693272]\n",
      "8328 [D loss: 0.693283, acc.: 0.00%] [G loss: 0.015879] [C loss: 0.693271]\n",
      "8329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004866] [C loss: 0.693271]\n",
      "8330 [D loss: 0.693282, acc.: 0.00%] [G loss: 0.010079] [C loss: 0.693272]\n",
      "8331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004503] [C loss: 0.693272]\n",
      "8332 [D loss: 0.693282, acc.: 0.00%] [G loss: 0.006949] [C loss: 0.693272]\n",
      "8333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005166] [C loss: 0.693272]\n",
      "8334 [D loss: 0.693282, acc.: 0.00%] [G loss: 0.007764] [C loss: 0.693271]\n",
      "8335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006407] [C loss: 0.693271]\n",
      "8336 [D loss: 0.693282, acc.: 0.00%] [G loss: 0.006667] [C loss: 0.693271]\n",
      "8337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.693271]\n",
      "8338 [D loss: 0.693281, acc.: 0.00%] [G loss: 0.007014] [C loss: 0.693270]\n",
      "8339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004646] [C loss: 0.693270]\n",
      "8340 [D loss: 0.693281, acc.: 0.00%] [G loss: 0.005432] [C loss: 0.693270]\n",
      "8341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004743] [C loss: 0.693270]\n",
      "8342 [D loss: 0.693281, acc.: 0.00%] [G loss: 0.004329] [C loss: 0.693269]\n",
      "8343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005360] [C loss: 0.693269]\n",
      "8344 [D loss: 0.693280, acc.: 0.00%] [G loss: 0.003473] [C loss: 0.693268]\n",
      "8345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005406] [C loss: 0.693268]\n",
      "8346 [D loss: 0.693280, acc.: 0.00%] [G loss: 0.004601] [C loss: 0.693268]\n",
      "8347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007781] [C loss: 0.693268]\n",
      "8348 [D loss: 0.693279, acc.: 0.00%] [G loss: 0.005551] [C loss: 0.693268]\n",
      "8349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006112] [C loss: 0.693268]\n",
      "8350 [D loss: 0.693279, acc.: 0.00%] [G loss: 0.006867] [C loss: 0.693267]\n",
      "8351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004648] [C loss: 0.693267]\n",
      "8352 [D loss: 0.693278, acc.: 0.00%] [G loss: 0.008636] [C loss: 0.693267]\n",
      "8353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004654] [C loss: 0.693267]\n",
      "8354 [D loss: 0.693278, acc.: 0.00%] [G loss: 0.005896] [C loss: 0.693266]\n",
      "8355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010180] [C loss: 0.693266]\n",
      "8356 [D loss: 0.693277, acc.: 0.00%] [G loss: 0.005391] [C loss: 0.693266]\n",
      "8357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004478] [C loss: 0.693266]\n",
      "8358 [D loss: 0.693277, acc.: 0.00%] [G loss: 0.005126] [C loss: 0.693266]\n",
      "8359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.693266]\n",
      "8360 [D loss: 0.693277, acc.: 0.00%] [G loss: 0.004222] [C loss: 0.693265]\n",
      "8361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004365] [C loss: 0.693265]\n",
      "8362 [D loss: 0.693276, acc.: 0.00%] [G loss: 0.010070] [C loss: 0.693265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004143] [C loss: 0.693265]\n",
      "8364 [D loss: 0.693277, acc.: 0.00%] [G loss: 0.006208] [C loss: 0.693264]\n",
      "8365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005780] [C loss: 0.693264]\n",
      "8366 [D loss: 0.693276, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.693264]\n",
      "8367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004868] [C loss: 0.693264]\n",
      "8368 [D loss: 0.693276, acc.: 0.00%] [G loss: 0.005536] [C loss: 0.693264]\n",
      "8369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005160] [C loss: 0.693264]\n",
      "8370 [D loss: 0.693276, acc.: 0.00%] [G loss: 0.008474] [C loss: 0.693264]\n",
      "8371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006530] [C loss: 0.693264]\n",
      "8372 [D loss: 0.693276, acc.: 0.00%] [G loss: 0.004865] [C loss: 0.693263]\n",
      "8373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.693263]\n",
      "8374 [D loss: 0.693275, acc.: 0.00%] [G loss: 0.006509] [C loss: 0.693262]\n",
      "8375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004944] [C loss: 0.693262]\n",
      "8376 [D loss: 0.693275, acc.: 0.00%] [G loss: 0.007481] [C loss: 0.693262]\n",
      "8377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.693262]\n",
      "8378 [D loss: 0.693274, acc.: 0.00%] [G loss: 0.007663] [C loss: 0.693262]\n",
      "8379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004135] [C loss: 0.693262]\n",
      "8380 [D loss: 0.693274, acc.: 0.00%] [G loss: 0.010545] [C loss: 0.693262]\n",
      "8381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009001] [C loss: 0.693262]\n",
      "8382 [D loss: 0.693273, acc.: 0.00%] [G loss: 0.006077] [C loss: 0.693261]\n",
      "8383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007999] [C loss: 0.693261]\n",
      "8384 [D loss: 0.693273, acc.: 0.00%] [G loss: 0.005172] [C loss: 0.693261]\n",
      "8385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005947] [C loss: 0.693261]\n",
      "8386 [D loss: 0.693273, acc.: 0.00%] [G loss: 0.005675] [C loss: 0.693261]\n",
      "8387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006030] [C loss: 0.693261]\n",
      "8388 [D loss: 0.693273, acc.: 0.00%] [G loss: 0.005589] [C loss: 0.693260]\n",
      "8389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004455] [C loss: 0.693260]\n",
      "8390 [D loss: 0.693273, acc.: 0.00%] [G loss: 0.006391] [C loss: 0.693259]\n",
      "8391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004980] [C loss: 0.693259]\n",
      "8392 [D loss: 0.693272, acc.: 0.00%] [G loss: 0.005122] [C loss: 0.693260]\n",
      "8393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004020] [C loss: 0.693260]\n",
      "8394 [D loss: 0.693272, acc.: 0.00%] [G loss: 0.004458] [C loss: 0.693259]\n",
      "8395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007262] [C loss: 0.693259]\n",
      "8396 [D loss: 0.693271, acc.: 0.00%] [G loss: 0.002999] [C loss: 0.693259]\n",
      "8397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003920] [C loss: 0.693259]\n",
      "8398 [D loss: 0.693272, acc.: 0.00%] [G loss: 0.020606] [C loss: 0.693259]\n",
      "8399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008893] [C loss: 0.693259]\n",
      "8400 [D loss: 0.693271, acc.: 0.00%] [G loss: 0.006918] [C loss: 0.693258]\n",
      "8401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008455] [C loss: 0.693258]\n",
      "8402 [D loss: 0.693271, acc.: 0.00%] [G loss: 0.008971] [C loss: 0.693258]\n",
      "8403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.693258]\n",
      "8404 [D loss: 0.693271, acc.: 0.00%] [G loss: 0.004480] [C loss: 0.693258]\n",
      "8405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005485] [C loss: 0.693258]\n",
      "8406 [D loss: 0.693270, acc.: 0.00%] [G loss: 0.007040] [C loss: 0.693257]\n",
      "8407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005812] [C loss: 0.693257]\n",
      "8408 [D loss: 0.693270, acc.: 0.00%] [G loss: 0.004089] [C loss: 0.693256]\n",
      "8409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005258] [C loss: 0.693256]\n",
      "8410 [D loss: 0.693270, acc.: 0.00%] [G loss: 0.005297] [C loss: 0.693255]\n",
      "8411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005830] [C loss: 0.693255]\n",
      "8412 [D loss: 0.693269, acc.: 0.00%] [G loss: 0.004845] [C loss: 0.693257]\n",
      "8413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005268] [C loss: 0.693257]\n",
      "8414 [D loss: 0.693269, acc.: 0.00%] [G loss: 0.005164] [C loss: 0.693255]\n",
      "8415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.693255]\n",
      "8416 [D loss: 0.693269, acc.: 0.00%] [G loss: 0.004902] [C loss: 0.693256]\n",
      "8417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006113] [C loss: 0.693256]\n",
      "8418 [D loss: 0.693268, acc.: 0.00%] [G loss: 0.006088] [C loss: 0.693254]\n",
      "8419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003958] [C loss: 0.693254]\n",
      "8420 [D loss: 0.693268, acc.: 0.00%] [G loss: 0.003500] [C loss: 0.693255]\n",
      "8421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004613] [C loss: 0.693255]\n",
      "8422 [D loss: 0.693268, acc.: 0.00%] [G loss: 0.004734] [C loss: 0.693254]\n",
      "8423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.693254]\n",
      "8424 [D loss: 0.693267, acc.: 0.00%] [G loss: 0.005070] [C loss: 0.693254]\n",
      "8425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005700] [C loss: 0.693254]\n",
      "8426 [D loss: 0.693267, acc.: 0.00%] [G loss: 0.009849] [C loss: 0.693253]\n",
      "8427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005342] [C loss: 0.693253]\n",
      "8428 [D loss: 0.693267, acc.: 0.00%] [G loss: 0.006612] [C loss: 0.693253]\n",
      "8429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004180] [C loss: 0.693253]\n",
      "8430 [D loss: 0.693266, acc.: 0.00%] [G loss: 0.004254] [C loss: 0.693253]\n",
      "8431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005123] [C loss: 0.693253]\n",
      "8432 [D loss: 0.693266, acc.: 0.00%] [G loss: 0.005187] [C loss: 0.693252]\n",
      "8433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006054] [C loss: 0.693252]\n",
      "8434 [D loss: 0.693266, acc.: 0.00%] [G loss: 0.003889] [C loss: 0.693253]\n",
      "8435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004190] [C loss: 0.693253]\n",
      "8436 [D loss: 0.693266, acc.: 0.00%] [G loss: 0.005020] [C loss: 0.693252]\n",
      "8437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003689] [C loss: 0.693252]\n",
      "8438 [D loss: 0.693265, acc.: 0.00%] [G loss: 0.006492] [C loss: 0.693252]\n",
      "8439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006713] [C loss: 0.693252]\n",
      "8440 [D loss: 0.693265, acc.: 0.00%] [G loss: 0.004065] [C loss: 0.693251]\n",
      "8441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.693251]\n",
      "8442 [D loss: 0.693265, acc.: 0.00%] [G loss: 0.004727] [C loss: 0.693251]\n",
      "8443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009337] [C loss: 0.693251]\n",
      "8444 [D loss: 0.693264, acc.: 0.00%] [G loss: 0.005966] [C loss: 0.693251]\n",
      "8445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005871] [C loss: 0.693251]\n",
      "8446 [D loss: 0.693264, acc.: 0.00%] [G loss: 0.005013] [C loss: 0.693250]\n",
      "8447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007945] [C loss: 0.693250]\n",
      "8448 [D loss: 0.693263, acc.: 0.00%] [G loss: 0.004434] [C loss: 0.693249]\n",
      "8449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005998] [C loss: 0.693249]\n",
      "8450 [D loss: 0.693263, acc.: 0.00%] [G loss: 0.005117] [C loss: 0.693250]\n",
      "8451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004071] [C loss: 0.693250]\n",
      "8452 [D loss: 0.693263, acc.: 0.00%] [G loss: 0.004795] [C loss: 0.693250]\n",
      "8453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011124] [C loss: 0.693250]\n",
      "8454 [D loss: 0.693263, acc.: 0.00%] [G loss: 0.004689] [C loss: 0.693249]\n",
      "8455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004085] [C loss: 0.693249]\n",
      "8456 [D loss: 0.693263, acc.: 0.00%] [G loss: 0.008147] [C loss: 0.693249]\n",
      "8457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006048] [C loss: 0.693249]\n",
      "8458 [D loss: 0.693262, acc.: 0.00%] [G loss: 0.003790] [C loss: 0.693248]\n",
      "8459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004417] [C loss: 0.693248]\n",
      "8460 [D loss: 0.693262, acc.: 0.00%] [G loss: 0.004255] [C loss: 0.693248]\n",
      "8461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.020502] [C loss: 0.693248]\n",
      "8462 [D loss: 0.693262, acc.: 0.00%] [G loss: 0.005002] [C loss: 0.693248]\n",
      "8463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.693248]\n",
      "8464 [D loss: 0.693262, acc.: 0.00%] [G loss: 0.004473] [C loss: 0.693247]\n",
      "8465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007155] [C loss: 0.693247]\n",
      "8466 [D loss: 0.693261, acc.: 0.00%] [G loss: 0.006013] [C loss: 0.693247]\n",
      "8467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.016081] [C loss: 0.693247]\n",
      "8468 [D loss: 0.693261, acc.: 0.00%] [G loss: 0.004723] [C loss: 0.693246]\n",
      "8469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007185] [C loss: 0.693246]\n",
      "8470 [D loss: 0.693260, acc.: 0.00%] [G loss: 0.003483] [C loss: 0.693246]\n",
      "8471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006556] [C loss: 0.693246]\n",
      "8472 [D loss: 0.693261, acc.: 0.00%] [G loss: 0.004163] [C loss: 0.693245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003410] [C loss: 0.693245]\n",
      "8474 [D loss: 0.693260, acc.: 0.00%] [G loss: 0.009333] [C loss: 0.693245]\n",
      "8475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005378] [C loss: 0.693245]\n",
      "8476 [D loss: 0.693260, acc.: 0.00%] [G loss: 0.004997] [C loss: 0.693245]\n",
      "8477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004703] [C loss: 0.693245]\n",
      "8478 [D loss: 0.693259, acc.: 0.00%] [G loss: 0.004279] [C loss: 0.693244]\n",
      "8479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012171] [C loss: 0.693244]\n",
      "8480 [D loss: 0.693259, acc.: 0.00%] [G loss: 0.004945] [C loss: 0.693245]\n",
      "8481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005880] [C loss: 0.693245]\n",
      "8482 [D loss: 0.693260, acc.: 0.00%] [G loss: 0.004713] [C loss: 0.693244]\n",
      "8483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005015] [C loss: 0.693244]\n",
      "8484 [D loss: 0.693259, acc.: 0.00%] [G loss: 0.005616] [C loss: 0.693243]\n",
      "8485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004614] [C loss: 0.693243]\n",
      "8486 [D loss: 0.693258, acc.: 0.00%] [G loss: 0.004886] [C loss: 0.693243]\n",
      "8487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015318] [C loss: 0.693243]\n",
      "8488 [D loss: 0.693258, acc.: 0.00%] [G loss: 0.004057] [C loss: 0.693243]\n",
      "8489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006515] [C loss: 0.693243]\n",
      "8490 [D loss: 0.693258, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.693242]\n",
      "8491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005187] [C loss: 0.693242]\n",
      "8492 [D loss: 0.693257, acc.: 0.00%] [G loss: 0.004501] [C loss: 0.693242]\n",
      "8493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004107] [C loss: 0.693242]\n",
      "8494 [D loss: 0.693257, acc.: 0.00%] [G loss: 0.006400] [C loss: 0.693242]\n",
      "8495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006430] [C loss: 0.693242]\n",
      "8496 [D loss: 0.693257, acc.: 0.00%] [G loss: 0.003731] [C loss: 0.693242]\n",
      "8497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004163] [C loss: 0.693242]\n",
      "8498 [D loss: 0.693257, acc.: 0.00%] [G loss: 0.003467] [C loss: 0.693241]\n",
      "8499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006415] [C loss: 0.693241]\n",
      "8500 [D loss: 0.693256, acc.: 0.00%] [G loss: 0.014522] [C loss: 0.693241]\n",
      "8501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004748] [C loss: 0.693241]\n",
      "8502 [D loss: 0.693256, acc.: 0.00%] [G loss: 0.005038] [C loss: 0.693241]\n",
      "8503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003919] [C loss: 0.693241]\n",
      "8504 [D loss: 0.693256, acc.: 0.00%] [G loss: 0.004410] [C loss: 0.693241]\n",
      "8505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009696] [C loss: 0.693241]\n",
      "8506 [D loss: 0.693256, acc.: 0.00%] [G loss: 0.004385] [C loss: 0.693240]\n",
      "8507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003652] [C loss: 0.693240]\n",
      "8508 [D loss: 0.693255, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.693239]\n",
      "8509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004158] [C loss: 0.693239]\n",
      "8510 [D loss: 0.693255, acc.: 0.00%] [G loss: 0.004930] [C loss: 0.693240]\n",
      "8511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003879] [C loss: 0.693240]\n",
      "8512 [D loss: 0.693255, acc.: 0.00%] [G loss: 0.006074] [C loss: 0.693239]\n",
      "8513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004796] [C loss: 0.693239]\n",
      "8514 [D loss: 0.693255, acc.: 0.00%] [G loss: 0.004272] [C loss: 0.693238]\n",
      "8515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005077] [C loss: 0.693238]\n",
      "8516 [D loss: 0.693254, acc.: 0.00%] [G loss: 0.005959] [C loss: 0.693238]\n",
      "8517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003617] [C loss: 0.693238]\n",
      "8518 [D loss: 0.693254, acc.: 0.00%] [G loss: 0.006011] [C loss: 0.693238]\n",
      "8519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005947] [C loss: 0.693238]\n",
      "8520 [D loss: 0.693254, acc.: 0.00%] [G loss: 0.004548] [C loss: 0.693238]\n",
      "8521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003865] [C loss: 0.693238]\n",
      "8522 [D loss: 0.693253, acc.: 0.00%] [G loss: 0.004918] [C loss: 0.693236]\n",
      "8523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004324] [C loss: 0.693236]\n",
      "8524 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.006330] [C loss: 0.693237]\n",
      "8525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004218] [C loss: 0.693237]\n",
      "8526 [D loss: 0.693253, acc.: 0.00%] [G loss: 0.005018] [C loss: 0.693236]\n",
      "8527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004126] [C loss: 0.693236]\n",
      "8528 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.005094] [C loss: 0.693236]\n",
      "8529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004688] [C loss: 0.693236]\n",
      "8530 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.004930] [C loss: 0.693236]\n",
      "8531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003896] [C loss: 0.693236]\n",
      "8532 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.004091] [C loss: 0.693236]\n",
      "8533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006103] [C loss: 0.693236]\n",
      "8534 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.005773] [C loss: 0.693235]\n",
      "8535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006338] [C loss: 0.693235]\n",
      "8536 [D loss: 0.693252, acc.: 0.00%] [G loss: 0.003818] [C loss: 0.693235]\n",
      "8537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003808] [C loss: 0.693235]\n",
      "8538 [D loss: 0.693251, acc.: 0.00%] [G loss: 0.004932] [C loss: 0.693235]\n",
      "8539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010579] [C loss: 0.693235]\n",
      "8540 [D loss: 0.693251, acc.: 0.00%] [G loss: 0.005816] [C loss: 0.693234]\n",
      "8541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005902] [C loss: 0.693234]\n",
      "8542 [D loss: 0.693251, acc.: 0.00%] [G loss: 0.004529] [C loss: 0.693234]\n",
      "8543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005626] [C loss: 0.693234]\n",
      "8544 [D loss: 0.693251, acc.: 0.00%] [G loss: 0.004972] [C loss: 0.693234]\n",
      "8545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004186] [C loss: 0.693234]\n",
      "8546 [D loss: 0.693250, acc.: 0.00%] [G loss: 0.004974] [C loss: 0.693233]\n",
      "8547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005506] [C loss: 0.693233]\n",
      "8548 [D loss: 0.693250, acc.: 0.00%] [G loss: 0.005073] [C loss: 0.693234]\n",
      "8549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004433] [C loss: 0.693234]\n",
      "8550 [D loss: 0.693250, acc.: 0.00%] [G loss: 0.004582] [C loss: 0.693233]\n",
      "8551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003674] [C loss: 0.693233]\n",
      "8552 [D loss: 0.693249, acc.: 0.00%] [G loss: 0.004470] [C loss: 0.693233]\n",
      "8553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004949] [C loss: 0.693233]\n",
      "8554 [D loss: 0.693249, acc.: 0.00%] [G loss: 0.005198] [C loss: 0.693232]\n",
      "8555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004778] [C loss: 0.693232]\n",
      "8556 [D loss: 0.693249, acc.: 0.00%] [G loss: 0.007538] [C loss: 0.693232]\n",
      "8557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003390] [C loss: 0.693232]\n",
      "8558 [D loss: 0.693249, acc.: 0.00%] [G loss: 0.003698] [C loss: 0.693232]\n",
      "8559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007433] [C loss: 0.693232]\n",
      "8560 [D loss: 0.693249, acc.: 0.00%] [G loss: 0.005111] [C loss: 0.693232]\n",
      "8561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006216] [C loss: 0.693232]\n",
      "8562 [D loss: 0.693248, acc.: 0.00%] [G loss: 0.005159] [C loss: 0.693231]\n",
      "8563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006829] [C loss: 0.693231]\n",
      "8564 [D loss: 0.693248, acc.: 0.00%] [G loss: 0.005196] [C loss: 0.693231]\n",
      "8565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007396] [C loss: 0.693231]\n",
      "8566 [D loss: 0.693248, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.693230]\n",
      "8567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004500] [C loss: 0.693230]\n",
      "8568 [D loss: 0.693247, acc.: 0.00%] [G loss: 0.004232] [C loss: 0.693230]\n",
      "8569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004308] [C loss: 0.693230]\n",
      "8570 [D loss: 0.693247, acc.: 0.00%] [G loss: 0.004418] [C loss: 0.693230]\n",
      "8571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004761] [C loss: 0.693230]\n",
      "8572 [D loss: 0.693247, acc.: 0.00%] [G loss: 0.006730] [C loss: 0.693229]\n",
      "8573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004124] [C loss: 0.693229]\n",
      "8574 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.006712] [C loss: 0.693229]\n",
      "8575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003764] [C loss: 0.693229]\n",
      "8576 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.003448] [C loss: 0.693228]\n",
      "8577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005352] [C loss: 0.693228]\n",
      "8578 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.020189] [C loss: 0.693229]\n",
      "8579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004113] [C loss: 0.693229]\n",
      "8580 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.005900] [C loss: 0.693228]\n",
      "8581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004903] [C loss: 0.693228]\n",
      "8582 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.005475] [C loss: 0.693228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003779] [C loss: 0.693228]\n",
      "8584 [D loss: 0.693246, acc.: 0.00%] [G loss: 0.007843] [C loss: 0.693228]\n",
      "8585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004421] [C loss: 0.693228]\n",
      "8586 [D loss: 0.693245, acc.: 0.00%] [G loss: 0.005333] [C loss: 0.693227]\n",
      "8587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.693227]\n",
      "8588 [D loss: 0.693245, acc.: 0.00%] [G loss: 0.004995] [C loss: 0.693226]\n",
      "8589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004176] [C loss: 0.693226]\n",
      "8590 [D loss: 0.693244, acc.: 0.00%] [G loss: 0.005299] [C loss: 0.693227]\n",
      "8591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005226] [C loss: 0.693227]\n",
      "8592 [D loss: 0.693244, acc.: 0.00%] [G loss: 0.030254] [C loss: 0.693226]\n",
      "8593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004247] [C loss: 0.693226]\n",
      "8594 [D loss: 0.693244, acc.: 0.00%] [G loss: 0.008002] [C loss: 0.693227]\n",
      "8595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003800] [C loss: 0.693227]\n",
      "8596 [D loss: 0.693244, acc.: 0.00%] [G loss: 0.007117] [C loss: 0.693225]\n",
      "8597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014265] [C loss: 0.693225]\n",
      "8598 [D loss: 0.693243, acc.: 0.00%] [G loss: 0.005049] [C loss: 0.693225]\n",
      "8599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006150] [C loss: 0.693225]\n",
      "8600 [D loss: 0.693243, acc.: 0.00%] [G loss: 0.006510] [C loss: 0.693225]\n",
      "8601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008738] [C loss: 0.693225]\n",
      "8602 [D loss: 0.693243, acc.: 0.00%] [G loss: 0.005577] [C loss: 0.693225]\n",
      "8603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005785] [C loss: 0.693225]\n",
      "8604 [D loss: 0.693243, acc.: 0.00%] [G loss: 0.006055] [C loss: 0.693224]\n",
      "8605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007759] [C loss: 0.693224]\n",
      "8606 [D loss: 0.693243, acc.: 0.00%] [G loss: 0.004534] [C loss: 0.693224]\n",
      "8607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003753] [C loss: 0.693224]\n",
      "8608 [D loss: 0.693242, acc.: 0.00%] [G loss: 0.006145] [C loss: 0.693224]\n",
      "8609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004658] [C loss: 0.693224]\n",
      "8610 [D loss: 0.693242, acc.: 0.00%] [G loss: 0.004468] [C loss: 0.693223]\n",
      "8611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006055] [C loss: 0.693223]\n",
      "8612 [D loss: 0.693242, acc.: 0.00%] [G loss: 0.003293] [C loss: 0.693224]\n",
      "8613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003712] [C loss: 0.693224]\n",
      "8614 [D loss: 0.693242, acc.: 0.00%] [G loss: 0.005006] [C loss: 0.693223]\n",
      "8615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004593] [C loss: 0.693223]\n",
      "8616 [D loss: 0.693242, acc.: 0.00%] [G loss: 0.004422] [C loss: 0.693223]\n",
      "8617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007479] [C loss: 0.693223]\n",
      "8618 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.005224] [C loss: 0.693223]\n",
      "8619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004950] [C loss: 0.693223]\n",
      "8620 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.693222]\n",
      "8621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.693222]\n",
      "8622 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.004188] [C loss: 0.693222]\n",
      "8623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012448] [C loss: 0.693222]\n",
      "8624 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.004490] [C loss: 0.693222]\n",
      "8625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008746] [C loss: 0.693222]\n",
      "8626 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.004333] [C loss: 0.693222]\n",
      "8627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003612] [C loss: 0.693222]\n",
      "8628 [D loss: 0.693241, acc.: 0.00%] [G loss: 0.005821] [C loss: 0.693222]\n",
      "8629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019416] [C loss: 0.693222]\n",
      "8630 [D loss: 0.693240, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.693220]\n",
      "8631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006628] [C loss: 0.693220]\n",
      "8632 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.003938] [C loss: 0.693220]\n",
      "8633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004954] [C loss: 0.693220]\n",
      "8634 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.004010] [C loss: 0.693220]\n",
      "8635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005104] [C loss: 0.693220]\n",
      "8636 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.006763] [C loss: 0.693220]\n",
      "8637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005838] [C loss: 0.693220]\n",
      "8638 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.004327] [C loss: 0.693220]\n",
      "8639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005277] [C loss: 0.693220]\n",
      "8640 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.006435] [C loss: 0.693219]\n",
      "8641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005283] [C loss: 0.693219]\n",
      "8642 [D loss: 0.693239, acc.: 0.00%] [G loss: 0.004169] [C loss: 0.693218]\n",
      "8643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005277] [C loss: 0.693218]\n",
      "8644 [D loss: 0.693238, acc.: 0.00%] [G loss: 0.009588] [C loss: 0.693219]\n",
      "8645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006066] [C loss: 0.693219]\n",
      "8646 [D loss: 0.693238, acc.: 0.00%] [G loss: 0.006964] [C loss: 0.693218]\n",
      "8647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007313] [C loss: 0.693218]\n",
      "8648 [D loss: 0.693238, acc.: 0.00%] [G loss: 0.006621] [C loss: 0.693217]\n",
      "8649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003954] [C loss: 0.693217]\n",
      "8650 [D loss: 0.693237, acc.: 0.00%] [G loss: 0.008283] [C loss: 0.693218]\n",
      "8651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006597] [C loss: 0.693218]\n",
      "8652 [D loss: 0.693237, acc.: 0.00%] [G loss: 0.005708] [C loss: 0.693218]\n",
      "8653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005942] [C loss: 0.693218]\n",
      "8654 [D loss: 0.693237, acc.: 0.00%] [G loss: 0.006487] [C loss: 0.693217]\n",
      "8655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005363] [C loss: 0.693217]\n",
      "8656 [D loss: 0.693237, acc.: 0.00%] [G loss: 0.019562] [C loss: 0.693217]\n",
      "8657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003872] [C loss: 0.693217]\n",
      "8658 [D loss: 0.693237, acc.: 0.00%] [G loss: 0.005555] [C loss: 0.693216]\n",
      "8659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007322] [C loss: 0.693216]\n",
      "8660 [D loss: 0.693236, acc.: 0.00%] [G loss: 0.006553] [C loss: 0.693216]\n",
      "8661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006915] [C loss: 0.693216]\n",
      "8662 [D loss: 0.693236, acc.: 0.00%] [G loss: 0.005663] [C loss: 0.693216]\n",
      "8663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005767] [C loss: 0.693216]\n",
      "8664 [D loss: 0.693236, acc.: 0.00%] [G loss: 0.009153] [C loss: 0.693215]\n",
      "8665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006230] [C loss: 0.693215]\n",
      "8666 [D loss: 0.693235, acc.: 0.00%] [G loss: 0.005454] [C loss: 0.693216]\n",
      "8667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008579] [C loss: 0.693216]\n",
      "8668 [D loss: 0.693236, acc.: 0.00%] [G loss: 0.005131] [C loss: 0.693214]\n",
      "8669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004169] [C loss: 0.693214]\n",
      "8670 [D loss: 0.693235, acc.: 0.00%] [G loss: 0.005536] [C loss: 0.693214]\n",
      "8671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004737] [C loss: 0.693214]\n",
      "8672 [D loss: 0.693235, acc.: 0.00%] [G loss: 0.009131] [C loss: 0.693215]\n",
      "8673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006373] [C loss: 0.693215]\n",
      "8674 [D loss: 0.693235, acc.: 0.00%] [G loss: 0.004152] [C loss: 0.693214]\n",
      "8675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005150] [C loss: 0.693214]\n",
      "8676 [D loss: 0.693234, acc.: 0.00%] [G loss: 0.006030] [C loss: 0.693214]\n",
      "8677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010078] [C loss: 0.693214]\n",
      "8678 [D loss: 0.693234, acc.: 0.00%] [G loss: 0.005373] [C loss: 0.693213]\n",
      "8679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005140] [C loss: 0.693213]\n",
      "8680 [D loss: 0.693234, acc.: 0.00%] [G loss: 0.003970] [C loss: 0.693213]\n",
      "8681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004674] [C loss: 0.693213]\n",
      "8682 [D loss: 0.693234, acc.: 0.00%] [G loss: 0.017553] [C loss: 0.693213]\n",
      "8683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005925] [C loss: 0.693213]\n",
      "8684 [D loss: 0.693234, acc.: 0.00%] [G loss: 0.015264] [C loss: 0.693213]\n",
      "8685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004843] [C loss: 0.693213]\n",
      "8686 [D loss: 0.693233, acc.: 0.00%] [G loss: 0.005834] [C loss: 0.693213]\n",
      "8687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005193] [C loss: 0.693213]\n",
      "8688 [D loss: 0.693233, acc.: 0.00%] [G loss: 0.005821] [C loss: 0.693212]\n",
      "8689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004197] [C loss: 0.693212]\n",
      "8690 [D loss: 0.693233, acc.: 0.00%] [G loss: 0.006185] [C loss: 0.693212]\n",
      "8691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005387] [C loss: 0.693212]\n",
      "8692 [D loss: 0.693233, acc.: 0.00%] [G loss: 0.005358] [C loss: 0.693212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005665] [C loss: 0.693212]\n",
      "8694 [D loss: 0.693233, acc.: 0.00%] [G loss: 0.003974] [C loss: 0.693211]\n",
      "8695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014203] [C loss: 0.693211]\n",
      "8696 [D loss: 0.693232, acc.: 0.00%] [G loss: 0.006758] [C loss: 0.693211]\n",
      "8697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003790] [C loss: 0.693211]\n",
      "8698 [D loss: 0.693232, acc.: 0.00%] [G loss: 0.006882] [C loss: 0.693211]\n",
      "8699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004956] [C loss: 0.693211]\n",
      "8700 [D loss: 0.693232, acc.: 0.00%] [G loss: 0.004418] [C loss: 0.693210]\n",
      "8701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008582] [C loss: 0.693210]\n",
      "8702 [D loss: 0.693232, acc.: 0.00%] [G loss: 0.006322] [C loss: 0.693210]\n",
      "8703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003738] [C loss: 0.693210]\n",
      "8704 [D loss: 0.693231, acc.: 0.00%] [G loss: 0.003506] [C loss: 0.693210]\n",
      "8705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004957] [C loss: 0.693210]\n",
      "8706 [D loss: 0.693231, acc.: 0.00%] [G loss: 0.003536] [C loss: 0.693210]\n",
      "8707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004153] [C loss: 0.693210]\n",
      "8708 [D loss: 0.693231, acc.: 0.00%] [G loss: 0.009574] [C loss: 0.693209]\n",
      "8709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005885] [C loss: 0.693209]\n",
      "8710 [D loss: 0.693231, acc.: 0.00%] [G loss: 0.003930] [C loss: 0.693208]\n",
      "8711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004538] [C loss: 0.693208]\n",
      "8712 [D loss: 0.693230, acc.: 0.00%] [G loss: 0.004560] [C loss: 0.693208]\n",
      "8713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003913] [C loss: 0.693208]\n",
      "8714 [D loss: 0.693230, acc.: 0.00%] [G loss: 0.006278] [C loss: 0.693208]\n",
      "8715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004357] [C loss: 0.693208]\n",
      "8716 [D loss: 0.693230, acc.: 0.00%] [G loss: 0.006444] [C loss: 0.693208]\n",
      "8717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005404] [C loss: 0.693208]\n",
      "8718 [D loss: 0.693230, acc.: 0.00%] [G loss: 0.006962] [C loss: 0.693207]\n",
      "8719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005275] [C loss: 0.693207]\n",
      "8720 [D loss: 0.693229, acc.: 0.00%] [G loss: 0.005880] [C loss: 0.693207]\n",
      "8721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006921] [C loss: 0.693207]\n",
      "8722 [D loss: 0.693229, acc.: 0.00%] [G loss: 0.008123] [C loss: 0.693207]\n",
      "8723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004424] [C loss: 0.693207]\n",
      "8724 [D loss: 0.693229, acc.: 0.00%] [G loss: 0.005166] [C loss: 0.693207]\n",
      "8725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.693207]\n",
      "8726 [D loss: 0.693229, acc.: 0.00%] [G loss: 0.023320] [C loss: 0.693206]\n",
      "8727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006041] [C loss: 0.693206]\n",
      "8728 [D loss: 0.693229, acc.: 0.00%] [G loss: 0.005216] [C loss: 0.693206]\n",
      "8729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.693206]\n",
      "8730 [D loss: 0.693228, acc.: 0.00%] [G loss: 0.003653] [C loss: 0.693206]\n",
      "8731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.693206]\n",
      "8732 [D loss: 0.693228, acc.: 0.00%] [G loss: 0.005833] [C loss: 0.693206]\n",
      "8733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004680] [C loss: 0.693206]\n",
      "8734 [D loss: 0.693228, acc.: 0.00%] [G loss: 0.007023] [C loss: 0.693206]\n",
      "8735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004097] [C loss: 0.693206]\n",
      "8736 [D loss: 0.693228, acc.: 0.00%] [G loss: 0.004798] [C loss: 0.693205]\n",
      "8737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004471] [C loss: 0.693205]\n",
      "8738 [D loss: 0.693228, acc.: 0.00%] [G loss: 0.004626] [C loss: 0.693205]\n",
      "8739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004977] [C loss: 0.693205]\n",
      "8740 [D loss: 0.693227, acc.: 0.00%] [G loss: 0.004371] [C loss: 0.693205]\n",
      "8741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005276] [C loss: 0.693205]\n",
      "8742 [D loss: 0.693227, acc.: 0.00%] [G loss: 0.006264] [C loss: 0.693205]\n",
      "8743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004726] [C loss: 0.693205]\n",
      "8744 [D loss: 0.693227, acc.: 0.00%] [G loss: 0.004091] [C loss: 0.693204]\n",
      "8745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005385] [C loss: 0.693204]\n",
      "8746 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.004913] [C loss: 0.693205]\n",
      "8747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005879] [C loss: 0.693205]\n",
      "8748 [D loss: 0.693227, acc.: 0.00%] [G loss: 0.004935] [C loss: 0.693204]\n",
      "8749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004979] [C loss: 0.693204]\n",
      "8750 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.004681] [C loss: 0.693203]\n",
      "8751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004397] [C loss: 0.693203]\n",
      "8752 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.006736] [C loss: 0.693203]\n",
      "8753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006456] [C loss: 0.693203]\n",
      "8754 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.004334] [C loss: 0.693203]\n",
      "8755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004707] [C loss: 0.693203]\n",
      "8756 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.003709] [C loss: 0.693202]\n",
      "8757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006799] [C loss: 0.693202]\n",
      "8758 [D loss: 0.693226, acc.: 0.00%] [G loss: 0.005525] [C loss: 0.693202]\n",
      "8759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007098] [C loss: 0.693202]\n",
      "8760 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.004425] [C loss: 0.693202]\n",
      "8761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004756] [C loss: 0.693202]\n",
      "8762 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.004458] [C loss: 0.693202]\n",
      "8763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009353] [C loss: 0.693202]\n",
      "8764 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.003439] [C loss: 0.693202]\n",
      "8765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005872] [C loss: 0.693202]\n",
      "8766 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.003813] [C loss: 0.693201]\n",
      "8767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004496] [C loss: 0.693201]\n",
      "8768 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.005137] [C loss: 0.693201]\n",
      "8769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006350] [C loss: 0.693201]\n",
      "8770 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.007208] [C loss: 0.693201]\n",
      "8771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693201]\n",
      "8772 [D loss: 0.693225, acc.: 0.00%] [G loss: 0.006042] [C loss: 0.693201]\n",
      "8773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006430] [C loss: 0.693201]\n",
      "8774 [D loss: 0.693224, acc.: 0.00%] [G loss: 0.004538] [C loss: 0.693200]\n",
      "8775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004379] [C loss: 0.693200]\n",
      "8776 [D loss: 0.693224, acc.: 0.00%] [G loss: 0.006628] [C loss: 0.693199]\n",
      "8777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004890] [C loss: 0.693199]\n",
      "8778 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.005256] [C loss: 0.693200]\n",
      "8779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005044] [C loss: 0.693200]\n",
      "8780 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.008038] [C loss: 0.693199]\n",
      "8781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005006] [C loss: 0.693199]\n",
      "8782 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.004129] [C loss: 0.693199]\n",
      "8783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004202] [C loss: 0.693199]\n",
      "8784 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.005384] [C loss: 0.693199]\n",
      "8785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003922] [C loss: 0.693199]\n",
      "8786 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.005241] [C loss: 0.693198]\n",
      "8787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006474] [C loss: 0.693198]\n",
      "8788 [D loss: 0.693222, acc.: 0.00%] [G loss: 0.006099] [C loss: 0.693199]\n",
      "8789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006981] [C loss: 0.693199]\n",
      "8790 [D loss: 0.693223, acc.: 0.00%] [G loss: 0.003767] [C loss: 0.693198]\n",
      "8791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004371] [C loss: 0.693198]\n",
      "8792 [D loss: 0.693222, acc.: 0.00%] [G loss: 0.006389] [C loss: 0.693198]\n",
      "8793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004826] [C loss: 0.693198]\n",
      "8794 [D loss: 0.693222, acc.: 0.00%] [G loss: 0.006051] [C loss: 0.693198]\n",
      "8795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010348] [C loss: 0.693198]\n",
      "8796 [D loss: 0.693222, acc.: 0.00%] [G loss: 0.004095] [C loss: 0.693197]\n",
      "8797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006404] [C loss: 0.693197]\n",
      "8798 [D loss: 0.693222, acc.: 0.00%] [G loss: 0.005449] [C loss: 0.693197]\n",
      "8799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004576] [C loss: 0.693197]\n",
      "8800 [D loss: 0.693221, acc.: 0.00%] [G loss: 0.004399] [C loss: 0.693197]\n",
      "8801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008673] [C loss: 0.693197]\n",
      "8802 [D loss: 0.693221, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.693197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006931] [C loss: 0.693197]\n",
      "8804 [D loss: 0.693221, acc.: 0.00%] [G loss: 0.005799] [C loss: 0.693196]\n",
      "8805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004080] [C loss: 0.693196]\n",
      "8806 [D loss: 0.693221, acc.: 0.00%] [G loss: 0.004505] [C loss: 0.693196]\n",
      "8807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007041] [C loss: 0.693196]\n",
      "8808 [D loss: 0.693221, acc.: 0.00%] [G loss: 0.005853] [C loss: 0.693195]\n",
      "8809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005376] [C loss: 0.693195]\n",
      "8810 [D loss: 0.693220, acc.: 0.00%] [G loss: 0.005987] [C loss: 0.693196]\n",
      "8811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004912] [C loss: 0.693196]\n",
      "8812 [D loss: 0.693220, acc.: 0.00%] [G loss: 0.005735] [C loss: 0.693195]\n",
      "8813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004371] [C loss: 0.693195]\n",
      "8814 [D loss: 0.693220, acc.: 0.00%] [G loss: 0.005969] [C loss: 0.693196]\n",
      "8815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004785] [C loss: 0.693196]\n",
      "8816 [D loss: 0.693220, acc.: 0.00%] [G loss: 0.004725] [C loss: 0.693195]\n",
      "8817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004317] [C loss: 0.693195]\n",
      "8818 [D loss: 0.693220, acc.: 0.00%] [G loss: 0.005493] [C loss: 0.693194]\n",
      "8819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004421] [C loss: 0.693194]\n",
      "8820 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.003552] [C loss: 0.693194]\n",
      "8821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005847] [C loss: 0.693194]\n",
      "8822 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.004868] [C loss: 0.693194]\n",
      "8823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004680] [C loss: 0.693194]\n",
      "8824 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.005736] [C loss: 0.693193]\n",
      "8825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004307] [C loss: 0.693193]\n",
      "8826 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.006486] [C loss: 0.693193]\n",
      "8827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003459] [C loss: 0.693193]\n",
      "8828 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.004691] [C loss: 0.693193]\n",
      "8829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004706] [C loss: 0.693193]\n",
      "8830 [D loss: 0.693219, acc.: 0.00%] [G loss: 0.004524] [C loss: 0.693193]\n",
      "8831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003850] [C loss: 0.693193]\n",
      "8832 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.004045] [C loss: 0.693192]\n",
      "8833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004490] [C loss: 0.693192]\n",
      "8834 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.003628] [C loss: 0.693192]\n",
      "8835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004016] [C loss: 0.693192]\n",
      "8836 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.006681] [C loss: 0.693192]\n",
      "8837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003819] [C loss: 0.693192]\n",
      "8838 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.005757] [C loss: 0.693192]\n",
      "8839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005181] [C loss: 0.693192]\n",
      "8840 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.004611] [C loss: 0.693192]\n",
      "8841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004799] [C loss: 0.693192]\n",
      "8842 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.004512] [C loss: 0.693192]\n",
      "8843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007600] [C loss: 0.693192]\n",
      "8844 [D loss: 0.693217, acc.: 0.00%] [G loss: 0.003980] [C loss: 0.693192]\n",
      "8845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005920] [C loss: 0.693192]\n",
      "8846 [D loss: 0.693218, acc.: 0.00%] [G loss: 0.006060] [C loss: 0.693191]\n",
      "8847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005333] [C loss: 0.693191]\n",
      "8848 [D loss: 0.693217, acc.: 0.00%] [G loss: 0.005720] [C loss: 0.693191]\n",
      "8849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.693191]\n",
      "8850 [D loss: 0.693217, acc.: 0.00%] [G loss: 0.004410] [C loss: 0.693190]\n",
      "8851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009614] [C loss: 0.693190]\n",
      "8852 [D loss: 0.693217, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.693190]\n",
      "8853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006253] [C loss: 0.693190]\n",
      "8854 [D loss: 0.693216, acc.: 0.00%] [G loss: 0.004858] [C loss: 0.693190]\n",
      "8855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005543] [C loss: 0.693190]\n",
      "8856 [D loss: 0.693216, acc.: 0.00%] [G loss: 0.006253] [C loss: 0.693190]\n",
      "8857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004717] [C loss: 0.693190]\n",
      "8858 [D loss: 0.693217, acc.: 0.00%] [G loss: 0.004813] [C loss: 0.693190]\n",
      "8859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006569] [C loss: 0.693190]\n",
      "8860 [D loss: 0.693216, acc.: 0.00%] [G loss: 0.004146] [C loss: 0.693189]\n",
      "8861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004303] [C loss: 0.693189]\n",
      "8862 [D loss: 0.693216, acc.: 0.00%] [G loss: 0.003959] [C loss: 0.693189]\n",
      "8863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007607] [C loss: 0.693189]\n",
      "8864 [D loss: 0.693216, acc.: 0.00%] [G loss: 0.006353] [C loss: 0.693188]\n",
      "8865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004118] [C loss: 0.693188]\n",
      "8866 [D loss: 0.693215, acc.: 0.00%] [G loss: 0.005754] [C loss: 0.693188]\n",
      "8867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007548] [C loss: 0.693188]\n",
      "8868 [D loss: 0.693215, acc.: 0.00%] [G loss: 0.007432] [C loss: 0.693188]\n",
      "8869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004731] [C loss: 0.693188]\n",
      "8870 [D loss: 0.693215, acc.: 0.00%] [G loss: 0.004527] [C loss: 0.693187]\n",
      "8871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.693187]\n",
      "8872 [D loss: 0.693215, acc.: 0.00%] [G loss: 0.005022] [C loss: 0.693187]\n",
      "8873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004950] [C loss: 0.693187]\n",
      "8874 [D loss: 0.693215, acc.: 0.00%] [G loss: 0.005224] [C loss: 0.693187]\n",
      "8875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.693187]\n",
      "8876 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.693187]\n",
      "8877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007215] [C loss: 0.693187]\n",
      "8878 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.005323] [C loss: 0.693187]\n",
      "8879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005439] [C loss: 0.693187]\n",
      "8880 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.005357] [C loss: 0.693186]\n",
      "8881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.693186]\n",
      "8882 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.004584] [C loss: 0.693186]\n",
      "8883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004929] [C loss: 0.693186]\n",
      "8884 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.002784] [C loss: 0.693186]\n",
      "8885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004359] [C loss: 0.693186]\n",
      "8886 [D loss: 0.693214, acc.: 0.00%] [G loss: 0.021313] [C loss: 0.693186]\n",
      "8887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005092] [C loss: 0.693186]\n",
      "8888 [D loss: 0.693213, acc.: 0.00%] [G loss: 0.004744] [C loss: 0.693186]\n",
      "8889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004015] [C loss: 0.693186]\n",
      "8890 [D loss: 0.693213, acc.: 0.00%] [G loss: 0.004406] [C loss: 0.693186]\n",
      "8891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015495] [C loss: 0.693186]\n",
      "8892 [D loss: 0.693213, acc.: 0.00%] [G loss: 0.005697] [C loss: 0.693185]\n",
      "8893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008730] [C loss: 0.693185]\n",
      "8894 [D loss: 0.693213, acc.: 0.00%] [G loss: 0.004930] [C loss: 0.693185]\n",
      "8895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005129] [C loss: 0.693185]\n",
      "8896 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.005226] [C loss: 0.693185]\n",
      "8897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005469] [C loss: 0.693185]\n",
      "8898 [D loss: 0.693213, acc.: 0.00%] [G loss: 0.006066] [C loss: 0.693185]\n",
      "8899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004379] [C loss: 0.693185]\n",
      "8900 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.004126] [C loss: 0.693184]\n",
      "8901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004489] [C loss: 0.693184]\n",
      "8902 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.013151] [C loss: 0.693184]\n",
      "8903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004210] [C loss: 0.693184]\n",
      "8904 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.004008] [C loss: 0.693184]\n",
      "8905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005004] [C loss: 0.693184]\n",
      "8906 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.004222] [C loss: 0.693184]\n",
      "8907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.022248] [C loss: 0.693184]\n",
      "8908 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.004837] [C loss: 0.693183]\n",
      "8909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005303] [C loss: 0.693183]\n",
      "8910 [D loss: 0.693212, acc.: 0.00%] [G loss: 0.005780] [C loss: 0.693183]\n",
      "8911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005186] [C loss: 0.693183]\n",
      "8912 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.004181] [C loss: 0.693182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005336] [C loss: 0.693182]\n",
      "8914 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.005080] [C loss: 0.693183]\n",
      "8915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005662] [C loss: 0.693183]\n",
      "8916 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.005151] [C loss: 0.693182]\n",
      "8917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021544] [C loss: 0.693182]\n",
      "8918 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.006620] [C loss: 0.693182]\n",
      "8919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004041] [C loss: 0.693182]\n",
      "8920 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.005195] [C loss: 0.693182]\n",
      "8921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004249] [C loss: 0.693182]\n",
      "8922 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.009667] [C loss: 0.693182]\n",
      "8923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004951] [C loss: 0.693182]\n",
      "8924 [D loss: 0.693211, acc.: 0.00%] [G loss: 0.009500] [C loss: 0.693182]\n",
      "8925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005071] [C loss: 0.693182]\n",
      "8926 [D loss: 0.693210, acc.: 0.00%] [G loss: 0.004578] [C loss: 0.693181]\n",
      "8927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004254] [C loss: 0.693181]\n",
      "8928 [D loss: 0.693210, acc.: 0.00%] [G loss: 0.004922] [C loss: 0.693181]\n",
      "8929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004741] [C loss: 0.693181]\n",
      "8930 [D loss: 0.693210, acc.: 0.00%] [G loss: 0.005865] [C loss: 0.693181]\n",
      "8931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004767] [C loss: 0.693181]\n",
      "8932 [D loss: 0.693210, acc.: 0.00%] [G loss: 0.004861] [C loss: 0.693180]\n",
      "8933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006077] [C loss: 0.693180]\n",
      "8934 [D loss: 0.693210, acc.: 0.00%] [G loss: 0.006459] [C loss: 0.693180]\n",
      "8935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005995] [C loss: 0.693180]\n",
      "8936 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.005030] [C loss: 0.693180]\n",
      "8937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004893] [C loss: 0.693180]\n",
      "8938 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.003801] [C loss: 0.693180]\n",
      "8939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003992] [C loss: 0.693180]\n",
      "8940 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.005826] [C loss: 0.693180]\n",
      "8941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004407] [C loss: 0.693180]\n",
      "8942 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.006941] [C loss: 0.693179]\n",
      "8943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004936] [C loss: 0.693179]\n",
      "8944 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.006966] [C loss: 0.693179]\n",
      "8945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007592] [C loss: 0.693179]\n",
      "8946 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.005867] [C loss: 0.693178]\n",
      "8947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003767] [C loss: 0.693178]\n",
      "8948 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.004259] [C loss: 0.693179]\n",
      "8949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005187] [C loss: 0.693179]\n",
      "8950 [D loss: 0.693209, acc.: 0.00%] [G loss: 0.013916] [C loss: 0.693178]\n",
      "8951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005185] [C loss: 0.693178]\n",
      "8952 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.003214] [C loss: 0.693178]\n",
      "8953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004334] [C loss: 0.693178]\n",
      "8954 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.005337] [C loss: 0.693178]\n",
      "8955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004742] [C loss: 0.693178]\n",
      "8956 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.004154] [C loss: 0.693177]\n",
      "8957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005241] [C loss: 0.693177]\n",
      "8958 [D loss: 0.693207, acc.: 0.00%] [G loss: 0.006432] [C loss: 0.693178]\n",
      "8959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003961] [C loss: 0.693178]\n",
      "8960 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.007186] [C loss: 0.693177]\n",
      "8961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005256] [C loss: 0.693177]\n",
      "8962 [D loss: 0.693208, acc.: 0.00%] [G loss: 0.005688] [C loss: 0.693177]\n",
      "8963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004072] [C loss: 0.693177]\n",
      "8964 [D loss: 0.693207, acc.: 0.00%] [G loss: 0.004907] [C loss: 0.693177]\n",
      "8965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004755] [C loss: 0.693177]\n",
      "8966 [D loss: 0.693207, acc.: 0.00%] [G loss: 0.003893] [C loss: 0.693177]\n",
      "8967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004729] [C loss: 0.693177]\n",
      "8968 [D loss: 0.693207, acc.: 0.00%] [G loss: 0.006431] [C loss: 0.693176]\n",
      "8969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003936] [C loss: 0.693176]\n",
      "8970 [D loss: 0.693207, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.693176]\n",
      "8971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005670] [C loss: 0.693176]\n",
      "8972 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.006153] [C loss: 0.693176]\n",
      "8973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003568] [C loss: 0.693176]\n",
      "8974 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.005887] [C loss: 0.693176]\n",
      "8975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004035] [C loss: 0.693176]\n",
      "8976 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.004889] [C loss: 0.693176]\n",
      "8977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007410] [C loss: 0.693176]\n",
      "8978 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.003469] [C loss: 0.693176]\n",
      "8979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003431] [C loss: 0.693176]\n",
      "8980 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.005441] [C loss: 0.693175]\n",
      "8981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005746] [C loss: 0.693175]\n",
      "8982 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.003272] [C loss: 0.693174]\n",
      "8983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003337] [C loss: 0.693174]\n",
      "8984 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.005639] [C loss: 0.693174]\n",
      "8985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003688] [C loss: 0.693174]\n",
      "8986 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.012995] [C loss: 0.693174]\n",
      "8987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005105] [C loss: 0.693174]\n",
      "8988 [D loss: 0.693206, acc.: 0.00%] [G loss: 0.007241] [C loss: 0.693174]\n",
      "8989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005558] [C loss: 0.693174]\n",
      "8990 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.006603] [C loss: 0.693174]\n",
      "8991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004715] [C loss: 0.693174]\n",
      "8992 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.005435] [C loss: 0.693174]\n",
      "8993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003667] [C loss: 0.693174]\n",
      "8994 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.005137] [C loss: 0.693173]\n",
      "8995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005028] [C loss: 0.693173]\n",
      "8996 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.003511] [C loss: 0.693173]\n",
      "8997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004022] [C loss: 0.693173]\n",
      "8998 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.004516] [C loss: 0.693173]\n",
      "8999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004856] [C loss: 0.693173]\n",
      "9000 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.006382] [C loss: 0.693173]\n",
      "9001 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004722] [C loss: 0.693173]\n",
      "9002 [D loss: 0.693205, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.693173]\n",
      "9003 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003846] [C loss: 0.693173]\n",
      "9004 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.003933] [C loss: 0.693172]\n",
      "9005 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012644] [C loss: 0.693172]\n",
      "9006 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.005540] [C loss: 0.693172]\n",
      "9007 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003639] [C loss: 0.693172]\n",
      "9008 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.004481] [C loss: 0.693172]\n",
      "9009 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004698] [C loss: 0.693172]\n",
      "9010 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.006027] [C loss: 0.693171]\n",
      "9011 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006550] [C loss: 0.693171]\n",
      "9012 [D loss: 0.693204, acc.: 0.00%] [G loss: 0.004884] [C loss: 0.693171]\n",
      "9013 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004066] [C loss: 0.693171]\n",
      "9014 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.005040] [C loss: 0.693171]\n",
      "9015 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004838] [C loss: 0.693171]\n",
      "9016 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.005206] [C loss: 0.693171]\n",
      "9017 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003868] [C loss: 0.693171]\n",
      "9018 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.004306] [C loss: 0.693171]\n",
      "9019 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011489] [C loss: 0.693171]\n",
      "9020 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.006129] [C loss: 0.693170]\n",
      "9021 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006395] [C loss: 0.693170]\n",
      "9022 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.004072] [C loss: 0.693170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9023 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005289] [C loss: 0.693170]\n",
      "9024 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.005672] [C loss: 0.693170]\n",
      "9025 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005344] [C loss: 0.693170]\n",
      "9026 [D loss: 0.693203, acc.: 0.00%] [G loss: 0.003986] [C loss: 0.693170]\n",
      "9027 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004086] [C loss: 0.693170]\n",
      "9028 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.003305] [C loss: 0.693169]\n",
      "9029 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005303] [C loss: 0.693169]\n",
      "9030 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.004416] [C loss: 0.693169]\n",
      "9031 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004817] [C loss: 0.693169]\n",
      "9032 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.003538] [C loss: 0.693169]\n",
      "9033 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007751] [C loss: 0.693169]\n",
      "9034 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.006162] [C loss: 0.693169]\n",
      "9035 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005625] [C loss: 0.693169]\n",
      "9036 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.020439] [C loss: 0.693169]\n",
      "9037 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003816] [C loss: 0.693169]\n",
      "9038 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.009724] [C loss: 0.693169]\n",
      "9039 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005870] [C loss: 0.693169]\n",
      "9040 [D loss: 0.693202, acc.: 0.00%] [G loss: 0.017275] [C loss: 0.693168]\n",
      "9041 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010339] [C loss: 0.693168]\n",
      "9042 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.005022] [C loss: 0.693168]\n",
      "9043 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007506] [C loss: 0.693168]\n",
      "9044 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.006993] [C loss: 0.693168]\n",
      "9045 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007439] [C loss: 0.693168]\n",
      "9046 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.006776] [C loss: 0.693168]\n",
      "9047 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008307] [C loss: 0.693168]\n",
      "9048 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.005185] [C loss: 0.693168]\n",
      "9049 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004711] [C loss: 0.693168]\n",
      "9050 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.004001] [C loss: 0.693167]\n",
      "9051 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.693167]\n",
      "9052 [D loss: 0.693201, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.693167]\n",
      "9053 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006317] [C loss: 0.693167]\n",
      "9054 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.005361] [C loss: 0.693167]\n",
      "9055 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005881] [C loss: 0.693167]\n",
      "9056 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.004403] [C loss: 0.693167]\n",
      "9057 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008436] [C loss: 0.693167]\n",
      "9058 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.004992] [C loss: 0.693166]\n",
      "9059 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005394] [C loss: 0.693166]\n",
      "9060 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.005917] [C loss: 0.693166]\n",
      "9061 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003862] [C loss: 0.693166]\n",
      "9062 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.004318] [C loss: 0.693166]\n",
      "9063 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.693166]\n",
      "9064 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.014065] [C loss: 0.693166]\n",
      "9065 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007290] [C loss: 0.693166]\n",
      "9066 [D loss: 0.693200, acc.: 0.00%] [G loss: 0.004235] [C loss: 0.693166]\n",
      "9067 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007547] [C loss: 0.693166]\n",
      "9068 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.006357] [C loss: 0.693166]\n",
      "9069 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005173] [C loss: 0.693166]\n",
      "9070 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.005670] [C loss: 0.693165]\n",
      "9071 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004916] [C loss: 0.693165]\n",
      "9072 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.006346] [C loss: 0.693165]\n",
      "9073 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008218] [C loss: 0.693165]\n",
      "9074 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.004321] [C loss: 0.693165]\n",
      "9075 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004827] [C loss: 0.693165]\n",
      "9076 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.007033] [C loss: 0.693165]\n",
      "9077 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004781] [C loss: 0.693165]\n",
      "9078 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.006774] [C loss: 0.693165]\n",
      "9079 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004087] [C loss: 0.693165]\n",
      "9080 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.006705] [C loss: 0.693164]\n",
      "9081 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003609] [C loss: 0.693164]\n",
      "9082 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.005181] [C loss: 0.693164]\n",
      "9083 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005786] [C loss: 0.693164]\n",
      "9084 [D loss: 0.693199, acc.: 0.00%] [G loss: 0.004636] [C loss: 0.693164]\n",
      "9085 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005979] [C loss: 0.693164]\n",
      "9086 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.004889] [C loss: 0.693163]\n",
      "9087 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003559] [C loss: 0.693163]\n",
      "9088 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.693164]\n",
      "9089 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004016] [C loss: 0.693164]\n",
      "9090 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.004809] [C loss: 0.693162]\n",
      "9091 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005673] [C loss: 0.693162]\n",
      "9092 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.004480] [C loss: 0.693162]\n",
      "9093 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003929] [C loss: 0.693162]\n",
      "9094 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.012358] [C loss: 0.693162]\n",
      "9095 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003790] [C loss: 0.693162]\n",
      "9096 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.004991] [C loss: 0.693162]\n",
      "9097 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004486] [C loss: 0.693162]\n",
      "9098 [D loss: 0.693198, acc.: 0.00%] [G loss: 0.003561] [C loss: 0.693162]\n",
      "9099 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003593] [C loss: 0.693162]\n",
      "9100 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.008952] [C loss: 0.693163]\n",
      "9101 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003914] [C loss: 0.693163]\n",
      "9102 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.004458] [C loss: 0.693162]\n",
      "9103 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007761] [C loss: 0.693162]\n",
      "9104 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.006975] [C loss: 0.693162]\n",
      "9105 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006263] [C loss: 0.693162]\n",
      "9106 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.005243] [C loss: 0.693162]\n",
      "9107 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003989] [C loss: 0.693162]\n",
      "9108 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.004726] [C loss: 0.693162]\n",
      "9109 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005619] [C loss: 0.693162]\n",
      "9110 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.005204] [C loss: 0.693161]\n",
      "9111 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005698] [C loss: 0.693161]\n",
      "9112 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.003082] [C loss: 0.693161]\n",
      "9113 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004689] [C loss: 0.693161]\n",
      "9114 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.021966] [C loss: 0.693161]\n",
      "9115 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004539] [C loss: 0.693161]\n",
      "9116 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.005307] [C loss: 0.693160]\n",
      "9117 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005899] [C loss: 0.693160]\n",
      "9118 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.004923] [C loss: 0.693160]\n",
      "9119 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006122] [C loss: 0.693160]\n",
      "9120 [D loss: 0.693197, acc.: 0.00%] [G loss: 0.004852] [C loss: 0.693160]\n",
      "9121 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004383] [C loss: 0.693160]\n",
      "9122 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.004858] [C loss: 0.693160]\n",
      "9123 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004325] [C loss: 0.693160]\n",
      "9124 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.005231] [C loss: 0.693160]\n",
      "9125 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006964] [C loss: 0.693160]\n",
      "9126 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.004194] [C loss: 0.693160]\n",
      "9127 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005349] [C loss: 0.693160]\n",
      "9128 [D loss: 0.693196, acc.: 0.00%] [G loss: 0.004899] [C loss: 0.693159]\n",
      "9129 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008424] [C loss: 0.693159]\n",
      "9130 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.004558] [C loss: 0.693159]\n",
      "9131 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006218] [C loss: 0.693159]\n",
      "9132 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.004896] [C loss: 0.693159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9133 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005690] [C loss: 0.693159]\n",
      "9134 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.004822] [C loss: 0.693159]\n",
      "9135 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005026] [C loss: 0.693159]\n",
      "9136 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693159]\n",
      "9137 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.021108] [C loss: 0.693159]\n",
      "9138 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.004992] [C loss: 0.693158]\n",
      "9139 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004675] [C loss: 0.693158]\n",
      "9140 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.005508] [C loss: 0.693158]\n",
      "9141 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.693158]\n",
      "9142 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.004557] [C loss: 0.693158]\n",
      "9143 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007027] [C loss: 0.693158]\n",
      "9144 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.005342] [C loss: 0.693158]\n",
      "9145 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007596] [C loss: 0.693158]\n",
      "9146 [D loss: 0.693195, acc.: 0.00%] [G loss: 0.005846] [C loss: 0.693158]\n",
      "9147 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003503] [C loss: 0.693158]\n",
      "9148 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.004398] [C loss: 0.693157]\n",
      "9149 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004621] [C loss: 0.693157]\n",
      "9150 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.004832] [C loss: 0.693158]\n",
      "9151 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006886] [C loss: 0.693158]\n",
      "9152 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.008402] [C loss: 0.693157]\n",
      "9153 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008011] [C loss: 0.693157]\n",
      "9154 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.003722] [C loss: 0.693157]\n",
      "9155 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004255] [C loss: 0.693157]\n",
      "9156 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.005328] [C loss: 0.693157]\n",
      "9157 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005034] [C loss: 0.693157]\n",
      "9158 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.003675] [C loss: 0.693157]\n",
      "9159 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004331] [C loss: 0.693157]\n",
      "9160 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.004994] [C loss: 0.693156]\n",
      "9161 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004479] [C loss: 0.693156]\n",
      "9162 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.006725] [C loss: 0.693157]\n",
      "9163 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005617] [C loss: 0.693157]\n",
      "9164 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.004215] [C loss: 0.693157]\n",
      "9165 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003572] [C loss: 0.693157]\n",
      "9166 [D loss: 0.693194, acc.: 0.00%] [G loss: 0.004228] [C loss: 0.693156]\n",
      "9167 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003818] [C loss: 0.693156]\n",
      "9168 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.004606] [C loss: 0.693155]\n",
      "9169 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005133] [C loss: 0.693155]\n",
      "9170 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.003427] [C loss: 0.693155]\n",
      "9171 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006631] [C loss: 0.693155]\n",
      "9172 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.004883] [C loss: 0.693155]\n",
      "9173 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004547] [C loss: 0.693155]\n",
      "9174 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.005523] [C loss: 0.693155]\n",
      "9175 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005886] [C loss: 0.693155]\n",
      "9176 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004699] [C loss: 0.693155]\n",
      "9177 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004365] [C loss: 0.693155]\n",
      "9178 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.003856] [C loss: 0.693155]\n",
      "9179 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005063] [C loss: 0.693155]\n",
      "9180 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.005408] [C loss: 0.693155]\n",
      "9181 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008020] [C loss: 0.693155]\n",
      "9182 [D loss: 0.693193, acc.: 0.00%] [G loss: 0.004548] [C loss: 0.693155]\n",
      "9183 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005231] [C loss: 0.693155]\n",
      "9184 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004668] [C loss: 0.693154]\n",
      "9185 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004812] [C loss: 0.693154]\n",
      "9186 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.005646] [C loss: 0.693154]\n",
      "9187 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.015832] [C loss: 0.693154]\n",
      "9188 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.006455] [C loss: 0.693154]\n",
      "9189 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004748] [C loss: 0.693154]\n",
      "9190 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.006928] [C loss: 0.693154]\n",
      "9191 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004935] [C loss: 0.693154]\n",
      "9192 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004623] [C loss: 0.693154]\n",
      "9193 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005612] [C loss: 0.693154]\n",
      "9194 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.005890] [C loss: 0.693153]\n",
      "9195 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006230] [C loss: 0.693153]\n",
      "9196 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004647] [C loss: 0.693153]\n",
      "9197 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004604] [C loss: 0.693153]\n",
      "9198 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004324] [C loss: 0.693153]\n",
      "9199 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004870] [C loss: 0.693153]\n",
      "9200 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.004471] [C loss: 0.693153]\n",
      "9201 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004653] [C loss: 0.693153]\n",
      "9202 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.006216] [C loss: 0.693153]\n",
      "9203 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004719] [C loss: 0.693153]\n",
      "9204 [D loss: 0.693192, acc.: 0.00%] [G loss: 0.003576] [C loss: 0.693152]\n",
      "9205 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003467] [C loss: 0.693152]\n",
      "9206 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.693152]\n",
      "9207 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005819] [C loss: 0.693152]\n",
      "9208 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.005041] [C loss: 0.693152]\n",
      "9209 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007175] [C loss: 0.693152]\n",
      "9210 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.005089] [C loss: 0.693152]\n",
      "9211 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004319] [C loss: 0.693152]\n",
      "9212 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.004912] [C loss: 0.693152]\n",
      "9213 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006687] [C loss: 0.693152]\n",
      "9214 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.004484] [C loss: 0.693152]\n",
      "9215 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004526] [C loss: 0.693152]\n",
      "9216 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.003510] [C loss: 0.693152]\n",
      "9217 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004160] [C loss: 0.693152]\n",
      "9218 [D loss: 0.693191, acc.: 0.00%] [G loss: 0.008868] [C loss: 0.693151]\n",
      "9219 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006047] [C loss: 0.693151]\n",
      "9220 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.005085] [C loss: 0.693151]\n",
      "9221 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005694] [C loss: 0.693151]\n",
      "9222 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.004399] [C loss: 0.693151]\n",
      "9223 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005668] [C loss: 0.693151]\n",
      "9224 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.005186] [C loss: 0.693151]\n",
      "9225 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010470] [C loss: 0.693151]\n",
      "9226 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.004283] [C loss: 0.693151]\n",
      "9227 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006231] [C loss: 0.693151]\n",
      "9228 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.005309] [C loss: 0.693150]\n",
      "9229 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005896] [C loss: 0.693150]\n",
      "9230 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.005046] [C loss: 0.693150]\n",
      "9231 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004045] [C loss: 0.693150]\n",
      "9232 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.005763] [C loss: 0.693150]\n",
      "9233 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008163] [C loss: 0.693150]\n",
      "9234 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.004733] [C loss: 0.693150]\n",
      "9235 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005800] [C loss: 0.693150]\n",
      "9236 [D loss: 0.693190, acc.: 0.00%] [G loss: 0.007085] [C loss: 0.693149]\n",
      "9237 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006074] [C loss: 0.693149]\n",
      "9238 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004093] [C loss: 0.693149]\n",
      "9239 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005500] [C loss: 0.693149]\n",
      "9240 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004949] [C loss: 0.693149]\n",
      "9241 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005769] [C loss: 0.693149]\n",
      "9242 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.005631] [C loss: 0.693149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9243 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005382] [C loss: 0.693149]\n",
      "9244 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004433] [C loss: 0.693149]\n",
      "9245 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004949] [C loss: 0.693149]\n",
      "9246 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004965] [C loss: 0.693148]\n",
      "9247 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005870] [C loss: 0.693148]\n",
      "9248 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.006292] [C loss: 0.693148]\n",
      "9249 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005159] [C loss: 0.693148]\n",
      "9250 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004889] [C loss: 0.693148]\n",
      "9251 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004515] [C loss: 0.693148]\n",
      "9252 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.006849] [C loss: 0.693148]\n",
      "9253 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003940] [C loss: 0.693148]\n",
      "9254 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.007878] [C loss: 0.693148]\n",
      "9255 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004961] [C loss: 0.693148]\n",
      "9256 [D loss: 0.693189, acc.: 0.00%] [G loss: 0.004506] [C loss: 0.693148]\n",
      "9257 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004681] [C loss: 0.693148]\n",
      "9258 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.004563] [C loss: 0.693148]\n",
      "9259 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004240] [C loss: 0.693148]\n",
      "9260 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.004439] [C loss: 0.693147]\n",
      "9261 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012676] [C loss: 0.693147]\n",
      "9262 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.006039] [C loss: 0.693148]\n",
      "9263 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004110] [C loss: 0.693148]\n",
      "9264 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.005188] [C loss: 0.693147]\n",
      "9265 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004637] [C loss: 0.693147]\n",
      "9266 [D loss: 0.693188, acc.: 50.00%] [G loss: 0.006979] [C loss: 0.693147]\n",
      "9267 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004595] [C loss: 0.693147]\n",
      "9268 [D loss: 0.693188, acc.: 0.00%] [G loss: 0.007216] [C loss: 0.693147]\n",
      "9269 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004680] [C loss: 0.693147]\n",
      "9270 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.004678] [C loss: 0.693147]\n",
      "9271 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003050] [C loss: 0.693147]\n",
      "9272 [D loss: 0.693188, acc.: 50.00%] [G loss: 0.005468] [C loss: 0.693147]\n",
      "9273 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004024] [C loss: 0.693147]\n",
      "9274 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.004177] [C loss: 0.693146]\n",
      "9275 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005519] [C loss: 0.693146]\n",
      "9276 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.004254] [C loss: 0.693146]\n",
      "9277 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004068] [C loss: 0.693146]\n",
      "9278 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.007971] [C loss: 0.693146]\n",
      "9279 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004332] [C loss: 0.693146]\n",
      "9280 [D loss: 0.693187, acc.: 93.75%] [G loss: 0.004744] [C loss: 0.693146]\n",
      "9281 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003486] [C loss: 0.693146]\n",
      "9282 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.005026] [C loss: 0.693146]\n",
      "9283 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005435] [C loss: 0.693146]\n",
      "9284 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.003756] [C loss: 0.693146]\n",
      "9285 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005926] [C loss: 0.693146]\n",
      "9286 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.005203] [C loss: 0.693145]\n",
      "9287 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004796] [C loss: 0.693145]\n",
      "9288 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.007782] [C loss: 0.693145]\n",
      "9289 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004601] [C loss: 0.693145]\n",
      "9290 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.004446] [C loss: 0.693145]\n",
      "9291 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006311] [C loss: 0.693145]\n",
      "9292 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.006166] [C loss: 0.693145]\n",
      "9293 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006461] [C loss: 0.693145]\n",
      "9294 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.017669] [C loss: 0.693145]\n",
      "9295 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005325] [C loss: 0.693145]\n",
      "9296 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.007293] [C loss: 0.693145]\n",
      "9297 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007022] [C loss: 0.693145]\n",
      "9298 [D loss: 0.693187, acc.: 50.00%] [G loss: 0.007016] [C loss: 0.693144]\n",
      "9299 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004317] [C loss: 0.693144]\n",
      "9300 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.005310] [C loss: 0.693144]\n",
      "9301 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005134] [C loss: 0.693144]\n",
      "9302 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.004594] [C loss: 0.693144]\n",
      "9303 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004157] [C loss: 0.693144]\n",
      "9304 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.005761] [C loss: 0.693144]\n",
      "9305 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004565] [C loss: 0.693144]\n",
      "9306 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.004251] [C loss: 0.693144]\n",
      "9307 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011822] [C loss: 0.693144]\n",
      "9308 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.003715] [C loss: 0.693143]\n",
      "9309 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006905] [C loss: 0.693143]\n",
      "9310 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.006372] [C loss: 0.693143]\n",
      "9311 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007258] [C loss: 0.693143]\n",
      "9312 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.005405] [C loss: 0.693143]\n",
      "9313 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004190] [C loss: 0.693143]\n",
      "9314 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.006365] [C loss: 0.693143]\n",
      "9315 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004836] [C loss: 0.693143]\n",
      "9316 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.012458] [C loss: 0.693143]\n",
      "9317 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003900] [C loss: 0.693143]\n",
      "9318 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.004471] [C loss: 0.693143]\n",
      "9319 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004295] [C loss: 0.693143]\n",
      "9320 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.005290] [C loss: 0.693143]\n",
      "9321 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010166] [C loss: 0.693143]\n",
      "9322 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.006981] [C loss: 0.693143]\n",
      "9323 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005475] [C loss: 0.693143]\n",
      "9324 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.005800] [C loss: 0.693142]\n",
      "9325 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004892] [C loss: 0.693142]\n",
      "9326 [D loss: 0.693185, acc.: 62.50%] [G loss: 0.005396] [C loss: 0.693142]\n",
      "9327 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007807] [C loss: 0.693142]\n",
      "9328 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.004942] [C loss: 0.693142]\n",
      "9329 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005352] [C loss: 0.693142]\n",
      "9330 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.003267] [C loss: 0.693142]\n",
      "9331 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005180] [C loss: 0.693142]\n",
      "9332 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.004675] [C loss: 0.693142]\n",
      "9333 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004628] [C loss: 0.693142]\n",
      "9334 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.004816] [C loss: 0.693142]\n",
      "9335 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005128] [C loss: 0.693142]\n",
      "9336 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.003200] [C loss: 0.693141]\n",
      "9337 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004427] [C loss: 0.693141]\n",
      "9338 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.004883] [C loss: 0.693142]\n",
      "9339 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004652] [C loss: 0.693142]\n",
      "9340 [D loss: 0.693185, acc.: 50.00%] [G loss: 0.018301] [C loss: 0.693141]\n",
      "9341 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004431] [C loss: 0.693141]\n",
      "9342 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.005779] [C loss: 0.693141]\n",
      "9343 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003767] [C loss: 0.693141]\n",
      "9344 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.006083] [C loss: 0.693141]\n",
      "9345 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005173] [C loss: 0.693141]\n",
      "9346 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.007081] [C loss: 0.693141]\n",
      "9347 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005701] [C loss: 0.693141]\n",
      "9348 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.005262] [C loss: 0.693141]\n",
      "9349 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004921] [C loss: 0.693141]\n",
      "9350 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.005187] [C loss: 0.693140]\n",
      "9351 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007633] [C loss: 0.693140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9352 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.004272] [C loss: 0.693140]\n",
      "9353 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005942] [C loss: 0.693140]\n",
      "9354 [D loss: 0.693184, acc.: 50.00%] [G loss: 0.008571] [C loss: 0.693140]\n",
      "9355 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010183] [C loss: 0.693140]\n",
      "9356 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.005324] [C loss: 0.693140]\n",
      "9357 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005228] [C loss: 0.693140]\n",
      "9358 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.013056] [C loss: 0.693140]\n",
      "9359 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006121] [C loss: 0.693140]\n",
      "9360 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.008451] [C loss: 0.693140]\n",
      "9361 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005226] [C loss: 0.693140]\n",
      "9362 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.007343] [C loss: 0.693140]\n",
      "9363 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009318] [C loss: 0.693140]\n",
      "9364 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.005548] [C loss: 0.693139]\n",
      "9365 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005411] [C loss: 0.693139]\n",
      "9366 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.004866] [C loss: 0.693139]\n",
      "9367 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004087] [C loss: 0.693139]\n",
      "9368 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.012915] [C loss: 0.693139]\n",
      "9369 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005616] [C loss: 0.693139]\n",
      "9370 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.004215] [C loss: 0.693139]\n",
      "9371 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004539] [C loss: 0.693139]\n",
      "9372 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.004632] [C loss: 0.693139]\n",
      "9373 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006245] [C loss: 0.693139]\n",
      "9374 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.006398] [C loss: 0.693139]\n",
      "9375 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005463] [C loss: 0.693139]\n",
      "9376 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.005247] [C loss: 0.693139]\n",
      "9377 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004274] [C loss: 0.693139]\n",
      "9378 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.005605] [C loss: 0.693138]\n",
      "9379 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003864] [C loss: 0.693138]\n",
      "9380 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.005167] [C loss: 0.693138]\n",
      "9381 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004601] [C loss: 0.693138]\n",
      "9382 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.004384] [C loss: 0.693138]\n",
      "9383 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008386] [C loss: 0.693138]\n",
      "9384 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.004559] [C loss: 0.693138]\n",
      "9385 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004238] [C loss: 0.693138]\n",
      "9386 [D loss: 0.693183, acc.: 50.00%] [G loss: 0.007825] [C loss: 0.693138]\n",
      "9387 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006964] [C loss: 0.693138]\n",
      "9388 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.004622] [C loss: 0.693137]\n",
      "9389 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004201] [C loss: 0.693137]\n",
      "9390 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.003777] [C loss: 0.693137]\n",
      "9391 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004216] [C loss: 0.693137]\n",
      "9392 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.004706] [C loss: 0.693137]\n",
      "9393 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005533] [C loss: 0.693137]\n",
      "9394 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.005332] [C loss: 0.693137]\n",
      "9395 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005624] [C loss: 0.693137]\n",
      "9396 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.006615] [C loss: 0.693137]\n",
      "9397 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004962] [C loss: 0.693137]\n",
      "9398 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.004334] [C loss: 0.693137]\n",
      "9399 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004007] [C loss: 0.693137]\n",
      "9400 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.004544] [C loss: 0.693137]\n",
      "9401 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005052] [C loss: 0.693137]\n",
      "9402 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.003839] [C loss: 0.693137]\n",
      "9403 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004973] [C loss: 0.693137]\n",
      "9404 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.003943] [C loss: 0.693137]\n",
      "9405 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005310] [C loss: 0.693137]\n",
      "9406 [D loss: 0.693182, acc.: 50.00%] [G loss: 0.007425] [C loss: 0.693136]\n",
      "9407 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006760] [C loss: 0.693136]\n",
      "9408 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.003518] [C loss: 0.693137]\n",
      "9409 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006036] [C loss: 0.693137]\n",
      "9410 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.010109] [C loss: 0.693136]\n",
      "9411 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005349] [C loss: 0.693136]\n",
      "9412 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.005722] [C loss: 0.693136]\n",
      "9413 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003487] [C loss: 0.693136]\n",
      "9414 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.004826] [C loss: 0.693136]\n",
      "9415 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005158] [C loss: 0.693136]\n",
      "9416 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.009627] [C loss: 0.693135]\n",
      "9417 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004248] [C loss: 0.693135]\n",
      "9418 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.005382] [C loss: 0.693135]\n",
      "9419 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008900] [C loss: 0.693135]\n",
      "9420 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.005674] [C loss: 0.693135]\n",
      "9421 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004535] [C loss: 0.693135]\n",
      "9422 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.005077] [C loss: 0.693135]\n",
      "9423 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004346] [C loss: 0.693135]\n",
      "9424 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.005389] [C loss: 0.693135]\n",
      "9425 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007409] [C loss: 0.693135]\n",
      "9426 [D loss: 0.693181, acc.: 50.00%] [G loss: 0.004068] [C loss: 0.693135]\n",
      "9427 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005865] [C loss: 0.693135]\n",
      "9428 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.015578] [C loss: 0.693135]\n",
      "9429 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005748] [C loss: 0.693135]\n",
      "9430 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.008031] [C loss: 0.693135]\n",
      "9431 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007202] [C loss: 0.693135]\n",
      "9432 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.006690] [C loss: 0.693134]\n",
      "9433 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005371] [C loss: 0.693134]\n",
      "9434 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.003837] [C loss: 0.693135]\n",
      "9435 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005725] [C loss: 0.693135]\n",
      "9436 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.004889] [C loss: 0.693134]\n",
      "9437 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005115] [C loss: 0.693134]\n",
      "9438 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.004716] [C loss: 0.693134]\n",
      "9439 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006390] [C loss: 0.693134]\n",
      "9440 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.004599] [C loss: 0.693134]\n",
      "9441 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003626] [C loss: 0.693134]\n",
      "9442 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.004311] [C loss: 0.693134]\n",
      "9443 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004253] [C loss: 0.693134]\n",
      "9444 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.005178] [C loss: 0.693134]\n",
      "9445 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004094] [C loss: 0.693134]\n",
      "9446 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.003532] [C loss: 0.693134]\n",
      "9447 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003863] [C loss: 0.693134]\n",
      "9448 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.013069] [C loss: 0.693134]\n",
      "9449 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005315] [C loss: 0.693134]\n",
      "9450 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.004045] [C loss: 0.693133]\n",
      "9451 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005186] [C loss: 0.693133]\n",
      "9452 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.005720] [C loss: 0.693133]\n",
      "9453 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005142] [C loss: 0.693133]\n",
      "9454 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.003589] [C loss: 0.693133]\n",
      "9455 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004885] [C loss: 0.693133]\n",
      "9456 [D loss: 0.693180, acc.: 50.00%] [G loss: 0.006770] [C loss: 0.693133]\n",
      "9457 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004080] [C loss: 0.693133]\n",
      "9458 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.004119] [C loss: 0.693133]\n",
      "9459 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005836] [C loss: 0.693133]\n",
      "9460 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.010340] [C loss: 0.693133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9461 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.012445] [C loss: 0.693133]\n",
      "9462 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.004161] [C loss: 0.693133]\n",
      "9463 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005785] [C loss: 0.693133]\n",
      "9464 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.003831] [C loss: 0.693132]\n",
      "9465 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007202] [C loss: 0.693132]\n",
      "9466 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.005020] [C loss: 0.693132]\n",
      "9467 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005316] [C loss: 0.693132]\n",
      "9468 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.005954] [C loss: 0.693132]\n",
      "9469 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004017] [C loss: 0.693132]\n",
      "9470 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.003917] [C loss: 0.693132]\n",
      "9471 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004519] [C loss: 0.693132]\n",
      "9472 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.006091] [C loss: 0.693132]\n",
      "9473 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004076] [C loss: 0.693132]\n",
      "9474 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.005083] [C loss: 0.693132]\n",
      "9475 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005615] [C loss: 0.693132]\n",
      "9476 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.004295] [C loss: 0.693132]\n",
      "9477 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005788] [C loss: 0.693132]\n",
      "9478 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.005247] [C loss: 0.693132]\n",
      "9479 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007751] [C loss: 0.693132]\n",
      "9480 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.006521] [C loss: 0.693132]\n",
      "9481 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003875] [C loss: 0.693132]\n",
      "9482 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.004294] [C loss: 0.693131]\n",
      "9483 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005190] [C loss: 0.693131]\n",
      "9484 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.003955] [C loss: 0.693131]\n",
      "9485 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009795] [C loss: 0.693131]\n",
      "9486 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.005451] [C loss: 0.693131]\n",
      "9487 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009313] [C loss: 0.693131]\n",
      "9488 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.004073] [C loss: 0.693131]\n",
      "9489 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006640] [C loss: 0.693131]\n",
      "9490 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.004282] [C loss: 0.693131]\n",
      "9491 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005841] [C loss: 0.693131]\n",
      "9492 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.004508] [C loss: 0.693131]\n",
      "9493 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008177] [C loss: 0.693131]\n",
      "9494 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.004771] [C loss: 0.693131]\n",
      "9495 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004299] [C loss: 0.693131]\n",
      "9496 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.005810] [C loss: 0.693130]\n",
      "9497 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008295] [C loss: 0.693130]\n",
      "9498 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.003744] [C loss: 0.693130]\n",
      "9499 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005858] [C loss: 0.693130]\n",
      "9500 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.006022] [C loss: 0.693130]\n",
      "9501 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003859] [C loss: 0.693130]\n",
      "9502 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.003780] [C loss: 0.693130]\n",
      "9503 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004029] [C loss: 0.693130]\n",
      "9504 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.005451] [C loss: 0.693130]\n",
      "9505 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004284] [C loss: 0.693130]\n",
      "9506 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.003881] [C loss: 0.693130]\n",
      "9507 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003946] [C loss: 0.693130]\n",
      "9508 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.006149] [C loss: 0.693129]\n",
      "9509 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004693] [C loss: 0.693129]\n",
      "9510 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.005268] [C loss: 0.693130]\n",
      "9511 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005086] [C loss: 0.693130]\n",
      "9512 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.003898] [C loss: 0.693129]\n",
      "9513 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005775] [C loss: 0.693129]\n",
      "9514 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.006293] [C loss: 0.693130]\n",
      "9515 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004594] [C loss: 0.693130]\n",
      "9516 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.003661] [C loss: 0.693129]\n",
      "9517 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005165] [C loss: 0.693129]\n",
      "9518 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004854] [C loss: 0.693129]\n",
      "9519 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005406] [C loss: 0.693129]\n",
      "9520 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.003927] [C loss: 0.693129]\n",
      "9521 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005164] [C loss: 0.693129]\n",
      "9522 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004500] [C loss: 0.693129]\n",
      "9523 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005790] [C loss: 0.693129]\n",
      "9524 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004563] [C loss: 0.693129]\n",
      "9525 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004137] [C loss: 0.693129]\n",
      "9526 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.005964] [C loss: 0.693129]\n",
      "9527 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003864] [C loss: 0.693129]\n",
      "9528 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.011220] [C loss: 0.693129]\n",
      "9529 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005612] [C loss: 0.693129]\n",
      "9530 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004118] [C loss: 0.693129]\n",
      "9531 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004638] [C loss: 0.693129]\n",
      "9532 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004431] [C loss: 0.693129]\n",
      "9533 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004794] [C loss: 0.693129]\n",
      "9534 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004394] [C loss: 0.693128]\n",
      "9535 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006899] [C loss: 0.693128]\n",
      "9536 [D loss: 0.693177, acc.: 50.00%] [G loss: 0.004502] [C loss: 0.693128]\n",
      "9537 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005747] [C loss: 0.693128]\n",
      "9538 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.005126] [C loss: 0.693128]\n",
      "9539 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011124] [C loss: 0.693128]\n",
      "9540 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004829] [C loss: 0.693128]\n",
      "9541 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006062] [C loss: 0.693128]\n",
      "9542 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004747] [C loss: 0.693128]\n",
      "9543 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004316] [C loss: 0.693128]\n",
      "9544 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004694] [C loss: 0.693128]\n",
      "9545 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005481] [C loss: 0.693128]\n",
      "9546 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.006029] [C loss: 0.693128]\n",
      "9547 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006137] [C loss: 0.693128]\n",
      "9548 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004182] [C loss: 0.693128]\n",
      "9549 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007891] [C loss: 0.693128]\n",
      "9550 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.007954] [C loss: 0.693128]\n",
      "9551 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003581] [C loss: 0.693128]\n",
      "9552 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.003612] [C loss: 0.693127]\n",
      "9553 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007234] [C loss: 0.693127]\n",
      "9554 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.005959] [C loss: 0.693127]\n",
      "9555 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005766] [C loss: 0.693127]\n",
      "9556 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.005813] [C loss: 0.693127]\n",
      "9557 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005400] [C loss: 0.693127]\n",
      "9558 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.006435] [C loss: 0.693127]\n",
      "9559 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007550] [C loss: 0.693127]\n",
      "9560 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.006004] [C loss: 0.693127]\n",
      "9561 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004851] [C loss: 0.693127]\n",
      "9562 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004850] [C loss: 0.693126]\n",
      "9563 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005185] [C loss: 0.693126]\n",
      "9564 [D loss: 0.693176, acc.: 50.00%] [G loss: 0.004389] [C loss: 0.693127]\n",
      "9565 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005754] [C loss: 0.693127]\n",
      "9566 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.003670] [C loss: 0.693127]\n",
      "9567 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003315] [C loss: 0.693127]\n",
      "9568 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004003] [C loss: 0.693126]\n",
      "9569 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005604] [C loss: 0.693126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9570 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.006102] [C loss: 0.693126]\n",
      "9571 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004700] [C loss: 0.693126]\n",
      "9572 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.003763] [C loss: 0.693126]\n",
      "9573 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005144] [C loss: 0.693126]\n",
      "9574 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.003424] [C loss: 0.693126]\n",
      "9575 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003826] [C loss: 0.693126]\n",
      "9576 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.006942] [C loss: 0.693126]\n",
      "9577 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003931] [C loss: 0.693126]\n",
      "9578 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.006074] [C loss: 0.693126]\n",
      "9579 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005777] [C loss: 0.693126]\n",
      "9580 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004481] [C loss: 0.693126]\n",
      "9581 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004522] [C loss: 0.693126]\n",
      "9582 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.005293] [C loss: 0.693126]\n",
      "9583 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.693126]\n",
      "9584 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004298] [C loss: 0.693126]\n",
      "9585 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004118] [C loss: 0.693126]\n",
      "9586 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.007492] [C loss: 0.693126]\n",
      "9587 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006184] [C loss: 0.693126]\n",
      "9588 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004779] [C loss: 0.693126]\n",
      "9589 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009713] [C loss: 0.693126]\n",
      "9590 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.005309] [C loss: 0.693125]\n",
      "9591 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007783] [C loss: 0.693125]\n",
      "9592 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004425] [C loss: 0.693125]\n",
      "9593 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003868] [C loss: 0.693125]\n",
      "9594 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.007896] [C loss: 0.693125]\n",
      "9595 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003052] [C loss: 0.693125]\n",
      "9596 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.004684] [C loss: 0.693125]\n",
      "9597 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004761] [C loss: 0.693125]\n",
      "9598 [D loss: 0.693175, acc.: 50.00%] [G loss: 0.005223] [C loss: 0.693125]\n",
      "9599 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004649] [C loss: 0.693125]\n",
      "9600 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.006548] [C loss: 0.693125]\n",
      "9601 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004568] [C loss: 0.693125]\n",
      "9602 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004750] [C loss: 0.693125]\n",
      "9603 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004412] [C loss: 0.693125]\n",
      "9604 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004239] [C loss: 0.693124]\n",
      "9605 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004351] [C loss: 0.693124]\n",
      "9606 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004573] [C loss: 0.693125]\n",
      "9607 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006548] [C loss: 0.693125]\n",
      "9608 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.005119] [C loss: 0.693124]\n",
      "9609 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006127] [C loss: 0.693124]\n",
      "9610 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004568] [C loss: 0.693124]\n",
      "9611 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003928] [C loss: 0.693124]\n",
      "9612 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.005592] [C loss: 0.693124]\n",
      "9613 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004173] [C loss: 0.693124]\n",
      "9614 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004965] [C loss: 0.693124]\n",
      "9615 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009812] [C loss: 0.693124]\n",
      "9616 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004469] [C loss: 0.693124]\n",
      "9617 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004875] [C loss: 0.693124]\n",
      "9618 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004408] [C loss: 0.693124]\n",
      "9619 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004141] [C loss: 0.693124]\n",
      "9620 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.005803] [C loss: 0.693124]\n",
      "9621 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004028] [C loss: 0.693124]\n",
      "9622 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004139] [C loss: 0.693124]\n",
      "9623 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004100] [C loss: 0.693124]\n",
      "9624 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.005809] [C loss: 0.693124]\n",
      "9625 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005690] [C loss: 0.693124]\n",
      "9626 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.005478] [C loss: 0.693124]\n",
      "9627 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004901] [C loss: 0.693124]\n",
      "9628 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.006055] [C loss: 0.693124]\n",
      "9629 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006791] [C loss: 0.693124]\n",
      "9630 [D loss: 0.693174, acc.: 50.00%] [G loss: 0.004955] [C loss: 0.693123]\n",
      "9631 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005576] [C loss: 0.693123]\n",
      "9632 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.008055] [C loss: 0.693123]\n",
      "9633 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005975] [C loss: 0.693123]\n",
      "9634 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.006466] [C loss: 0.693123]\n",
      "9635 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004445] [C loss: 0.693123]\n",
      "9636 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.003960] [C loss: 0.693123]\n",
      "9637 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011999] [C loss: 0.693123]\n",
      "9638 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004827] [C loss: 0.693123]\n",
      "9639 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004921] [C loss: 0.693123]\n",
      "9640 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.003882] [C loss: 0.693123]\n",
      "9641 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004674] [C loss: 0.693123]\n",
      "9642 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004259] [C loss: 0.693123]\n",
      "9643 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004261] [C loss: 0.693123]\n",
      "9644 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.005882] [C loss: 0.693123]\n",
      "9645 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003448] [C loss: 0.693123]\n",
      "9646 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004324] [C loss: 0.693123]\n",
      "9647 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003783] [C loss: 0.693123]\n",
      "9648 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.005406] [C loss: 0.693123]\n",
      "9649 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.693123]\n",
      "9650 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.007949] [C loss: 0.693122]\n",
      "9651 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003876] [C loss: 0.693122]\n",
      "9652 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.003505] [C loss: 0.693123]\n",
      "9653 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005343] [C loss: 0.693123]\n",
      "9654 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.022068] [C loss: 0.693122]\n",
      "9655 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004663] [C loss: 0.693122]\n",
      "9656 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004396] [C loss: 0.693122]\n",
      "9657 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005979] [C loss: 0.693122]\n",
      "9658 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004194] [C loss: 0.693122]\n",
      "9659 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004466] [C loss: 0.693122]\n",
      "9660 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.009914] [C loss: 0.693122]\n",
      "9661 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005389] [C loss: 0.693122]\n",
      "9662 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.013043] [C loss: 0.693122]\n",
      "9663 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005680] [C loss: 0.693122]\n",
      "9664 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.005123] [C loss: 0.693122]\n",
      "9665 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005229] [C loss: 0.693122]\n",
      "9666 [D loss: 0.693173, acc.: 50.00%] [G loss: 0.004834] [C loss: 0.693122]\n",
      "9667 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005232] [C loss: 0.693122]\n",
      "9668 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.006392] [C loss: 0.693122]\n",
      "9669 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006013] [C loss: 0.693122]\n",
      "9670 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005225] [C loss: 0.693122]\n",
      "9671 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007389] [C loss: 0.693122]\n",
      "9672 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.004855] [C loss: 0.693121]\n",
      "9673 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005034] [C loss: 0.693121]\n",
      "9674 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.021397] [C loss: 0.693121]\n",
      "9675 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005751] [C loss: 0.693121]\n",
      "9676 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.004729] [C loss: 0.693121]\n",
      "9677 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005569] [C loss: 0.693121]\n",
      "9678 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005671] [C loss: 0.693121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9679 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004594] [C loss: 0.693121]\n",
      "9680 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.008834] [C loss: 0.693121]\n",
      "9681 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003739] [C loss: 0.693121]\n",
      "9682 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.006339] [C loss: 0.693121]\n",
      "9683 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004098] [C loss: 0.693121]\n",
      "9684 [D loss: 0.693172, acc.: 93.75%] [G loss: 0.005921] [C loss: 0.693121]\n",
      "9685 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005134] [C loss: 0.693121]\n",
      "9686 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.004769] [C loss: 0.693121]\n",
      "9687 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006542] [C loss: 0.693121]\n",
      "9688 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.004495] [C loss: 0.693121]\n",
      "9689 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004816] [C loss: 0.693121]\n",
      "9690 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.003807] [C loss: 0.693121]\n",
      "9691 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004793] [C loss: 0.693121]\n",
      "9692 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.009929] [C loss: 0.693121]\n",
      "9693 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005733] [C loss: 0.693121]\n",
      "9694 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.004856] [C loss: 0.693121]\n",
      "9695 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.693121]\n",
      "9696 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005298] [C loss: 0.693120]\n",
      "9697 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006087] [C loss: 0.693120]\n",
      "9698 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005128] [C loss: 0.693120]\n",
      "9699 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005063] [C loss: 0.693120]\n",
      "9700 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005617] [C loss: 0.693120]\n",
      "9701 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014041] [C loss: 0.693120]\n",
      "9702 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.005653] [C loss: 0.693120]\n",
      "9703 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007457] [C loss: 0.693120]\n",
      "9704 [D loss: 0.693172, acc.: 50.00%] [G loss: 0.006069] [C loss: 0.693120]\n",
      "9705 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004215] [C loss: 0.693120]\n",
      "9706 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004734] [C loss: 0.693120]\n",
      "9707 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004276] [C loss: 0.693120]\n",
      "9708 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.005344] [C loss: 0.693120]\n",
      "9709 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006226] [C loss: 0.693120]\n",
      "9710 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004316] [C loss: 0.693120]\n",
      "9711 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005667] [C loss: 0.693120]\n",
      "9712 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.003415] [C loss: 0.693120]\n",
      "9713 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005563] [C loss: 0.693120]\n",
      "9714 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.009989] [C loss: 0.693120]\n",
      "9715 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004354] [C loss: 0.693120]\n",
      "9716 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.003848] [C loss: 0.693120]\n",
      "9717 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007548] [C loss: 0.693120]\n",
      "9718 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004935] [C loss: 0.693120]\n",
      "9719 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005463] [C loss: 0.693120]\n",
      "9720 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.005065] [C loss: 0.693119]\n",
      "9721 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005153] [C loss: 0.693119]\n",
      "9722 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004090] [C loss: 0.693120]\n",
      "9723 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005451] [C loss: 0.693120]\n",
      "9724 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.003548] [C loss: 0.693120]\n",
      "9725 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005526] [C loss: 0.693120]\n",
      "9726 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.007656] [C loss: 0.693119]\n",
      "9727 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005647] [C loss: 0.693119]\n",
      "9728 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.006220] [C loss: 0.693119]\n",
      "9729 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003868] [C loss: 0.693119]\n",
      "9730 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.006892] [C loss: 0.693119]\n",
      "9731 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005650] [C loss: 0.693119]\n",
      "9732 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004922] [C loss: 0.693119]\n",
      "9733 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005471] [C loss: 0.693119]\n",
      "9734 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004858] [C loss: 0.693119]\n",
      "9735 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003772] [C loss: 0.693119]\n",
      "9736 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.013227] [C loss: 0.693119]\n",
      "9737 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007225] [C loss: 0.693119]\n",
      "9738 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004639] [C loss: 0.693119]\n",
      "9739 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006592] [C loss: 0.693119]\n",
      "9740 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.005219] [C loss: 0.693119]\n",
      "9741 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007166] [C loss: 0.693119]\n",
      "9742 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.005355] [C loss: 0.693119]\n",
      "9743 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004534] [C loss: 0.693119]\n",
      "9744 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.006609] [C loss: 0.693119]\n",
      "9745 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004984] [C loss: 0.693119]\n",
      "9746 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.004625] [C loss: 0.693119]\n",
      "9747 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007437] [C loss: 0.693119]\n",
      "9748 [D loss: 0.693171, acc.: 50.00%] [G loss: 0.003820] [C loss: 0.693119]\n",
      "9749 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005069] [C loss: 0.693119]\n",
      "9750 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.005991] [C loss: 0.693118]\n",
      "9751 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006894] [C loss: 0.693118]\n",
      "9752 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.003875] [C loss: 0.693118]\n",
      "9753 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005476] [C loss: 0.693118]\n",
      "9754 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.005536] [C loss: 0.693118]\n",
      "9755 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005809] [C loss: 0.693118]\n",
      "9756 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.006051] [C loss: 0.693118]\n",
      "9757 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007180] [C loss: 0.693118]\n",
      "9758 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.003952] [C loss: 0.693118]\n",
      "9759 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006980] [C loss: 0.693118]\n",
      "9760 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004624] [C loss: 0.693118]\n",
      "9761 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004427] [C loss: 0.693118]\n",
      "9762 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.005357] [C loss: 0.693118]\n",
      "9763 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005940] [C loss: 0.693118]\n",
      "9764 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004024] [C loss: 0.693118]\n",
      "9765 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005726] [C loss: 0.693118]\n",
      "9766 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004069] [C loss: 0.693118]\n",
      "9767 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003785] [C loss: 0.693118]\n",
      "9768 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.003175] [C loss: 0.693118]\n",
      "9769 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009443] [C loss: 0.693118]\n",
      "9770 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.005335] [C loss: 0.693118]\n",
      "9771 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006329] [C loss: 0.693118]\n",
      "9772 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.005092] [C loss: 0.693118]\n",
      "9773 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005089] [C loss: 0.693118]\n",
      "9774 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.003665] [C loss: 0.693118]\n",
      "9775 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004901] [C loss: 0.693118]\n",
      "9776 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.006964] [C loss: 0.693118]\n",
      "9777 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005324] [C loss: 0.693118]\n",
      "9778 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004014] [C loss: 0.693117]\n",
      "9779 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004584] [C loss: 0.693117]\n",
      "9780 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004523] [C loss: 0.693117]\n",
      "9781 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003091] [C loss: 0.693117]\n",
      "9782 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.003740] [C loss: 0.693117]\n",
      "9783 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003849] [C loss: 0.693117]\n",
      "9784 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.003610] [C loss: 0.693117]\n",
      "9785 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004629] [C loss: 0.693117]\n",
      "9786 [D loss: 0.693170, acc.: 50.00%] [G loss: 0.004254] [C loss: 0.693117]\n",
      "9787 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004941] [C loss: 0.693117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9788 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005220] [C loss: 0.693117]\n",
      "9789 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008236] [C loss: 0.693117]\n",
      "9790 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005479] [C loss: 0.693117]\n",
      "9791 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005280] [C loss: 0.693117]\n",
      "9792 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004620] [C loss: 0.693117]\n",
      "9793 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004159] [C loss: 0.693117]\n",
      "9794 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.007341] [C loss: 0.693117]\n",
      "9795 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004216] [C loss: 0.693117]\n",
      "9796 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005650] [C loss: 0.693117]\n",
      "9797 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005725] [C loss: 0.693117]\n",
      "9798 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004965] [C loss: 0.693117]\n",
      "9799 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005219] [C loss: 0.693117]\n",
      "9800 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005912] [C loss: 0.693117]\n",
      "9801 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005238] [C loss: 0.693117]\n",
      "9802 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.007137] [C loss: 0.693117]\n",
      "9803 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004126] [C loss: 0.693117]\n",
      "9804 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.021546] [C loss: 0.693117]\n",
      "9805 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004754] [C loss: 0.693117]\n",
      "9806 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.013767] [C loss: 0.693117]\n",
      "9807 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006304] [C loss: 0.693117]\n",
      "9808 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005516] [C loss: 0.693117]\n",
      "9809 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005775] [C loss: 0.693117]\n",
      "9810 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.006093] [C loss: 0.693116]\n",
      "9811 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004093] [C loss: 0.693116]\n",
      "9812 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004019] [C loss: 0.693116]\n",
      "9813 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004396] [C loss: 0.693116]\n",
      "9814 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.006257] [C loss: 0.693116]\n",
      "9815 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006495] [C loss: 0.693116]\n",
      "9816 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004644] [C loss: 0.693116]\n",
      "9817 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009661] [C loss: 0.693116]\n",
      "9818 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.003851] [C loss: 0.693116]\n",
      "9819 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004367] [C loss: 0.693116]\n",
      "9820 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004617] [C loss: 0.693116]\n",
      "9821 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003429] [C loss: 0.693116]\n",
      "9822 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004098] [C loss: 0.693116]\n",
      "9823 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004655] [C loss: 0.693116]\n",
      "9824 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004886] [C loss: 0.693116]\n",
      "9825 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005200] [C loss: 0.693116]\n",
      "9826 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.019597] [C loss: 0.693116]\n",
      "9827 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003212] [C loss: 0.693116]\n",
      "9828 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.003698] [C loss: 0.693116]\n",
      "9829 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004757] [C loss: 0.693116]\n",
      "9830 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004779] [C loss: 0.693116]\n",
      "9831 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005184] [C loss: 0.693116]\n",
      "9832 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004399] [C loss: 0.693116]\n",
      "9833 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004191] [C loss: 0.693116]\n",
      "9834 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004318] [C loss: 0.693116]\n",
      "9835 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006513] [C loss: 0.693116]\n",
      "9836 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.004238] [C loss: 0.693116]\n",
      "9837 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005409] [C loss: 0.693116]\n",
      "9838 [D loss: 0.693169, acc.: 50.00%] [G loss: 0.005800] [C loss: 0.693116]\n",
      "9839 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005635] [C loss: 0.693116]\n",
      "9840 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.003864] [C loss: 0.693116]\n",
      "9841 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005176] [C loss: 0.693116]\n",
      "9842 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004643] [C loss: 0.693116]\n",
      "9843 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004607] [C loss: 0.693116]\n",
      "9844 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005613] [C loss: 0.693115]\n",
      "9845 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004412] [C loss: 0.693115]\n",
      "9846 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.003637] [C loss: 0.693115]\n",
      "9847 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003304] [C loss: 0.693115]\n",
      "9848 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004271] [C loss: 0.693115]\n",
      "9849 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006051] [C loss: 0.693115]\n",
      "9850 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.003320] [C loss: 0.693115]\n",
      "9851 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005132] [C loss: 0.693115]\n",
      "9852 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.006188] [C loss: 0.693115]\n",
      "9853 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005939] [C loss: 0.693115]\n",
      "9854 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005650] [C loss: 0.693115]\n",
      "9855 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005162] [C loss: 0.693115]\n",
      "9856 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.006878] [C loss: 0.693115]\n",
      "9857 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.002973] [C loss: 0.693115]\n",
      "9858 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005949] [C loss: 0.693115]\n",
      "9859 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006666] [C loss: 0.693115]\n",
      "9860 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.007297] [C loss: 0.693115]\n",
      "9861 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005979] [C loss: 0.693115]\n",
      "9862 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005135] [C loss: 0.693115]\n",
      "9863 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006001] [C loss: 0.693115]\n",
      "9864 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.003612] [C loss: 0.693115]\n",
      "9865 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004764] [C loss: 0.693115]\n",
      "9866 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005773] [C loss: 0.693115]\n",
      "9867 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.018505] [C loss: 0.693115]\n",
      "9868 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.011770] [C loss: 0.693115]\n",
      "9869 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004838] [C loss: 0.693115]\n",
      "9870 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.006566] [C loss: 0.693115]\n",
      "9871 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005855] [C loss: 0.693115]\n",
      "9872 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005042] [C loss: 0.693115]\n",
      "9873 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006859] [C loss: 0.693115]\n",
      "9874 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.006458] [C loss: 0.693115]\n",
      "9875 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004492] [C loss: 0.693115]\n",
      "9876 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004095] [C loss: 0.693115]\n",
      "9877 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.014773] [C loss: 0.693115]\n",
      "9878 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005609] [C loss: 0.693115]\n",
      "9879 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005523] [C loss: 0.693115]\n",
      "9880 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.005081] [C loss: 0.693115]\n",
      "9881 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006356] [C loss: 0.693115]\n",
      "9882 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004175] [C loss: 0.693115]\n",
      "9883 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007248] [C loss: 0.693115]\n",
      "9884 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.007194] [C loss: 0.693115]\n",
      "9885 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.019998] [C loss: 0.693115]\n",
      "9886 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004643] [C loss: 0.693115]\n",
      "9887 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004631] [C loss: 0.693115]\n",
      "9888 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.007747] [C loss: 0.693115]\n",
      "9889 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004486] [C loss: 0.693115]\n",
      "9890 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.003595] [C loss: 0.693115]\n",
      "9891 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.011595] [C loss: 0.693115]\n",
      "9892 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005084] [C loss: 0.693114]\n",
      "9893 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004757] [C loss: 0.693114]\n",
      "9894 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.010690] [C loss: 0.693115]\n",
      "9895 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006596] [C loss: 0.693115]\n",
      "9896 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.004330] [C loss: 0.693115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9897 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004934] [C loss: 0.693115]\n",
      "9898 [D loss: 0.693168, acc.: 50.00%] [G loss: 0.004233] [C loss: 0.693115]\n",
      "9899 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009127] [C loss: 0.693115]\n",
      "9900 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.004871] [C loss: 0.693114]\n",
      "9901 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005956] [C loss: 0.693114]\n",
      "9902 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005423] [C loss: 0.693114]\n",
      "9903 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006974] [C loss: 0.693114]\n",
      "9904 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005078] [C loss: 0.693114]\n",
      "9905 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006597] [C loss: 0.693114]\n",
      "9906 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.004708] [C loss: 0.693114]\n",
      "9907 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005424] [C loss: 0.693114]\n",
      "9908 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005246] [C loss: 0.693114]\n",
      "9909 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004251] [C loss: 0.693114]\n",
      "9910 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006106] [C loss: 0.693114]\n",
      "9911 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004283] [C loss: 0.693114]\n",
      "9912 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.009293] [C loss: 0.693114]\n",
      "9913 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009412] [C loss: 0.693114]\n",
      "9914 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005261] [C loss: 0.693114]\n",
      "9915 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005633] [C loss: 0.693114]\n",
      "9916 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.003702] [C loss: 0.693114]\n",
      "9917 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004969] [C loss: 0.693114]\n",
      "9918 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.004083] [C loss: 0.693114]\n",
      "9919 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004811] [C loss: 0.693114]\n",
      "9920 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.004472] [C loss: 0.693114]\n",
      "9921 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004891] [C loss: 0.693114]\n",
      "9922 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006677] [C loss: 0.693114]\n",
      "9923 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008059] [C loss: 0.693114]\n",
      "9924 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005743] [C loss: 0.693114]\n",
      "9925 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.017171] [C loss: 0.693114]\n",
      "9926 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006952] [C loss: 0.693114]\n",
      "9927 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004543] [C loss: 0.693114]\n",
      "9928 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006214] [C loss: 0.693114]\n",
      "9929 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005603] [C loss: 0.693114]\n",
      "9930 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005880] [C loss: 0.693114]\n",
      "9931 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004862] [C loss: 0.693114]\n",
      "9932 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006015] [C loss: 0.693114]\n",
      "9933 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008892] [C loss: 0.693114]\n",
      "9934 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.003943] [C loss: 0.693114]\n",
      "9935 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004385] [C loss: 0.693114]\n",
      "9936 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.006138] [C loss: 0.693113]\n",
      "9937 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005962] [C loss: 0.693113]\n",
      "9938 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.005251] [C loss: 0.693113]\n",
      "9939 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004148] [C loss: 0.693113]\n",
      "9940 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.003973] [C loss: 0.693113]\n",
      "9941 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005753] [C loss: 0.693113]\n",
      "9942 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.011262] [C loss: 0.693113]\n",
      "9943 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004551] [C loss: 0.693113]\n",
      "9944 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005964] [C loss: 0.693113]\n",
      "9945 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003880] [C loss: 0.693113]\n",
      "9946 [D loss: 0.693167, acc.: 50.00%] [G loss: 0.003958] [C loss: 0.693113]\n",
      "9947 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010711] [C loss: 0.693113]\n",
      "9948 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004486] [C loss: 0.693113]\n",
      "9949 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005244] [C loss: 0.693113]\n",
      "9950 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.006081] [C loss: 0.693113]\n",
      "9951 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.008737] [C loss: 0.693113]\n",
      "9952 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004635] [C loss: 0.693113]\n",
      "9953 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004190] [C loss: 0.693113]\n",
      "9954 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.006124] [C loss: 0.693113]\n",
      "9955 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005137] [C loss: 0.693113]\n",
      "9956 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005632] [C loss: 0.693113]\n",
      "9957 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004473] [C loss: 0.693113]\n",
      "9958 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004279] [C loss: 0.693113]\n",
      "9959 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003864] [C loss: 0.693113]\n",
      "9960 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004119] [C loss: 0.693113]\n",
      "9961 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006062] [C loss: 0.693113]\n",
      "9962 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.006208] [C loss: 0.693113]\n",
      "9963 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.009826] [C loss: 0.693113]\n",
      "9964 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.006780] [C loss: 0.693113]\n",
      "9965 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003495] [C loss: 0.693113]\n",
      "9966 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.003811] [C loss: 0.693113]\n",
      "9967 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005106] [C loss: 0.693113]\n",
      "9968 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005657] [C loss: 0.693113]\n",
      "9969 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.010322] [C loss: 0.693113]\n",
      "9970 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005434] [C loss: 0.693113]\n",
      "9971 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003855] [C loss: 0.693113]\n",
      "9972 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005565] [C loss: 0.693113]\n",
      "9973 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005813] [C loss: 0.693113]\n",
      "9974 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004150] [C loss: 0.693113]\n",
      "9975 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004118] [C loss: 0.693113]\n",
      "9976 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004239] [C loss: 0.693113]\n",
      "9977 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004092] [C loss: 0.693113]\n",
      "9978 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.003878] [C loss: 0.693113]\n",
      "9979 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.007669] [C loss: 0.693113]\n",
      "9980 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.008156] [C loss: 0.693113]\n",
      "9981 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.003668] [C loss: 0.693113]\n",
      "9982 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005218] [C loss: 0.693113]\n",
      "9983 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006675] [C loss: 0.693113]\n",
      "9984 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004355] [C loss: 0.693113]\n",
      "9985 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006188] [C loss: 0.693113]\n",
      "9986 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005004] [C loss: 0.693113]\n",
      "9987 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004436] [C loss: 0.693113]\n",
      "9988 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.007237] [C loss: 0.693113]\n",
      "9989 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.006572] [C loss: 0.693113]\n",
      "9990 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005122] [C loss: 0.693113]\n",
      "9991 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005946] [C loss: 0.693113]\n",
      "9992 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.006918] [C loss: 0.693113]\n",
      "9993 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005298] [C loss: 0.693113]\n",
      "9994 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004942] [C loss: 0.693112]\n",
      "9995 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005803] [C loss: 0.693112]\n",
      "9996 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.005430] [C loss: 0.693113]\n",
      "9997 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.004522] [C loss: 0.693113]\n",
      "9998 [D loss: 0.693166, acc.: 50.00%] [G loss: 0.004787] [C loss: 0.693112]\n",
      "9999 [D loss: 0.000000, acc.: 0.00%] [G loss: 0.005033] [C loss: 0.693112]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dcg)\n",
    "dcgan = dcg.DCGAN()\n",
    "dcgan.train(epochs=10000, batch_size=8, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:26:03.062875Z",
     "start_time": "2018-05-07T13:26:03.059869Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_dir = r\"D:\\CBCT\\Test\\AARON\"\n",
    "dcm_path = img_dir+\"\\\\15814.dcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:26:24.772134Z",
     "start_time": "2018-05-07T13:26:24.746039Z"
    }
   },
   "outputs": [],
   "source": [
    "dicom_input = hf.load_dcm_header(dcm_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:28:24.197788Z",
     "start_time": "2018-05-07T13:28:24.184725Z"
    }
   },
   "outputs": [],
   "source": [
    "cine_rate = float(dicom_input[('0018', '0040')].value)\n",
    "kvp = float(dicom_input[('0018', '0060')].value)\n",
    "d_detector = int(dicom_input[('0018', '1110')].value)\n",
    "d_patient = int(dicom_input[('0018', '1111')].value)\n",
    "t_exposure = int(dicom_input[('0018', '1150')].value)\n",
    "current = int(dicom_input[('0018', '1151')].value)\n",
    "exposure = int(dicom_input[('0018', '1152')].value)\n",
    "pixel_spacing = dicom_input[('0018', '1164')].value\n",
    "angle1 = float(dicom_input[('0018', '1510')].value)\n",
    "angle2 = float(dicom_input[('0018', '1511')].value)\n",
    "angle1_increment = dicom_input[('0018', '1520')].value\n",
    "angle2_increment = dicom_input[('0018', '1521')].value\n",
    "shutter_edges = [int(dicom_input[('0018', str(x))].value) for x in [1602, 1604, 1606, 1608]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:26:44.081293Z",
     "start_time": "2018-05-07T13:26:30.322088Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj_imgs, _ = hf.dcm_load(dcm_path)\n",
    "proj_imgs = np.transpose(proj_imgs, (1,2,0))\n",
    "proj_imgs_unscaled = copy.deepcopy(proj_imgs)\n",
    "proj_imgs = (proj_imgs - np.amin(proj_imgs)) / (np.amax(proj_imgs) - np.amin(proj_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj_imgs = proj_imgs[:, :, 50:-50]\n",
    "rows, frames, cols = proj_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:27:11.963153Z",
     "start_time": "2018-05-07T13:27:11.826195Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23fd01ca0f0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23fd0174f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "proj_imgs = np.transpose(proj_imgs, (2,0,1))\n",
    "pylab.imshow(proj_imgs[:,:,100], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:28:02.623833Z",
     "start_time": "2018-05-07T13:28:02.619847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_imgs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T13:27:56.859539Z",
     "start_time": "2018-05-07T13:27:56.856531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(angle1_increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 308, 512)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_cropped = rec[:,10:120,20:170]\n",
    "rec_cropped[rec_cropped < 0] = 0\n",
    "rec_cropped = np.transpose(rec_cropped, (2,1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_cropped = rec_cropped * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABnCAYAAAA3+Dg6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x203843a0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf.draw_slices(rec_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-59.913471"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.amin(rec[80,10:120,20:170]), np.amax(rec[80,10:120,20:170]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pylab.imshow(np.transpose(rec[:,:,::-1][50,20:170,10:120], (1,0)), vmin=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20595ca9908>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAD8CAYAAAAoqlyCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20595c95f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(rec[100,10:120,20:170])#, vmin=-50, vmax=150) #vmin=.0010, vmax=0.0035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recon_img = hf.dcm_load(img_dir+\"\\\\15810.dcm\")\n",
    "recon_img = np.transpose(recon_img, (1,2,0))[::-1,:,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
